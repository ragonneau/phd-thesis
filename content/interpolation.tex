%% contents/interpolation.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Interpolation models for \glsfmtlong{dfo}}
\label{ch:interpolation}

\todo[noline]{Say what we are going to do (probably moving some of the next section)}

\section{Introduction and motivation}

As mentioned in \cref{ch:introduction}, model-based \gls{dfo} methods necessitate approximating locally the functions involved in optimization problems by simple functions, referred to as \emph{models} or \emph{surrogates}.
These models are used to construct subproblems that are, in turn, approximately minimized.
Examples of such functions commonly used in the literature are polynomials and \glspl{rbf}~\cite{Powell_2004a}.
In this thesis, we focus on linear and quadratic polynomial models, i.e., on polynomials of degree at most one and two, respectively.

Let~$\lpoly$\nomenclature[Sh]{$\lpoly$}{Space of linear polynomials in~$\R^n$} and~$\qpoly$\nomenclature[Si]{$\qpoly$}{Space of quadratic polynomials in~$\R^n$} denote the spaces of linear and quadratic polynomials on~$\R^n$, respectively.
In a \gls{dfo} context, models from~$\lpoly$ or~$\qpoly$ are built for a real-valued function~$\obj$ without using derivatives.
This can be done by interpolation schemes based on function values.
Given a finite set of points~$\xpt = \set{y^1, y^2, \dots, y^m} \subseteq \R^n$, we construct a model~$\objm$ that interpolates the function~$\obj$ on~$\xpt$, i.e.,
\begin{equation}
    \label{eq:interpolation-conditions}
    \objm(y^i) = \obj(y^i), \quad \text{for~$i \in \set{1, 2, \dots, m}$}.
\end{equation}

The conditions~\cref{eq:interpolation-conditions} may be inconsistent.
In such a case, models can be built using regression schemes.
For example, a least-square regression model~$\objm$ minimizes
\begin{equation*}
    \sum_{i = 1}^m [\obj(y^i) - \objm(y^i)]^2.
\end{equation*}
Although there are successful methods that use regression models (see, e.g.,~\cite{Billups_Larson_Graf_2013,Conn_Scheinberg_Vicente_2008b}), the \gls{dfo} methods we present and develop in this thesis use interpolation models and ensure that the interpolation conditions are consistent and well-conditioned (see \cref{ch:pdfo,ch:cobyqa-introduction}).

It is also possible to use polynomials of degree higher than two.
However, we do not consider such models in this thesis due to the following observations.
\begin{enumerate}
    \item As shown in~\cite[Thm.~2.5]{Wendland_2005}, the space of polynomials on~$\R^n$ of degree at most~$k$ has a dimension of
    \begin{equation*}
        \binom{n + k}{n} = \frac{1}{k!} \prod_{i = 1}^k (n + i) \ge \frac{n^k}{k!}.
    \end{equation*}
    Therefore, to determine a model from this space merely by the interpolation conditions~\cref{eq:interpolation-conditions}, we need in general~$\bigo(n^k)$ function values.
    This amount is unacceptable in a \gls{dfo} context unless~$k$ is small.
    It is possible to reduce this number with underdetermined interpolation, which is used by several optimization methods for~$k \le 2$ (see \cref{sec:underdetermined-interpolation}), including the method \gls{cobyqa} developed in this thesis (see \cref{ch:cobyqa-introduction}).
    Using underdetermined interpolation models with~$k \ge 3$ is out of the scope of this thesis, although it is an interesting research direction.
    \item The \gls{dfo} methods need to solve approximately subproblems (e.g., trust-region subproblems) built upon these models.
    Sophisticated models usually lead to complicated subproblems to solve.
    On the other hand, even with models that are not quadratic, practical algorithms normally solve the subproblems based on first- or second-order approximations of the models, e.g., calculate the approximate Cauchy point discussed in~\cite[\S~6.3.3]{Conn_Gould_Toint_2000}.
    Therefore, building polynomial models of degrees higher than two may not be necessary.
\end{enumerate}

Although we do not study \gls{rbf} models, we mention that there also exist many \gls{dfo} methods based on these models.
Examples of such methods include \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}.

\section{Elementary concepts of multivariate interpolation}
\label{sec:multivariate-interpolation}

The space in which~$\objm$ lies in this section is either~$\lpoly$ or~$\qpoly$.
Moreover, we assume in this section that number of interpolation points~$m$ equals the dimension of the chosen space, i.e., either~$n + 1$ or~$(n + 1)(n + 2) / 2$, respectively.

\subsection{Poisedness of interpolation sets}
\label{subsec:poisedness}

Before studying properties of multivariate interpolation, we must introduce the following notion of poisedness.

\begin{definition}[Poisedness]
    The set~$\xpt$ is \emph{poised} for interpolation on~$\lpoly$ if the interpolation system~\cref{eq:interpolation-conditions} has a unique solution in~$\lpoly$ for any real-valued function~$\obj$.
    Similarly, it is \emph{poised} for interpolation on~$\qpoly$ if the system has a unique solution in~$\qpoly$ for any real-valued function~$\obj$.
\end{definition}

Let us first consider the problem of finding a linear model~$\objm \in \lpoly$ satisfying the interpolation system~\cref{eq:interpolation-conditions} whenever~$m = \dim \lpoly = n + 1$.
In the natural basis of~$\lpoly$, the system~\cref{eq:interpolation-conditions} can be reformulated as
\begin{equation}
    \label{eq:linear-interpolation-conditions}
    \alpha + g^{\T} y^i = \obj(y^i), \quad \text{for~$i \in \set{1, 2, \dots, m}$},
\end{equation}
where~$\alpha \in \R$ and~$g \in \R^n$ are the coefficients to determine.
If~$\xpt$ is poised for linear interpolation, given~$(\alpha^{\ast}, g^{\ast})$ the unique solution to the system~\cref{eq:linear-interpolation-conditions}, the linear model~$\objm$ is defined by
\begin{equation*}
    \objm(\iter) = \alpha^{\ast} + (g^{\ast})^{\T} x, \quad \text{for~$\iter \in \R^n$},
\end{equation*}
and the vector~$\nabla \objm \equiv g^{\ast}$ is referred to as the \emph{simplex gradient} of~$\obj$ for the interpolation set~$\xpt$.
Such models are used for instance by \gls{cobyla}~\cite{Powell_1994}, a \gls{dfo} method for nonlinearly-constrained optimization detailed in \cref{subsec:cobyla}.

The problem of finding a quadratic model~$\objm \in \qpoly$ satisfying the interpolation system~\cref{eq:interpolation-conditions} when~$m = \dim \qpoly = (n + 1)(n + 2) / 2$ is very similar.
This kind of models are used for instance by \gls{uobyqa}~\cite{Powell_2002}, a \gls{dfo} method for unconstrained optimization detailed in \cref{subsec:uobyqa}.
The advantage of quadratic models is that they capture curvature information of the function~$\obj$, and are more precise than linear models.
In the remaining of this chapter, we focus our discussions on quadratic models.

\subsection{Lagrange polynomials}
\label{sec:lagrange-polynomials}

Throughout this section, we fix a poised interpolation set~$\xpt = \set{y^1, y^2, \dots, y^m} \subseteq \R^n$ of~$m = \dim \qpoly = (n + 1)(n + 2) / 2$ points.
We introduce the Lagrange polynomials for the interpolation problem
\begin{equation}
    \label{eq:interpolation-conditions-quadratic}
    \objm(y^i) = \obj(y^i), \quad \text{for~$i \in \set{1, 2, \dots, m}$ with~$\objm \in \qpoly$}.
\end{equation}

\begin{definition}[Lagrange polynomials for~\cref{eq:interpolation-conditions-quadratic}]
    \label{def:lagrange-polynomials}
    For each~$i \in \set{1, 2, \dots, m}$, the~$i$th Lagrange polynomial~$\lagp[i]$ for the interpolation problem~\cref{eq:interpolation-conditions-quadratic} is the unique quadratic polynomial that satisfies
    \begin{empheq}[left={\lagp[i](y^j) = \empheqlbrace}]{alignat*=1}
        & 1, ~ j = i,\\
        & 0, ~ j \in \set{1, 2, \dots, m} \setminus \set{i}.
    \end{empheq}
\end{definition}

We note that in \cref{def:lagrange-polynomials}, the poisedness of the interpolation set ensures the existence and the uniqueness of the Lagrange polynomials.
Further, it is well-known that the interpolant~$\objm \in \qpoly$ of~$\obj$ on~$\xpt$ can be formulated as a linear combinaison of the Lagrange polynomials, as detailed in \cref{thm:lagrange-polynomials-basis}

\begin{theorem}
    \label{thm:lagrange-polynomials-basis}
    The Lagrange polynomials~$\set{\lagp[1], \lagp[2], \dots, \lagp[m]}$ form a basis of~$\qpoly$.
    Moreover, the quadratic interpolant~$\objm$ of~$\obj$ on~$\xpt$ is given by
    \begin{equation*}
        \objm(\iter) = \sum_{i = 1}^m \obj(y^i) \lagp[i](\iter), \quad \text{for~$\iter \in \R^n$}.
    \end{equation*}
\end{theorem}

One of the most important application of the Lagrange polynomials is to measure the well-poisedness of the interpolation set~$\xpt$, as we discuss in the next section.

\section{Well-poisedness of interpolation sets}
\label{sec:poisedness}

As we mentioned above, the interpolation set~$\xpt$ is poised if it admits a unique interpolant in~$\qpoly$.
However, given a poised set, how good is this interpolation set?
This section briefly discusses this topic and introduce the concept of~$\Lambda$-poisedness.

Suppose that~$\obj$ is thrice differentiable on~$\R^n$.
It is well-known that for any~$\iter \in \R^n$, we have
\begin{equation}
    \label{eq:Ciarlet_Raviart_0}
    \abs{\obj(\iter) - \objm(\iter)} \le \frac{\theta}{6} \sum_{i = 1}^m \abs{\lagp[i](\iter)} \norm{x - y^i}^3,
\end{equation}
where~$\theta$ is an upper bound on the absolute value of the third-order directional derivatives of~$\obj$~\cite[Thm.~2]{Powell_2001}.
A similar bound is established in~\cite[Thm.~2]{Ciarlet_Raviart_1972}.
For any compact set~$\mathcal{C} \subseteq \R^n$, we then clearly have
\begin{equation}
    \label{eq:ciarlet-raviard-bound}
    \max_{\iter \in \mathcal{C}} @@ \abs{\obj(\iter) - \objm(\iter)} \le \frac{m \theta \Lambda}{6} \max_{1 \le i \le m} \max_{\iter \in \mathcal{C}} @@ \norm{x - y^i}^3,
\end{equation}
where~$\Lambda$ is defined by
\begin{equation}
    \label{eq:lambda-poisedness-convex-hull}
    \Lambda \eqdef \max_{1 \le i \le m} \max_{\iter \in \mathcal{C}} @@ \abs{\lagp[i](\iter)}.
\end{equation}
In this sense,~$\Lambda$ measures the well-poisedness of the interpolation set~$\xpt$ with respect to the set~$\mathcal{C}$.
This motivates the following concept of~$\Lambda$-poisedness.

\begin{definition}[$\Lambda$-poisedness~{\cite[Def.~3.6]{Conn_Scheinberg_Vicente_2009b}}]
    \label{def:lambda-poisedness}
    A poised interpolation set~$\xpt \subseteq \R^n$ is said to be~$\Lambda$-poised in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{1 \le i \le m} \max_{\iter \in \mathcal{C}} @@ \abs{\lagp[i](\iter)}.
    \end{equation*}
\end{definition}

Note that in this definition, the compact set~$\mathcal{C} \subseteq \R^n$ may or may not contain the interpolation points in~$\xpt$.
The compactness of~$\mathcal{C}$ can be relaxed if we take the supremum instead of the maximum over~$\mathcal{C}$.
If~$\xpt$ is~$\Lambda_0$-poised in~$\mathcal{C}$, it is obviously~$\Lambda$-poised in~$\mathcal{C}$ for any~$\Lambda \ge \Lambda_0$.
Alternative definitions of the~$\Lambda$-poisedness of sets are given in~\cite[\S~3.3]{Conn_Scheinberg_Vicente_2009b}.

As discussed in~\cite[\S~3.3]{Conn_Scheinberg_Vicente_2009b}, the~$\Lambda$-poisedness measures how good an interpolation set is.
A lower~$\Lambda$ indicates a better interpolation set.
Indeed, if~$\xpt$ is~$\Lambda$-poised in a compact set~$\mathcal{C} \subseteq \R^n$ and~$\Lambda$ is reasonably low, then~\cref{eq:ciarlet-raviard-bound} shows that the quadratic interpolant~$\objm$ represents~$\obj$ reasonably well in~$\mathcal{C}$.

The notion of~$\Lambda$-poisedness is intrinsically related to the well-conditioning of the interpolation system.
Note that the interpolation system~\cref{eq:interpolation-conditions-quadratic} is linear with respect to the coefficients of the interpolant~$\objm$.
One can show that the set~$\xpt$ is~$\Lambda$-poised if and only if the condition number of the coefficient matrix of this linear system is bounded by some terms proportional to~$\Lambda$~\cite[Thm.~3.14]{Conn_Scheinberg_Vicente_2009b}.
Therefore, the lower~$\Lambda$ is, the better the conditioning of the interpolation system is and hence, the better the interpolation set is from a numerical standpoint.

In practice, however, we do not use the~$\Lambda$-poisedness of interpolation sets directly.
This is because determining whether a set is~$\Lambda$-poised is a difficult problem, even if the compact set~$\mathcal{C} \subseteq \R^n$ is simple (e.g., a ball).
It is a theoretical tool that we mostly use to justify design choices of algorithms.

\section{Underdetermined quadratic interpolation}
\label{sec:underdetermined-interpolation}

As before, we focus on quadratic interpolation in this section.
Building a quadratic model by the interpolation system~\cref{eq:interpolation-conditions-quadratic} requires~$\mathcal{O}(n^2)$ function evaluations.
Therefore, if a \gls{dfo} method employs such models, its initialization already costs~$\mathcal{O}(n^2)$ function evaluations, which are needed to build the first model.
We use underdetermined interpolation to reduce this amount of function evaluations, as presented hereinafter.

Underdetermined interpolation works as follows.
Assume that we are given an interpolation set~$\xpt \subseteq \R^n$ such that the interpolation conditions~\cref{eq:interpolation-conditions} are consistent.
However, the number~$m$ of interpolation points may be lower than~$\dim \qpoly$.
In such a case, the interpolation conditions may \emph{not} define a unique interpolant.
To select one, we choose a functional~$\mathcal{F} : \qpoly \to \R$ that reflects a desired property or regularity of the interpolants.
An interpolant~$\objm$ of~$\obj$ is then defined as a solution of
\begin{subequations}
    \label{eq:underdetermined-models}
    \begin{align}
        \min        & \quad \mathcal{F}(Q)\\
        \text{s.t.} & \quad Q(y^i) = \obj(y^i), ~ i \in \set{1, 2, \dots, m}\\
                    & \quad Q \in \qpoly. \nonumber
    \end{align}
\end{subequations}
Two examples of the functional~$\mathcal{F}$ are given in \cref{subsec:least-frobenius-norm-models,subsec:symmetric-broyden-updates}.
If~$m = \dim \qpoly$ and if~$\xpt$ is poised in the sense of \cref{def:poisedness}, then the underdetermined interpolant reduces to the unique model that satisfies the interpolation conditions.

In the remaining of this section, we adapt the definitions and some properties presented in the previous section to special cases of underdetermined interpolation.

\subsection{Least Frobenius norm quadratic models}
\label{subsec:least-frobenius-norm-models}

One of the simplest functional~$\mathcal{F}$ mentioned above is the Frobenius norm of the Hessian matrix of the interpolants.
In this context, a quadratic model~$\objm$ solves
\begin{subequations}
    \label{eq:least-frobenius-norm-models}
    \begin{align}
        \min        & \quad \norm{\nabla^2 Q}_{\mathsf{F}}\\
        \text{s.t.} & \quad Q(y^i) = \obj(y^i), ~ i \in \set{1, 2, \dots, m} \label{eq:least-frobenius-norm-models-eq}\\
                    & \quad Q \in \qpoly, \nonumber
    \end{align}
\end{subequations}
where~$\norm{\cdot}_{\mathsf{F}}$ denotes the Frobenius norm.
This norm is chosen because it can be easily evaluated, unlike the spectral norm, for example.
The Frobenius norm also brings computational advantages because~\cref{eq:least-frobenius-norm-models} is essentially a quadratic programming problem, which can be tackled by solving a linear system.
Examples of \gls{dfo} methods that use least Frobenius norm quadratic models include \gls{mnh}~\cite{Wild_2008} and \gls{dfoalg}~\cite{Conn_Scheinberg_Toint_1997a,Conn_Scheinberg_Toint_1997b,Conn_Scheinberg_Toint_1998}.
We also assume hereafter that~$m \ge n + 2$ as otherwise, any solution to~\cref{eq:least-frobenius-norm-models} would be linear.

We first present the notion of poisedness for problem~\cref{eq:least-frobenius-norm-models} as follows.

\begin{definition}[Poisedness]
    \label{def:poisedness}
    The set~$\xpt$ is \emph{poised} in the minimum Frobenius norm sense if the solution to problem~\cref{eq:least-frobenius-norm-models} exists and is unique for any real-valued function~$\obj$.
\end{definition}

To get an in-depth view of the poisedness of~$\xpt$, let us investigate how to solve the problem~\cref{eq:least-frobenius-norm-models}.
Given any fix~$\bar{x} \in \R^n$, we expand a quadratic polynomial~$Q \in \qpoly$ for any~$\iter \in \R^n$ as
\begin{equation*}
    Q(\iter) = \alpha + g^{\T} (\iter - \bar{x}) + \frac{1}{2} (\iter - \bar{x})^{\T} H(\iter - \bar{x}),
\end{equation*}
where~$\alpha \in \R$,~$g \in \R^n$, and~$H \in \R^{n \times n}$ are the coefficients of~$Q$, with~$H$ being symmetric.
Note that the constraint~\cref{eq:least-frobenius-norm-models-eq} is linear with respect to~$(\alpha, g, H)$.
Consequently, the problem~\cref{eq:least-frobenius-norm-models} is a convex quadratic problem with equality constraints.
We then consider its \gls{kkt} system.
Let~$\lm = [\lm_1, \lm_2, \dots, \lm_m]^{\T}$ be the Lagrange multiplier of~\cref{eq:least-frobenius-norm-models}.
The stationarity condition~\cref{eq:kkt-introduction-stationarity} can be formulated as
\begin{equation}
    \label{eq:least-frobenius-norm-kkt}
    \sum_{i = 1}^m \lm_i (y^i - \bar{x}) (y^i - \bar{x})^{\T} = H, \quad \sum_{i = 1}^m \lm_i = 0, \quad \text{and} \quad \sum_{i = 1}^m \lm_i (y^i - \bar{x}) = 0.
\end{equation}
Combining~\cref{eq:least-frobenius-norm-kkt} with the primal feasibility condition~\cref{eq:kkt-introduction-primal-feasibility-eq}, we obtain
\begin{equation}
    \label{eq:least-frobenius-kkt-system}
    \begin{bmatrix}
        Y_H     & e & Y_L^{\T}\\
        e^{\T}  & 0 & 0\\
        Y_L     & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
        \lm\\
        \alpha\\
        g
    \end{bmatrix}
    =
    \begin{bmatrix}
        \obj(\xpt)\\
        0\\
        0
    \end{bmatrix},
\end{equation}
where~$\obj(\xpt) \eqdef [\obj(y^1), \obj(y^2), \dots, \obj(y^m)]^{\T}$,~$e \in \R^m$ is the all-one vector, while~$Y_L \in \R^{n \times m}$ and~$Y_H \in \R^{m \times m}$\nomenclature[Sg]{$\R^{m \times n}$}{Real matrix space of dimension~$m \times n$} are defined by
\begin{equation}
    \label{eq:least-frobenius-kkt-system-matrices}
    Y_L \eqdef 
    \begin{bmatrix}
        y^1 - \bar{x}   & y^2 - \bar{x} & \cdots    & y^m - \bar{x}
    \end{bmatrix} \quad \text{and} \quad Y_H \eqdef \frac{1}{2} (Y_L^{\T} Y_L) \odot (Y_L^{\T} Y_L).
\end{equation}
\nomenclature[Oi]{$\odot$}{Hadamard product}%
In~\cref{eq:least-frobenius-kkt-system-matrices},~$\odot$ denotes the Hadamard product and hence, the~$(i, j)$th entry of~$Y_H$ is given by~$[(y^i - \bar{x})^{\T} (y^j - \bar{x})]^2 / 2$.
In fact, the problem~\cref{eq:least-frobenius-norm-models} is equivalent to the linear system~\cref{eq:least-frobenius-kkt-system}, where the matrix~$H$ can be retrieved from the value of~$\lm$ by~\cref{eq:least-frobenius-norm-kkt}~\cite[\S~2]{Powell_2004b}, and the interpolation set~$\xpt$ is poised if and only if the coefficient matrix in~\cref{eq:least-frobenius-kkt-system} is nonsingular~\cite[\S~5.3]{Conn_Scheinberg_Vicente_2009b}.
Since the poisedness of~$\xpt$ does not depends on the choice of~$\bar{x}$, neither does the nonsingularity of the coefficient matrix in~\cref{eq:least-frobenius-kkt-system}.

Any interpolation set that is poised in the sense of \cref{def:poisedness} must be affinely independent, i.e., it does not lie in a low-dimensional affine subset of~$\R^n$, because the matrix~$[e, Y_L^{\T}]$ has full-column rank according to the nonsingularity of the coefficient matrix in~\cref{eq:least-frobenius-kkt-system}.
Even if~$\xpt$ is not poised, the full-rankness of~$[e, Y_L^{\T}]$ holds as long as the solution to~\cref{eq:least-frobenius-norm-models} is unique when~$f \equiv 0$.
This is because~$\alpha + g^{\T} (\iter - \bar{x})$ is a nonzero linear polynomial that interpolates the zero function on~$\xpt$ if~$[\alpha, g^{\T}]^{\T} \in \R^{n + 1}$ is a nonzero vector in~$\ker([e, Y_L^{\T}])$.
Such a polynomial should not exist because of the aforementioned uniqueness, since the zero polynomial should be the only solution.

\subsubsection{Minimum Frobenius norm Lagrange polynomials}

Let~$\xpt \subseteq \R^n$ be a poised interpolation set of~$m$ points in the minimum Frobenius norm sense, with~$n + 2 \le m \le \dim \qpoly$.
We now present the minimum Frobenius norm Lagrange polynomials for the interpolation system~\cref{eq:interpolation-conditions-quadratic}.

\begin{definition}[Minimum Frobenius norm Lagrange polynomials for~\cref{eq:interpolation-conditions-quadratic}]
    \label{def:lagrange-polynomials-minimum-norm}
    For each~$i \in \set{1, 2, \dots, m}$, the~$i$th minimum Frobenius norm Lagrange polynomial~$\lagp[i]$ for the interpolation problem~\cref{eq:interpolation-conditions-quadratic} is the unique quadratic polynomial that solves
    \begin{align*}
        \min        & \quad \norm{\nabla^2 Q}_{\mathsf{F}}\\
        \text{s.t.} & \quad Q(y^i) = 1,\\
                    & \quad Q(y^j) = 0, ~ j \in \set{1, 2, \dots, m} \setminus \set{i},\\
                    & \quad Q \in \qpoly.
    \end{align*}
\end{definition}

As in the case where~$m = \dim \qpoly$, any minimum Frobenius norm interpolant can be expressed as a linear combinaison of minimum Frobenius norm Lagrange polynomials, as described by \cref{thm:lagrange-polynomials-basis-minimum-norm} (see~\cite[\S~3]{Powell_2004b}), even though the set of all minimum Frobenius norm Lagrange polynomials is not a basis of~$\qpoly$ if~$m < \dim \qpoly$.

\begin{theorem}
    \label{thm:lagrange-polynomials-basis-minimum-norm}
    The minimum Frobenius norm quadratic interpolant~$\objm$ of~$\obj$ on~$\xpt$ is given for any~$\iter \in \R^n$ by
    \begin{equation*}
        \objm(\iter) = \sum_{i = 1}^m \obj(y^i) \lagp[i](\iter).
    \end{equation*}
\end{theorem}

\subsubsection{Poisedness of interpolation sets in the minimum Frobenius norm sense}

We now present the notion of~$\Lambda$-poisedness in the minimum Frobenius norm sense, which is a straightforward adaptation of \cref{def:lambda-poisedness}.

\begin{definition}[$\Lambda$-poisedness in the minimum Frobeninus norm sense~{\cite[Def.~5.6]{Conn_Scheinberg_Vicente_2009b}}]
    \label{def:lambda-poisedness-minimum-norm}
    A poised interpolation set~$\xpt \subseteq \R^n$ in the minimum Frobenius norm sense is said to be~$\Lambda$-poised in the minimum Frobenius norm sense in a compact set~$\mathcal{C} \subseteq \R^n$, for some~$\Lambda > 0$, if
    \begin{equation*}
        \Lambda \ge \max_{1 \le i \le m} \max_{\iter \in \mathcal{C}} @@ \abs{\lagp[i](\iter)}.
    \end{equation*}
\end{definition}

Similarly to what we mentioned before, if~$\xpt$ is~$\Lambda_0$-poised in the minimum Frobenius norm sense in~$\mathcal{C}$, it is obviously~$\Lambda$-poised in the minimum Frobenius norm sense in~$\mathcal{C}$ for all~$\Lambda \ge \Lambda_0$.
Moreover, one can show that the set~$\xpt$ is~$\Lambda$-poised if and only if the condition number of the coefficient matrix in~\cref{eq:least-frobenius-kkt-system} is bounded by some terms proportional to~$\Lambda$~\cite[Thm.~5.8]{Conn_Scheinberg_Vicente_2009b}.

\subsection{Quadratic models based on the symmetric Broyden update}
\label{subsec:symmetric-broyden-updates}

\index{\glsfmtshort{psb}!derivative-free|(}In this section, we present another functional~$\mathcal{F}$ that can be used in the variational problem~\cref{eq:underdetermined-models}.
It is introduced by \citeauthor{Powell_2004b}~\cite{Powell_2004b}.
It takes up the freedom bequeathed by the interpolation conditions~\cref{eq:interpolation-conditions-quadratic} by minimizing the difference between the Hessian matrix of the interpolant and a prior estimation~$H_{\text{old}} \in \R^{n \times n}$ of this matrix instead of minimizing the Frobenius norm of the Hessian matrix.
More specifically, we build a quadratic model~$\objm$ by solving
\begin{subequations}
    \label{eq:symmetric-broyden-models}
    \begin{align}
        \min        & \quad \norm{\nabla^2 Q - H_{\text{old}}}_{\mathsf{F}}\\
        \text{s.t.} & \quad Q(y^i) = \obj(y^i), ~ i \in \set{1, 2, \dots, m},\\
                    & \quad Q \in \qpoly.
    \end{align}
\end{subequations}
In a \gls{dfo} method, the model~$\objm[k] \in \qpoly$ of~$\obj$ at the~$k$th iteration can be constructed by solving~\cref{eq:symmetric-broyden-models} with~$H_{\text{old}} = \nabla^2 \objm[k - 1]$, and we define~$\objm[-1] \equiv 0$.
This is referred to as the \emph{least Frobenius norm updating of quadratic models}~\cite{Powell_2004b}.
Examples of \gls{dfo} methods that use this update include \gls{newuoa}~\cite{Powell_2006}, \gls{bobyqa}~\cite{Powell_2009}, and~\gls{lincoa}, presented in \cref{subsec:newuoa-bobyqa-lincoa}.
The new method we introduce in \cref{ch:cobyqa-introduction} is also based on this update.

The above discussions we had on the minimum Frobenius norm models can be easily adapted to the least Frobenius norm updating.
If we are given a function~$\objm[\text{old}]$ such that~$H_{\text{old}} = \nabla^2 \objm[\text{old}]$, then~$\objm - \objm[\text{old}]$ solves
\begin{subequations}
    \label{eq:symmetric-broyden-models-reformulation}
    \begin{align}
        \min        & \quad \norm{\nabla^2 Q}_{\mathsf{F}}\\
        \text{s.t.} & \quad Q(y^i) = \obj(y^i) - \objm[\text{old}](y^i), ~ i \in \set{1, 2, \dots, m},\\
                    & \quad Q \in \qpoly.
    \end{align}
\end{subequations}
In a \gls{dfo} method, we have~$\objm[\text{old}] = \objm[k - 1]$.
Therefore, the coefficients of the model obtained by least Frobenius norm updates can be evaluated by solving a linear system similar to~\cref{eq:least-frobenius-kkt-system}.

The minimum Frobenius norm update can be regarded as a derivative-free variation of the \gls{psb} update in the quasi-Newton method.
The \gls{psb}\index{\glsfmtshort{psb}} method~\cite{Powell_1970b} approximates the function~$\obj$ at a point~$y^1 \in \R^n$ by a quadratic function
\begin{align*}
    \objm(\iter) = \objm(y^1) + \nabla \objm(y^1)^{\T} (\iter - y^1) + \frac{1}{2} (\iter - y^1)^{\T} \hat{H} (\iter - y^1),
\end{align*}
and, as shown in~\cite[Thm.~4.2]{Dennis_Schnabel_1979},~$\hat{H} \in \R^{n \times n}$ is the unique solution to
\begin{align*}
    \min        & \quad \norm{H - H_{\text{old}}}_{\mathsf{F}}\\
    \text{s.t.} & \quad H (y^1 - y^2) = \nabla \objm(y^1) - \nabla \objm(y^2),\\
                & \quad H^{\T} = H,\\
                & \quad H \in \R^{n \times n},
\end{align*}
where~$H_{\text{old}} \in \R^{n \times n}$ is a prior estimation of~$\nabla^2 \obj(y^1)$, and~$y^2 \in \R^n$ is a point that differs from~$y^1$.
In an optimization context,~$y^1$ and~$y^2$ are respectively the current and the previous iterates of the optimization method.
Clearly, the function~$\objm$ solves
\begin{subequations}
    \label{eq:psb-variational-problem}
    \begin{align}
        \min        & \quad \norm{\nabla^2 Q - H_{\text{old}}}_{\mathsf{F}}\\
        \text{s.t.} & \quad Q(y^1) = \obj(y^1),\\
                    & \quad \nabla Q(y^i) = \nabla \obj(y^i), ~ i \in \set{1, 2},\\
                    & \quad Q \in \qpoly.
    \end{align}
\end{subequations}
The variational problem~\cref{eq:symmetric-broyden-models} is a derivative-free analog of~\cref{eq:psb-variational-problem}.
Therefore, following \citeauthor{Powell_2013}~\cite{Powell_2013}, we will also refer to the minimum Frobenius norm update as the \emph{derivative-free symmetric Broyden update}.\index{\glsfmtshort{psb}!derivative-free|)}

\subsection{Implementation in \glsfmtlong{dfo} methods}
\label{subsec:implementation-symmetric-broyden-update}

We consider in this section a model-based \gls{dfo} algorithm that builds models by the derivative-free symmetric Broyden update, and we summarize the implementation of this update, as proposed by \citeauthor{Powell_2004b}~\cite{Powell_2004b,Powell_2004c}.
The implementation can straightforwardly be adapted to construct minimum Frobenius norm models.

For each~$k \ge 0$, we denote by~$\iter[k] \in \R^n$ and the~$k$th iterate, and the~$k$th interpolation set is~$\xpt[k] = \set{y^{k, 1}, y^{k, 2}, \dots, y^{k, m}} \subseteq \R^n$, with~$\iter[k] \in \xpt[k]$.
The \gls{dfo} algorithm builds a quadratic model~$\objm[k] \in \qpoly$ of the objective function~$\obj$ by the derivative-free symmetric Broyden update~\cref{eq:symmetric-broyden-models}, with~$H_{\text{old}} = \nabla^2 \objm[k - 1]$ and~$\xpt = \xpt[k]$.
The model~$\objm[k]$ is
\begin{equation*}
    \objm[k](\iter) = \alpha^k + (g^k)^{\T} (\iter - \iter[k]) + \frac{1}{2} (\iter - \iter[k])^{\T} H^k (\iter - \iter[k]), \quad \text{for~$\iter \in \R^n$},
\end{equation*}
where~$\alpha^k \in \R$,~$g^k \in \R^n$, and~$H^k \in \R^{n \times n}$ are the coefficients,~$H^k$ being symmetric.
As discussed in \cref{subsec:least-frobenius-norm-models}, thanks to~\cref{eq:symmetric-broyden-models-reformulation},~$\alpha^k$,~$g^k$, and~$H^k$ can be calculated by solving
\begin{equation}
    \label{eq:least-frobenius-kkt-system-2}
    \begin{bmatrix}
        Y_H     & e & Y_L^{\T}\\
        e^{\T}  & 0 & 0\\
        Y_L     & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
        \lm\\
        \alpha\\
        g
    \end{bmatrix}
    =
    \begin{bmatrix}
        v\\
        0\\
        0
    \end{bmatrix},
\end{equation}
where~$v \in \R^m$ is defined by
\begin{equation*}
    v \eqdef
    \begin{bmatrix}
        \obj(y^{k, 1}) - \objm[k - 1](y^{k, 1})\\
        \obj(y^{k, 2}) - \objm[k - 1](y^{k, 2})\\
        \vdots\\
        \obj(y^{k, m}) - \objm[k - 1](y^{k, m})
    \end{bmatrix}.
\end{equation*}
If~$\lm$,~$\alpha$, and~$g$ solve the system~\cref{eq:least-frobenius-kkt-system-2}, then the model's coefficients can be evaluated by
\begin{subequations}
    \begin{empheq}[left=\empheqlbrace]{alignat=1}
        & \alpha^k = \objm[k - 1](\iter[k]) + \alpha,\\
        & g^k = \nabla \objm[k - 1](\iter[k]) + g,\\
        & H^k = \nabla^2 \objm[k - 1] + \sum_{i = 1}^m \lm_i (y^{k, i} - \iter[k]) (y^{k, i} - \iter[k])^{\T}. \label{eq:derivative-free-symmetric-broyden-hessian}
    \end{empheq}
\end{subequations}

The methods we consider in this thesis build~$\xpt[k + 1]$ from~$\xpt[k]$ by modifying only one point.
Taking advantage of this fact, we can reduce the computational cost of these models and improve the stability of the computations, as is presented in the sequel.

\subsubsection{Storage and update of the quadratic model}

The intercept~$\alpha^k$ and the gradient vector~$g^k$ of the model~$\objm[k]$ can be stored directly.
However, forming its Hessian matrix~$H^k$ using~\cref{eq:derivative-free-symmetric-broyden-hessian} requires making~$\bigo(mn^2)$ basic operations.
To reduce this cost, as suggested by \citeauthor{Powell_2004b}~\cite[\S~3]{Powell_2004b}, we decompose the Hessian matrix as
\begin{equation}
    \label{eq:hessian-explicit-implicit}
    H^k = \Gamma^k + \sum_{i = 1}^m \gamma_i^k (y^{k, i} - \iter[k]) (y^{k, i} - \iter[k])^{\T},
\end{equation}
where~$\Gamma^k \in \R^{n \times n}$ is referred to as an explicit part of~$H^k$, and~$[\gamma_1^k, \gamma_2^k, \dots, \gamma_m^k]^{\T} \in \R^m$ is referred to as an implicit part of~$H^k$, which are both stored.
Recall that~$\xpt[k + 1]$ differs from~$\xpt[k]$ by only one point.
Suppose that~$y^{k + 1, i} = y^{k, i}$ for all~$i$ except for~$i = \ell$.
The explicit and implicit parts of~$H^k$ can then be updated as
\begin{subequations}
    \label{eq:hessian-explicit-implicit-update}
    \begin{empheq}[left=\empheqlbrace]{alignat=1}
        & \Gamma^{k + 1} = \Gamma^k + \gamma_{\ell}^k (y^{k, \ell} - \iter[k]) (y^{k, \ell} - \iter[k])^{\T},\\
        & \gamma_{i}^{k + 1} = \gamma_{i}^k + \lm_i, ~i \in \set{1, 2, \dots, m} \setminus \set{\ell},\\
        & \gamma_{\ell}^{k + 1} = \lm_{\ell}.
    \end{empheq}
\end{subequations}
In doing so, updating a quadratic model requires only~$\bigo(n^2)$ basic operations, after solving the linear system~\cref{eq:least-frobenius-kkt-system-2}.

\citeauthor{Powell_2006} applied~\cref{eq:hessian-explicit-implicit} and~\cref{eq:hessian-explicit-implicit-update} in his three recent solvers \gls{newuoa}~\cite{Powell_2006}, \gls{bobyqa}~\cite{Powell_2009}, and~\gls{lincoa}~\cite{Powell_2015}, presented in \cref{subsec:newuoa-bobyqa-lincoa}.
The new method we introduce in \cref{ch:cobyqa-introduction} also uses this technique.

\subsubsection{Storage and update of the inverse of the coefficient matrix}

Since the methods we consider build~$\xpt[k + 1]$ from~$\xpt[k]$ by changing only one point, only one row and one column of the coefficient matrix in~\cref{eq:least-frobenius-kkt-system-2} is modified from an iteration to another, which is a rank-$2$ modification of this matrix.
Therefore, we can update its inverse matrix using the Sherman-Morrison-Woodbury formula from an iteration to another.
Such an update requires only~$\bigo(n^2)$ operations, much less than what would be required to compute the inverse from scratch (see~\cite[\S~2]{Powell_2004c} for details).

Suppose that the above-mentioned inverse is
\begin{equation*}
    \begin{bmatrix}
        \Omega  & \Xi^{\T}\\
        \Xi     & \Upsilon
    \end{bmatrix},
\end{equation*}
where~$\Omega \in \R^{m \times m}$,~$\Xi \in \R^{(n + 1) \times m}$, and~$\Upsilon \in \R^{(n + 1) \times (n + 1)}$.
The submatrices~$\Xi$ and~$\Upsilon$ can be stored directly, although the first row of~$\Xi$ can be discarded, as well as the first row and the first column of~$\Upsilon$ (see~\cite[\S~4]{Powell_2006}).
However,~$\Omega$ should not be stored as is, due to the following reason.
As mentioned by \citeauthor{Powell_2004c}~\cite{Powell_2004c}, the submatrix~$\Omega$ is positive semidefinite with rank~$m - n - 1$.
If~$\Omega$ is stored as an~$m \times m$ matrix, the rank property may be lost in practice due to computer rounding errors, as observed by \citeauthor{Powell_2004c} during the development of \gls{newuoa} (see~\cite[\S~1]{Powell_2004c} and the last paragraph of~\cite[\S~8]{Powell_2006}).
To maintain the structure of~$\Omega$, we store the factorized form~$\Omega = Z D Z^{\T}$, with~$Z \in \R^{m \times (m - n - 1)}$ and~$D \in \R^{(m - n - 1) \times (m - n - 1)}$, where~$D$ is a diagonal matrix with elements~$\pm 1$ on the diagonal.
Theoretically, the matrix~$D$ should always be the identity.
However, in practice,~$D$ may have negative diagonal entries.
This is because the positive semidefiniteness of~$\Omega$ can be lost due to computer rounding errors.

\section{An optimal interpolation set}
\label{sec:optimal-interpolation-set}

We study in this section an interpolation set that we will use in \cref{ch:cobyqa-introduction} of this thesis, where we introduce a new model-based \gls{dfo} method.
This interpolation set is adapted from~\cite{Powell_2001} as follows.
Let~$\delta > 0$ be fixed and for~$j \in \set{1, 2, \dots, 2n + 1}$, let~$z^j \in \R^n$ be
\begin{subequations}
    \label{eq:initial-interpolation-set}
    \begin{empheq}[left={z^j \eqdef \empheqlbrace}]{alignat=2}
        & 0,                        && \quad \text{if~$j = 1$,}\\
        & \delta e_{j - 1},         && \quad \text{if~$2 \le j \le n + 1$,}\\
        & -\delta e_{j - n - 1},    && \quad \text{otherwise,}
    \end{empheq}
\end{subequations}
where~$e_j \in \R^n$ denotes the~$j$th standard coordinate vector.
We then define the interpolation set~$\mathcal{Z}_m \subseteq \R^n$ for each~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ by
\begin{equation}
    \label{eq:initial-interpolation-set-def}
    \mathcal{Z}_m \eqdef \set{z^1, z^2, \dots, z^m}.
\end{equation}
Further, we denote by~$\ball[p][m]$ the smallest~$\ell_p$-norm ball containing~$\mathcal{Z}_m$, for~$p \in [1, \infty]$.
Note that we allow~$p = \infty$.
By the construction of~$\mathcal{Z}_m$, we observe that
\begin{equation}
    \label{eq:ball-initial}
    \ball[p][m] \equiv \ball[p](\delta) \eqdef \set{\iter \in \R^n : \norm{x}_p \le \delta}.
\end{equation}

In the sequel, we study the~$\Lambda$-poisedness of the set~$\mathcal{Z}_m$ in~$\ball[p][m]$ for~$p \ge 1$.

We will show that this set is~$1$-poised in~$\ball[p][m]$ for all~$p \in [1, 2]$ when~$m = 2n + 1$.
In this sense,~$\mathcal{Z}_{2n + 1}$ is an optimal interpolation set.

\subsection{Formulation of the Lagrange polynomials}

According to \cref{def:lambda-poisedness-minimum-norm} and the equation~\cref{eq:ball-initial}, the set~$\mathcal{Z}_m$ is~$\Lambda_p$-poised in~$\ball[p][m]$ in the minimum Frobenius norm sense with
\begin{equation*}
    \Lambda_p \eqdef \max_{1 \le i \le m} \max_{\iter \in \ball[p](\delta)} @@ \abs{\lagp[i](\iter)},
\end{equation*}
where~$\lagp[i]$ is the~$i$th minimum Frobenius norm Lagrange polynomial associated with~$\mathcal{Z}_m$ for~$i \in \set{1, 2, \dots, m}$.

To study~$\Lambda_p$, we first present explicit formulae for~$\lagp[i]$ for all~$i \in \set{1, 2, \dots, m}$.
These formulae are given in~\cite[\S~3]{Powell_2006}, without a proof.

\begin{lemma}
    \label{lem:lagrange-polynomials-initial}
    For each~$m \in \set{n + 2, n + 3, \dots,  2n + 1}$ and all~$\iter \in \R^n$, we have
    \begin{empheq}[left={\lagp[i](\iter) = \empheqlbrace}]{alignat*=2}
        & 1 - \delta^{-2} \sum_{j = 1}^{m - n - 1} x_j^2 - \delta^{-1} \sum_{j = m - n}^n x_j,  && \quad \text{if~$i = 1$,}\\
        & (\sqrt{2} \delta)^{-2} x_{i - 1}^2 + (2 \delta)^{-1} x_{i - 1},                       && \quad \text{if~$2 \le i \le m - n$,}\\
        & \delta^{-1} x_{i - 1},                                                                && \quad \text{if~$m - n + 1 \le i \le n + 1$,}\\
        & (\sqrt{2} \delta)^{-2} x_{i - 1}^2 - (2 \delta)^{-1} x_{i - 1}.                       && \quad \text{otherwise.}
    \end{empheq}
    In the formulation of~$\lagp[1]$, if~$m = 2n + 1$, we define~$\sum_{j = m - n}^n x_j = 0$.
\end{lemma}

\begin{proof}
    Let~$i \in \set{1, 2, \dots, m}$ be fixed and let~$\lagp$ be a quadratic polynomial that satisfies
    \begin{subequations}
        \label{eq:lagrange-polynomials-initial-proof}
        \begin{empheq}[left={\lagp(z^j) = \empheqlbrace}]{alignat=2}
            & 1,    && \quad \text{if~$j = i$,}\\
            & 0,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
    First, it is straightforward to verify that~$\lagp[i]$ satisfies the interpolation conditions~\cref{eq:lagrange-polynomials-initial-proof}.
    Hence, it suffices to show that the Frobenius norm of its Hessian matrix is least.
    According to the equation~\cref{eq:initial-interpolation-set}, for any~$j \in \set{1, 2, \dots, m - n - 1}$, we have~$z^1 = 0$,~$z^{j + 1} = \delta e_j$, and~$z^{n + j + 1} = - \delta e_j$.
    Therefore,
    \begin{empheq}[left=\empheqlbrace]{alignat*=1}
        & \lagp(z^{j + 1}) = \lagp(z^1) + \delta \nabla \lagp(z^1)^{\T} e_j + \frac{\delta^2}{2} e_j^{\T} (\nabla^2 \lagp) e_j,\\
        & \lagp(z^{n + j + 1}) = \lagp(z^1) - \delta \nabla \lagp(z^1)^{\T} e_j + \frac{\delta^2}{2} e_j^{\T} (\nabla^2 \lagp) e_j,
    \end{empheq}
    and hence,
    \begin{equation*}
        e_j^{\T} (\nabla^2 \lagp) e_j = \frac{\lagp(z^{j + 1}) + \lagp(z^{n + j + 1}) - 2 \lagp(z^1)}{\delta^2}.
    \end{equation*}
    This fixes the first~$m - n - 1$ diagonal entries of~$\nabla^2 \lagp$, which are exactly those of~$\nabla^2 \lagp[i]$.
    Since all the other entries of~$\nabla^2 \lagp[i]$ are zero, we have
    \begin{equation*}
        \norm{\nabla^2 \lagp[i]}_{\mathsf{F}}^2 \le \norm{\nabla^2 \lagp}_{\mathsf{F}}^2,
    \end{equation*}
    which completes the proof.
\end{proof}
\nomenclature[Nc]{$\qedsymbol$}{Halmos symbol}%

\subsection{Bounds for the~\texorpdfstring{$\Lambda$}{\textLambda}-poisedness}

The next lemma simplifies the value of~$\Lambda_p$ for further computations.

\begin{lemma}
    \label{lem:lambda-poisedness-initial-simple}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation}
        \label{eq:lambda-poisedness-initial-simple}
        \Lambda_p = \max_{\iter \in \ball[p](\delta)} @@ \abs{\lagp[1](\iter)}.
    \end{equation}
\end{lemma}

\begin{proof}
    For each~$i \in \set{2, 3, \dots, n + 1}$, according to~\cref{lem:lagrange-polynomials-initial},~$\lagp[i](\iter)$ depends only on~$x_{i - 1}$ for all~$\iter \in \R^n$, and hence
    \begin{equation*}
        \max_{\iter \in \ball[p](\delta)} @@ \abs{\lagp[i](\iter)} = \max_{t \in [-\delta, \delta]} @@ \abs{\lagp[i](t e_{i - 1})} = 1.
    \end{equation*}
    Similarly, for each~$i \in \set{n + 2, n + 3, \dots, m}$, since~$\lagp[i](\iter)$ depends only on~$x_{i - n - 1}$ for all~$\iter \in \R^n$, we have
    \begin{equation*}
        \max_{\iter \in \ball[p](\delta)} @@ \abs{\lagp[i](\iter)} = \max_{t \in [-\delta, \delta]} @@ \abs{\lagp[i](t e_{i - n - 1})} = 1.
    \end{equation*}
    Noting that~$\lagp[1](z^1) = 1$ and~$z^1 \in \ball[p](\delta)$, we thus have
    \begin{equation*}
        \Lambda_p = \max_{\iter \in \ball[p](\delta)} @@ \abs{\lagp[1](\iter)}.
    \end{equation*}
\end{proof}

We are now equiped to develop bounds on~$\Lambda_p$ in the general case.

\begin{theorem}
    \label{thm:lambda-poisedness-initial}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$ and any~$p \in [1, \infty]$, we have
    \begin{equation*}
        1 + (2n + 1 - m)^{\frac{p - 1}{p}} \le \Lambda_p \le n,
    \end{equation*}
    where we assume that~$0^0 = 0$ and that the lower bound is~$2n + 2 - m$ for~$p = \infty$.
\end{theorem}

\begin{proof}
    We will establish the bounds using the formulation of~$\Lambda_p$ in~\cref{lem:lambda-poisedness-initial-simple}.
    For the lower bound, by considering only the points in~$\R^n$ whose leading~$m - n - 1$ components are zeros and whose remaining~$2n + 1 - m$ components are equal, we have
    \begin{equation*}
        \Lambda_p \ge \max_{t \in \R} @@ \set{1 - \delta^{-1} (2n + 1 - m) t : (2n + 1 - m) \abs{t}^p \le \delta^p} = 1 + (2n + 1 - m)^{\frac{p - 1}{p}}.
    \end{equation*}
    
    We now prove the upper bound.
    Note that for any~$p \ge 1$, we have~$\ball[p](\delta) \subseteq \ball[\infty](\delta)$, so that~$\Lambda_p \le \Lambda_{\infty}$.
    Therefore, we only need to show that~$\Lambda_{\infty} \le n$.
    Considering both~$\lagp[1]$ and~$-\lagp[1]$, we obtain
    \begin{equation*}
        \Lambda_{\infty} = \max_{\iter \in \ball[\infty](\delta)} @@ \abs{\lagp[1](\iter)} = \max @@ \set{2n + 2 - m, n - 1} \le n.
    \end{equation*}
\end{proof}

\subsection{Some special cases}

We now calculate the exact value of~$\Lambda_p$ in some special cases.

\begin{proposition}
    \label{prop:lambda-poisedness-1}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{subequations}
        \label{eq:lambda-poisedness-1}
        \begin{empheq}[left={\Lambda_1 = \empheqlbrace}]{alignat=2}
            & 2,    && \quad \text{if~$n + 2 \le m \le 2n$,}\\
            & 1,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
\end{proposition}

\begin{proof}
    According to~\cref{thm:lambda-poisedness-initial},~$\Lambda_1$ is lower bounded by the right-hand side of~\cref{eq:lambda-poisedness-1}.
    Therefore, we only need to prove that this right-hand side is also a lower bound for~$\Lambda_1$, using the formulation in \cref{lem:lambda-poisedness-initial-simple}.

    For any~$\iter \in \ball[1](\delta)$, we have
    \begin{equation*}
        \lagp[1](\iter) \le 1 - \frac{1}{\delta} \sum_{j = m - n}^n x_j \le 1 + \frac{1}{\delta} \sum_{j = m - n}^n \abs{x_j}.
    \end{equation*}
    Therefore,
    \begin{subequations}
        \label{eq:eq:lambda-poisedness-1-proof-1}
        \begin{empheq}[left={\lagp[1](\iter) \le \empheqlbrace}]{alignat=2}
            & 2,    && \quad \text{if~$n + 2 \le m \le 2n$,}\\
            & 1,    && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
    On the other hand,
    \begin{subequations}
        \label{eq:eq:lambda-poisedness-1-proof-2}
        \begin{align}
            \lagp[1](\iter) &= 1 - \sum_{j = 1}^{m - n - 1} \frac{x_j^2}{\delta^2} - \sum_{j = m - n}^n \frac{x_j}{\delta}\\
                            &\ge 1 - \sum_{j = 1}^{m - n - 1} \frac{\abs{x_j}}{\delta} - \sum_{j = m - n}^n \frac{\abs{x_j}}{\delta}\\
                            &\ge 1 - \frac{\norm{x}_1}{\delta} \ge 0.
        \end{align}
    \end{subequations}
    We conclude the proof by combining~\cref{eq:eq:lambda-poisedness-1-proof-1,eq:eq:lambda-poisedness-1-proof-2} with~\cref{lem:lambda-poisedness-initial-simple}.
\end{proof}

\begin{proposition}
    \label{prop:lambda-poisedness-2}
    For any~$m \in \set{n + 2, n + 3, \dots, 2n + 1}$, we have
    \begin{equation}
        \label{eq:lambda-poisedness-2}
        \Lambda_2 = 1 + \sqrt{2n + 1 - m}.
    \end{equation}
\end{proposition}

\begin{proof}
    If~$m = 2n + 1$, \cref{lem:lagrange-polynomials-initial} tells us that~$\lagp[1](\iter) = 1 - \delta^{-2} \norm{x}_2^2$ for~$\iter \in \ball[2](\delta)$.
    Therefore, \cref{lem:lambda-poisedness-initial-simple} directly provides the desired result~$\Lambda_2 = 1$.
    We now focus on the case with~$n + 2 \le m < 2n + 1$.

    According to~\cref{thm:lambda-poisedness-initial},~$\Lambda_2$ is lower bounded by the right-hand side of~\cref{eq:lambda-poisedness-2}.
    Therefore, we only need to prove that this right-hand side is also an upper bound for~$\Lambda_2$, using the formulation in \cref{lem:lambda-poisedness-initial-simple}.
    
    For any~$\iter \in \ball[2](\delta)$, we have
    \begin{subequations}
        \begin{align}
            \lagp[1](\iter) &= 1 - \frac{1}{\delta^2} \sum_{j = 1}^{m - n - 1} x_j^2 - \frac{1}{\delta} \sum_{j = m - n}^n x_j\\
                            &\ge 1 - \frac{1}{\delta^2} \bigg( \delta^2 - \sum_{j = m - n}^n x_j^2 \bigg) - \frac{1}{\delta} \sum_{j = m - n}^n x_j = \sum_{j = m - n}^n \frac{x_j}{\delta} \bigg( 1 - \frac{x_j}{\delta} \bigg)\\
                            &\ge \min_{y \in \ball[2](1)} @@ \sum_{j = m - n}^n y_j (1 - y_j). \label{eq:lambda-poisedness-initial-2-proof}
        \end{align}
    \end{subequations}
    Let~$y^{\ast} \in \ball[2](1)$ be a minimizer in~\cref{eq:lambda-poisedness-initial-2-proof}.
    \Cref{thm:first-order-necessary-conditions} ensures that there exists a Lagrange multiplier~$\lm[\ast] \ge 0$ such that~$1 - 2 y_j^{\ast} + 2 \lm[\ast] y_j^{\ast} = 0$ for all~$j \in \set{m - n, \dots, n}$.
    Therefore, the last~$(2n + 1 - m)$ components of~$y^{\ast}$ are equal, and hence,
    \begin{align*}
        \lagp[1](\iter) \ge \min_{y \in \ball[2](1)} @@ \sum_{j = m - n}^n y_j (1 - y_j)    &= \min_{t \in \R} @@ \set{(2n + 1 - m) t (1 - t) : (2n + 1 - m) t^2 \le 1}\\
                                                                                            &= -1 - \sqrt{2n + 1 - m}.
    \end{align*}

    Furthermore,
    \begin{equation*}
        \lagp[1](\iter) \le 1 + \sum_{j = m - n}^n \frac{\abs{x_j}}{\delta} \le 1 + \sqrt{2n + 1 - m} \sum_{j = m - n}^n \frac{x_j^2}{\delta^2} \le 1 + \sqrt{2n + 1 - m}.
    \end{equation*}
    Therefore,~$\abs{\lagp[1](\iter)} \le 1 + \sqrt{2n + 1 - m}$ and hence, according to~\cref{lem:lambda-poisedness-initial-simple,thm:lambda-poisedness-initial}, we have
    \begin{equation*}
        \Lambda_2 = \max_{\iter \in \ball[2](\delta)} @@ \abs{\lagp[1](\iter)} = 1 + \sqrt{2n + 1 - m},
    \end{equation*}
    which concludes the proof.
\end{proof}

Before developing the last special value of~$\Lambda_p$ we introduce the following lemma.

\begin{lemma}
    \label{lem:max-norm-pq}
    For any~$p \ge 1$ and~$q \ge 1$, we have
    \begin{subequations}
        \label{eq:max-norm-pq}
        \begin{empheq}[left={\max\limits_{\iter \in \ball[q](1)} @@ \norm{x}_p = \empheqlbrace}]{alignat=2}
            & 1,                    && \quad \text{if~$p \ge q$,} \label{eq:max-norm-pq-1}\\
            & n^{\frac{q - p}{pq}}, && \quad \text{otherwise.} \label{eq:max-norm-pq-2}
        \end{empheq}
    \end{subequations}
\end{lemma}

\begin{proof}
    Let us first consider the case where~$p \ge q$.
    For~$\iter \in \ball[q](1)$, we have~$\norm{x}_p \le 1$, and this bound is attained at the first coordinate vector~$e_1 \in \ball[q](1)$, so that~\cref{eq:max-norm-pq-1} holds.

    We now consider the case where~$p < q$.
    Let~$e \in \R^n$ be the all-one vector,~$r = q/p$, and~$s = r / (r - 1) = q / (q - p)$.
    For~$\iter \in \ball[q](1)$, define~$y = (\abs{x_1}^p, \abs{x_2}^p, \dots, \abs{x_n}^p)$.
    According to the H{\"{o}}lder inequality, we have
    \begin{equation*}
        \norm{x}_p  = (e^{\T} y)^{\frac{1}{p}} \le (\norm{e}_s \norm{y}_r)^{\frac{1}{p}} = n^{\frac{q - p}{pq}} \norm{x}_q \le n^{\frac{q - p}{pq}}.
    \end{equation*}
    Moreover, this bound is attained by~$x = n^{-\frac{1}{q}}e$, which proves~\cref{eq:max-norm-pq-2}.
\end{proof}

\begin{proposition}
    \label{prop:lambda-poisedness-initial-optimal}
    For any~$p \ge 1$, if~$m = 2n + 1$, then
    \begin{equation*}
        \Lambda_p = \max @@ \set[\big]{1, n^{\frac{p - 2}{p}} - 1}.
    \end{equation*}
\end{proposition}

\begin{proof}
    It is clear that
    \begin{equation}
        \label{eq:lambda-poisedness-infty-proof-1}
        \max_{\iter \in \ball[p](\delta)} @@ \lagp[1](\iter) = \max_{\iter \in \ball[p](\delta)} @@ \bigg( 1 - \frac{\norm{x}_2^2}{\delta^2} \bigg) = 1.
    \end{equation}
    Moreover, according to \cref{lem:max-norm-pq}, we have
    \begin{subequations}
        \label{eq:lambda-poisedness-infty-proof-2}
        \begin{empheq}[left={\max\limits_{\iter \in \ball[p](\delta)} -\lagp[1](\iter) @@ = \max\limits_{\iter \in \ball[p](\delta)} @@ \dfrac{\norm{x}_2^2}{\delta^2} - 1 = \empheqlbrace}]{alignat=2}
            & 0,                        && \quad \text{if~$p \le 2$,}\\
            & n^{\frac{p - 2}{p}} - 1,  && \quad \text{otherwise.}
        \end{empheq}
    \end{subequations}
    The desired result is obtained by combining~\cref{eq:lambda-poisedness-infty-proof-1,eq:lambda-poisedness-infty-proof-2} with \cref{lem:lambda-poisedness-initial-simple}.
\end{proof}

\subsection{Concluding remarks}

Note that~$\Lambda_p \ge 1$ for any~$p \ge 1$.
\Cref{prop:lambda-poisedness-initial-optimal} shows that~$\Lambda_p = 1$ for all~$p \in [1, 2]$.
In this sense,~$\mathcal{Z}_{2n + 1}$ in an optimal interpolation set in~$\ball[p][2n + 1]$ for~$p \in [1, 2]$, because~$\Lambda_p$ attains its lower bound.
The fact that~$\Lambda_p = 1$ for all~$p \in \set{1, 2}$ is in fact shown also by \cref{prop:lambda-poisedness-1,prop:lambda-poisedness-2}.

In addition,~$m = 2n + 1$ is optimal for~$\mathcal{Z}_m$ in the sense that~$\Lambda_p$ attains the lower bound~$1$ in such a case, when~$p \in [1, 2]$.
We conjecture that~$\Lambda_p$ is minimized at the value~$m = 2n + 1$ for any~$p \ge 1$, but we do not have any proof of such a statement yet.

Note that the definition of~$\mathcal{Z}_m$ in~\cref{eq:initial-interpolation-set,eq:initial-interpolation-set-def} assumes that~$m \le 2n + 1$.
\Citeauthor{Powell_2001}~\cite{Powell_2001} proposed an extension of~$\mathcal{Z}_m$ in the case~$m > 2n + 1$, and hence,~$\mathcal{Z}_m$ is defined for any~$m \in \set{1, 2, \dots, (n + 1) (n + 2) / 2}$.
With such an extension, it is interesting to ask whether~$m = 2n + 1$ still minimizes~$\Lambda_p$, which seems to be true according to some numerical experiments.
We leave this problem open and expect the analysis to be more challenging than what we have done.
One of the challenges is that \cref{lem:lambda-poisedness-initial-simple} does not hold in the case~$m > 2n + 1$, and hence, the calculations will become more involving.
