%% contents/introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Introduction}
\label{ch:introduction}

\nomenclature[Fa]{$\obj$}{Real-valued objective function defined on~$\R^n$}%
\nomenclature[Fb]{$\con{i}$}{Real-valued constraint function defined on~$\R^n$, with~$i \in \iub \cup \ieq$}%
\nomenclature[Fc]{$\lag$}{Lagrangian function}%
\nomenclature[Na]{$\in$}{Set membership notation}%
\nomenclature[Nb]{$\subseteq$}{Set inclusion notation}%
\nomenclature[Nc]{$\qedsymbol$}{Halmos symbol}%
\nomenclature[Nd]{$A^{\T}$,~$v^{\T}$}{Transpose of a matrix or a vector}%
\nomenclature[Ne]{$I_n$}{Identity matrix on~$\R^{n \times n}$}%
\nomenclature[Nf]{$e_i$}{Standard coordinate vector of~$\R^n$ ($i$th column of~$I_n$), with~$1 \le i \le n$}%
\nomenclature[Ng]{$\bigo(\cdot)$}{Big-O notation}%
\nomenclature[Nh]{$o(\cdot)$}{Little-O notation}%
\nomenclature[Ni]{$N(\mu, \sigma^2)$}{Gaussian distribution with mean~$\mu$ and variance~$\sigma^2$}%
\nomenclature[Oa]{$\posp{\cdot}$}{Elementwise positive-part operator}%
\nomenclature[Ob]{$\negp{\cdot}$}{Elementwise negative-part operator}%
\nomenclature[Oc]{$\abs{\cdot}$}{Elementwise modulus operator}%
\nomenclature[Od]{$\inner{\cdot, \cdot}$}{Inner-product operator (may be subscripted for sake of clarity)}%
\nomenclature[Oe]{$\norm{\cdot}$}{Norm of a vector or a matrix (may be subscripted for sake of clarity)}%
\nomenclature[Of]{$\nabla$}{Gradient operator (elements~$\partial / \partial x_i$, with~$i \in \set{1, 2, \dots, n}$)}%
\nomenclature[Og]{$\nabla^2$}{Hessian operator (elements~$\partial^2 / \partial x_i \partial x_j$, with~$i, j \in \set{1, 2, \dots, n}$)}%
\nomenclature[Oh]{$\odot$}{Hadamard product}%
\nomenclature[Oi]{$\card$}{Cardinal function}%
\nomenclature[Oj]{$\conv$}{Convex hull operator}%
\nomenclature[Sa]{$\emptyset$}{Empty set}%
\nomenclature[Sb]{$[a, b]$}{Closed set~$\set{\iter \in \R : a \le \iter \le b}$ with~$a \le b$}%
\nomenclature[Sc]{$(a, b)$}{Open set~$\set{\iter \in \R : a < \iter < b}$ with~$a < b$}%
\nomenclature[Sd]{$\lpoly$}{Space of linear polynomials in~$\R^n$}%
\nomenclature[Se]{$\qpoly$}{Space of quadratic polynomials in~$\R^n$}%
\nomenclature[Sf]{$\R$}{Set of real numbers}%
\nomenclature[Sg]{$\R^n$}{Real coordinate space of dimension~$n$}%
\nomenclature[Sh]{$\R^{m \times n}$}{Real matrix space of dimension~$m \times n$}%
\nomenclature[Si]{$\fset$}{Feasible set, included in~$\R^n$}%
\nomenclature[Sj]{$\ieq$}{Set of indices of the equality constraints}%
\nomenclature[Sk]{$\iub$}{Set of indices of the inequality constraints}%
\todo[noline]{Replace the nomenclature items}

\section{Overview of \glsfmtlong{dfo}}
\label{sec:overview}

Optimization is the study of extremal points and values of mathematical functions.
It aims at minimizing (or maximizing) a real-valued function~$\obj$, referred to as the \emph{objective function}, within a given set of points~$\fset \subseteq \R^n$, referred to as the \emph{feasible set}.
It is well known that essential information for optimization is embraced in the (possibly generalized) derivatives of the functions involved.
However, in practice, evaluations of such derivatives may be unreliable or prohibitively expensive, if not impossible.
It motivates the study of \gls{dfo}~\cite{Conn_Scheinberg_Vicente_2009b,Audet_Hare_2017,Custodio_Scheinberg_Vicente_2017,Larson_Menickelly_Wild_2019}, where problems are solved using only function values.
This thesis focuses on methods and software for \gls{dfo}.

\Gls{dfo} problems arise naturally when the objective function or the feasible set results from complex experiments or simulations.
Regarding these functions as black boxes, people often refer to those problems as \gls{bbo} problems~\cite{Audet_Hare_2017}, which constitute a significant type of \gls{dfo} problem in practice.
Note that \gls{dfo} differs from nonsmooth optimization~\cite{Clark_1983,Cui_Pang_2021}, which studies problems involving nonsmooth functions.
In \gls{dfo}, the major difficulty is not the possible nonsmoothness of the functions involved but the lack of knowledge about the structures of the problems.
In theoretical analysis of \gls{dfo} methods, it is not uncommon to assume that the underlying functions enjoy some smoothness, although algorithms cannot retrieve their (classical or generalized) derivatives.
We emphasize that if any derivative information can be evaluated at an affordable cost or approximated well enough, \gls{dfo} methods are not recommended, as they are very unlikely to outperform methods that use derivatives.
Consider, for example, minimizing an objective function defined by a sophisticated simulation whose source code is available.
One may then attempt to evaluate derivatives using automatic differentiation tools~\cite{Griewank_2003,Griewank_Walther_2008} and apply derivative-based methods.

For \gls{dfo} methods, the leading complexity measure we consider is the number of function evaluations.
In practice, each function evaluation may require several minutes or even several days to complete~\cite[\S~1.4]{Audet_Hare_2017}.
For instance, a recent application of \gls{dfo} is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}, for which every objective function evaluation necessitates training a machine learning model (see \cref{subsec:machine-learning}).
Hence, in \gls{dfo} methods, the expense of numerical linear algebra is less of a concern, although we will maintain it acceptable.

In this introduction, we consider the nonlinearly-constrained problem
\begin{subequations}
    \label{eq:problem-introduction}
    \begin{align}
        \min        & \quad \obj(\iter)\\
        \text{s.t.} & \quad \con{i}(\iter) \le 0, ~ i \in \iub, \label{eq:problem-introduction-cub}\\
                    & \quad \con{i}(\iter) = 0, ~ i \in \ieq, \label{eq:problem-introduction-ceq}\\
                    & \quad \iter \in \R^n, \nonumber
    \end{align}
\end{subequations}
where the \emph{objective} and \emph{constraint functions}~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are real-valued functions on~$\R^n$, and where the sets of indices~$\iub$ and~$\ieq$ are finite (perhaps empty) and disjoint.
The feasible set of this problem is
\begin{equation*}
    \fset \eqdef \set{\iter \in \R^n : \text{$\con{i}(\iter) \le 0$ for~$i \in \iub$ and~$\con{i}(\iter) = 0$ for~$i \in \ieq$}}.
\end{equation*}
If~$\obj$ is convex, while~$\con{i}$ is convex for all~$i \in \iub$ and affine for all~$i \in \ieq$, then the problem~\cref{eq:problem-introduction} is \emph{convex}.
However, this thesis does \emph{not} assume convexity.

We emphasize that~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq} may include bound constraints.
We do not extract them explicitly in this chapter, but they may need to be handled differently from other constraints because they often represent inalienable physical or theoretical restrictions.
We will later in this thesis introduce a new \gls{dfo} method, namely \gls{cobyqa}, which will consider this (see \cref{ch:cobyqa-introduction}).

\section{Examples of applications}

\subsection{Automatic error analysis}

A typical example of \gls{dfo} applications is automatic error analysis~\cite{Higham_1993,Higham_2002}, which formulates numerical computation's accuracies and stabilities using optimization problems.
Consider, for instance, the Gaussian elimination with partial pivoting of a matrix~$A \in \R^{n \times n}$, given in \cref{alg:gaussian-elimination}, where the superscripts denote iteration numbers.

\begin{algorithm}
    \caption{Gaussian elimination with partial pivoting}
    \label{alg:gaussian-elimination}
    \DontPrintSemicolon
    \KwData{Matrix~$A \in \R^{n \times n}$.}
    \KwResult{Factorized matrix~$A^{(n - 1)} \in \R^{n \times n}$.}
    Initialize $A^{(0)} \gets A$\;
    \For{$k = 1, 2, \dots, n - 1$}{
        Determine the pivot index~$j = \argmax \set[\big]{\abs[\big]{A_{i, k}^{(k - 1)}} : k \le i \le n}$\;
        \eIf{$A_{j, k}^{(k - 1)} \neq 0$}{
            Exchange the~$k$th and the~$j$th rows of~$A^{(k - 1)}$\;
            Evaluate the multiplier~$\tau^k \in \R^n$ with components
            \begin{algoempheq}[left={\tau_i^k = \empheqlbrace}]{alignat*=2}
                & A_{i, k}^{(k - 1)} / A_{k, k}^{(k - 1)},  && \quad \text{if~$i > k$,}\\
                & 0,                                        && \quad \text{otherwise}
            \end{algoempheq}
            Update~$A^{(k)} \gets (I_n - \tau^k e_k^{\T})A^{(k - 1)}$\;
        }{
            Set~$A^{(k)} \gets A^{(k - 1)}$\;
        }
    }
\end{algorithm}

\Citeauthor{Wilkinson_1963}'s backward error analysis (see, e.g., equation~(25.14) of chapter~3 in~\cite{Wilkinson_1963}, where~$t$ is introduced at the beginning of \P~10 and~$g$ at the end of p.~97) demonstrates that the growth factor of the Gaussian elimination, defined as
\begin{equation}
    \label{eq:gaussian-elimination-growth-factor}
    \rho_n(A) \eqdef \frac{\max_{0 \le k \le n - 1} \norm{A^{(k)}}_{\max}}{\norm{A}_{\max}},
\end{equation}
determinates the numerical stability of \cref{alg:gaussian-elimination}, where~$\norm{\cdot}_{\max}$ denotes the max norm of a matrix, i.e., the largest absolute value of the matrix's entries.
More specifically, the~$\ell_{\infty}$-norm of the backward error of the computed solution is bounded from above by a term proportional to~$\rho_n(A)$.
To study the worst-case scenario, we wish to determine how large~$\rho_n$ can be and hence, to solve
\begin{equation}
    \label{eq:gaussian-elimination-problem}
    \max_{A \in \R^{n \times n}} \rho_n(A).
\end{equation}
Note that~$\R^{n \times n}$ is isomorphic to~$\R^{n^2}$; hence, the problem~\cref{eq:gaussian-elimination-problem} can be formulated as the problem~\cref{eq:problem-introduction}.
Besides, although the growth factor is defined everywhere, it may not be continuous at the points yielding a tie in selecting the pivot element.
Moreover, it is not differentiable at the points yielding a tie in any maximum operator in equation~\cref{eq:gaussian-elimination-growth-factor}.
Hence, optimization methods based on derivative information cannot be used for this problem.
In such a case, \gls{dfo} methods can help solve the problem~\cref{eq:gaussian-elimination-problem}.
Note that the optimal value and all local solutions to the problem~\cref{eq:gaussian-elimination-problem} are known~\cite{Higham_Higham_1989}, but \gls{dfo} methods can be used to help the theoretical development~\cite{Higham_1993}.

\subsection{Tuning nonlinear optimization methods}
\label{subsec:tuning-nonlinear-optimization-methods}

Another well-known example of \gls{dfo} applications is the parameter tuning of nonlinear optimization methods~\cite{Audet_Orban_2006}.
For example, consider \cref{alg:trust-region}, a basic trust-region method for solving the problem~\cref{eq:problem-introduction} when~$\iub = \ieq = \emptyset$, where~$\norm{\cdot}$ can be any norm.

\begin{algorithm}
    \caption{Basic trust-region method for unconstrained optimization}
    \label{alg:trust-region}
    \DontPrintSemicolon
    \KwData{Objective function~$\obj$, initial guess~$\iter[0] \in \R^n$, initial trust-region radius~$\rad[0] > 0$, and parameters~$0 < \eta_1 \le \eta_2 < 1$ and~$0 < \theta_1 < 1 < \theta_2$.}
    \For{$k = 0, 1, \dots$}{
        Define a simple function~$m_k$ such that~$m_k(d) \approx f(\iter[k] + d)$ for~$\norm{d} \le \rad[k]$\;
        Set the trial step~$\step[k]$ to an approximate solution to
        \begin{algomathdisplay}
            \begin{aligned}
                \min        & \quad m_k(\step)\\
                \text{s.t.} & \quad \norm{\step} \le \rad[k],\\
                            & \quad \step \in \R^n
            \end{aligned}
        \end{algomathdisplay}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \ratio[k] \gets \frac{\obj(\iter[k]) - \obj(\iter[k] + \step[k])}{m_k(0) - m_k(\step[k])}
        \end{algomathdisplay}
        \eIf{$\ratio[k] \ge \eta_1$}{ \nllabel{alg:trust-region-success}
            Update the trial point~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
        }{
            Retain the trial point~$\iter[k + 1] \gets \iter[k]$\;
        }
        Update the trust-region radius
        \begin{algoempheq}[left={\rad[k + 1] \gets \empheqlbrace}]{alignat*=2}
            & \theta_1 \rad[k],  && \quad \text{if~$\ratio[k] \le \eta_1$,}\\
            & \rad[k],           && \quad \text{if~$\eta_1 < \ratio[k] \le \eta_2$,}\\
            & \theta_2 \rad[k],  && \quad \text{otherwise}
        \end{algoempheq}
    }
\end{algorithm}

The most critical simplification in \cref{alg:trust-region} lies in \cref{alg:trust-region-success}.
A complete framework includes a parameter~$\eta_0 \ge 0$ satisfying~$\eta_0 \le \eta_1$, and the condition in \cref{alg:trust-region-success} is replaced by~$\ratio[k] \ge \eta_0$.
In practice, we usually have~$\eta_0 = 0$.
However, this parameter genuinely complexifies the theoretical analysis of the trust-region method, and hence, we omit it here simplicity.
We consider only the four parameters~$\eta_1$,~$\eta_2$,~$\theta_1$, and~$\theta_2$.
To choose those parameters, we minimize some measure of the method's expense (e.g., the \glsxtrshort{cpu} time to solve a given set of optimization problems),~$C$ say.
In other words, we wish to solve the optimization problem
\begin{subequations}
    \label{eq:tuning-algorithms-problem}
    \begin{align}
        \min        & \quad C(\eta_1, \eta_2, \theta_1, \theta_2)\\
        \text{s.t.} & \quad 0 \le \eta_1 \le \eta_2 < 1,\\
                    & \quad 0 < \theta_1 < 1 < \theta_2.
    \end{align}
\end{subequations}
Derivatives of~$C$ cannot be evaluated if they even exist.
Such a problem is then solved using \gls{dfo} methods, such as the \gls{mads} method~\cite{Audet_Orban_2006}.
\Citeauthor{Audet_Digabel_Tribes_2019} modified the \gls{mads} method to solve the problem~\cref{eq:tuning-algorithms-problem} with a controlled number of significant digits~\cite{Audet_Digabel_Tribes_2019} to determine parameters that practitioners can use.
Interestingly, \gls{dfo} methods can be self-tuned using the method presented above.
The \gls{bfo}~\cite{Porcelli_Toint_2017}, a method for bound-constrained problems mixing continuous and discrete variables, is an example of self-tuned \gls{dfo} methods.

\subsection{Hyperparameter tuning in machine learning}
\label{subsec:machine-learning}

A more recent example of \gls{dfo} applications is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}.
For instance, Google solves hyperparameter tuning problems using Google Vizier~\cite{Golovin_Etal_2017}, the Google-internal service for performing black-box optimization.
To illustrate this example, we consider the following hyperparameter tuning problem of a \gls{svm} for binary classification.
Given a binary-labeled dataset~$\set{(\iter_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$, we build an \gls{svm} to classify the data with their respective labels.
Binary classification is obtained using a~$C$-\gls{svc}~\cite{Chang_Lin_2011} by solving the optimization problem
\begin{subequations}
    \label{eq:csvc}
    \begin{align}
        \min        & \quad \frac{1}{2} \norm{\omega}_2^2 + C \norm{\xi}_1\\
        \text{s.t.} & \quad y_i (\beta + \omega^{\T} \varphi_{\gamma}(\iter_i)) \ge 1 - \xi_i, ~ i \in \set{1, 2, \dots, m},\\
                    & \quad \xi \ge 0,\\
                    & \quad (\omega, \beta, \xi) \in \R^{\ell} \times \R \times \R^m,
    \end{align}
\end{subequations}
where~$\varphi_{\gamma}$ is a function mapping the data to a higher-dimensional space~$\R^{\ell}$ and~$\gamma > 0$ and~$C > 0$ are parameters.
Given a solution~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast}) \in \R^{\ell} \times \R \times \R^m$ to the problem~\cref{eq:csvc}, the~$C$-\gls{svc} classifies any data~$\iter \in \R^n$ according to
\begin{equation}
    \label{eq:csvc-classifier}
    \delta(\iter) \eqdef \sgn(\beta^{\ast} + (\omega^{\ast})^{\T} \varphi_{\gamma}(\iter)),
\end{equation}
which maps an observation~$\iter \in \R^n$ to a label in~$\set{\pm 1}$.
It is clear that~$\delta$ depends on the two parameters~$C$ and~$\gamma$, which can be chosen by solving an optimization problem.
The objective function~$P$ of this problem is a~$5$-fold cross-validation based on some performance measure of the model~\cref{eq:csvc-classifier}.
The general~$k$-fold cross-validation to define~$P(C, \gamma)$ is presented in \cref{alg:cross-validation}.

\begin{algorithm}
    \caption{$k$-fold cross-validation of an \glsxtrshort{svc} with parameters~$C$ and~$\gamma$}
    \label{alg:cross-validation}
    \DontPrintSemicolon
    \KwData{Labeled dataset~$\set{(\iter_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$ and fold number~$k > 0$.}
    \KwResult{Performance measure~$P(C, \gamma)$.}
    Split the dataset into~$k$ balanced groups\;
    \For{$i = 1, 2, \dots, k$}{
        Calculate~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast})$ with the all the data except those in the~$i$th group\;
        Evaluate the performance~$p_i$ of the model~\cref{eq:csvc-classifier} on the data in the~$i$th group\;
    }
    Define~$P(C, \gamma)$ by summarizing the performances in~$\set{p_1, p_2, \dots, p_k}$\;
\end{algorithm}

A typical example of a model's performance used in the~$k$-fold cross-validation is the model's accuracy, i.e., the percentage of data correctly classified.
The~\gls{auc}~\cite{Hanley_Mcneil_1982} is another example of performance measures, particularly effective for imbalanced datasets~\cite{Bradley_1997}.
The hyperparameter tuning problem can be formulated as
\begin{equation*}
    \begin{aligned}
        \min        & \quad P(C, \gamma)\\
        \text{s.t.} & \quad C > 0,\\
                    & \quad \gamma > 0.
    \end{aligned}
\end{equation*}
It is clear that derivatives of the objective function of such a problem cannot be easily evaluated and may even not exist.
This problem may be solved using \gls{dfo} methods.

As in~\cite{Qian_Yu_2021}, \gls{dfo} can also be applied to reinforcement learning.
Instead of training a model on a fixed labeled dataset, reinforcement learning bases the training process on rewarding expected behaviors and punishing undesired ones.
Hence, it often consists in finding optimal parameters that maximize a reward.
However, these reward derivatives often cannot be evaluated, and \gls{dfo} methods can be an approach to solving such problems.
This concept is often referred to as derivative-free reinforcement learning.


\subsection{Some industrial and engineering applications}

\Gls{dfo} methods are also widely used in industry and engineering, especially for solving problems that involve heavy simulations.
Such problems arise from helicopter rotor blade manufacturing~\cite{Booker_Etal_1998a,Booker_Etal_1998b,Serafini_1998}, aeroacoustic shape design~\cite{Marsden_2004,Marsden_Etal_2004}, computational fluid dynamics~\cite{Duvigneau_Visonneau_2004}, worst-case analysis of analog circuits~\cite{Latorre_Etal_2019}, rapid-cycling synchrotron accelerator modeling~\cite{Eldred_Etal_2021}, nuclear energy engineering~\cite{Kortelainen_Etal_2010,Kortelainen_Etal_2012,Kortelainen_Etal_2014}, reservoir engineering and engine calibration~\cite{Langouet_2011}, and groundwater supply and bioremediation engineering~\cite{Fowler_Etal_2008,Mugunthan_Shoemaker_Regis_2005,Yoon_Shoemaker_1999}, to name but a few.
In general, industrial and engineering problems that involve sophisticated models, simulations, or experiments, induce \gls{dfo} problems.

A particular application of \gls{dfo} comes from \gls{mdo} in the industry.
It is a field that uses optimization methods to solve design problems defined by multiple disciplines.
The objective and constraint functions of an \gls{mdo} problem can be provided by different departments of the same company or even by different companies.
It is the case in aircraft engine engineering~\cite{Gazaix_Etal_2019}, where the design problem of one component is solved while taking into account constraints imposed by other components handled by different departments.
\Gls{mdo} problems often involve simulations or experiments, and therefore, \gls{dfo} methods are often needed.
In \cref{ch:pdfo} of this thesis, we will present a piece of software we implemented for solving \gls{dfo} problems based on methods by Powell~\cite{Powell_1994,Powell_2002,Powell_2006,Powell_2009,Powell_2015}.
It has been included in GEMSEO~\cite{Gallard_Etal_2018}, an engine for \gls{mdo} initiated by a team from IRT Saint Exup{\'{e}}ry\footnote{\url{https://www.irt-saintexupery.com}} in France.

\section{Optimality conditions for smooth optimization}

We discuss in this section optimality conditions for the problem~\cref{eq:problem-introduction}.
We do not assume any structure on the objective and constraint functions, except some smoothness.
More specialized results can be obtained by assuming that problem~\cref{eq:problem-introduction} is convex, for example, but it is out of the scope of this work.

\subsection{Local and global solutions}

Before solving the problem~\cref{eq:problem-introduction}, we must define what a solution is.
\Cref{def:global-solution} presents the most natural understanding of a solution.

\begin{definition}[Global solution]
    \label{def:global-solution}
    To the problem~\cref{eq:problem-introduction}, a point~$\iter[\ast] \in \R^n$ is a \emph{global solution} if~$\iter[\ast] \in \fset$ and~$\obj(\iter) \ge \obj(\iter[\ast])$ for all~$\iter \in \fset$.
\end{definition}

The following relaxed concepts of solutions to the problem~\cref{eq:problem-introduction} are of interest in theory and practice.

\begin{definition}[Local solution]
    To the problem~\cref{eq:problem-introduction}, a point~$\iter[\ast] \in \R^n$ is
    \begin{itemize}
        \item a \emph{local solution} if~$\iter[\ast] \in \fset$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$\iter[\ast]$ such that~$\obj(\iter) \ge \obj(\iter[\ast])$ for all~$\iter \in \mathcal{N} \cap \fset$,
        \item a \emph{strict local solution} if~$\iter[\ast] \in \fset$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$\iter[\ast]$ such that~$\obj(\iter) > \obj(\iter[\ast])$ for all~$\iter \in \mathcal{N} \cap \fset \setminus \set{\iter[\ast]}$, and
        \item an \emph{isolated local solution} if there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$\iter[\ast]$ such that it is the only local solution in~$\mathcal{N} \cap \fset$.
    \end{itemize}
\end{definition}

It is known that finding a local solution to a general nonconvex problem is NP-hard.
It is already NP-hard to check whether a given point is a local solution in many cases.
There also exist convex optimization problems that are NP-hard, such as copositive programming, for which checking the feasibility of a given point is indeed NP-hard.
See~\cite{Murty_Kabadi_1987} for more discussions.

The methods we consider in this thesis are local.
They attempt to find approximate local solutions to the problem~\cref{eq:problem-introduction}.
However, in general, theoretical analyses of these methods can only guarantee approximations of stationary points, which will be introduced hereafter.

\subsection{Constraint qualifications}

Before introducing any necessary and sufficient conditions for local optimality, we discuss some regularity conditions on the constraints~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq}, referred to as \emph{constraint qualifications}.
They will be required for the necessary conditions to hold.
We first introduce the notion of \emph{active sets}.

\begin{definition}[Active set]
    The \emph{active set}~$\act(\iter) \subseteq \iub \cup \ieq$ for the problem~\cref{eq:problem-introduction} at a point~$\iter \in \R^n$ is defined by
    \begin{equation*}
        \act(\iter) \eqdef \ieq \cup \set{i \in \iub : \con{i}(\iter) \ge 0}.
    \end{equation*}
\end{definition}

If a constraint belongs to the active set\footnote{For simplicity, we do not distinguish a constraint from its index.} at a given point, it is said to be \emph{active} at this point and \emph{inactive} otherwise.
Note that a violated constraint is always considered active.

We introduce hereafter two classical constraint qualifications.

\begin{definition}[Constraint qualifications]
    Given~$\iter \in \fset$, denote~$\act(\iter)$ the active set for the problem~\cref{eq:problem-introduction} at~$x$, and assume that the constraint functions~$\con{i}$ are differentiable at~$x$ for all~$i \in \act(\iter)$.
    We say that
    \begin{itemize}
        \item the \gls{licq} holds at~$x$ if the gradients~$\nabla \con{i}(\iter)$ are linearly independent for all~$i \in \act(\iter)$, and
        \item the \gls{mfcq} holds at~$x$ if the gradients~$\nabla \con{i}(\iter)$ are linearly independent for all~$i \in \ieq$ and there exists a vector~$z \in \R^n$ such that
        \begin{subequations}
            \label{eq:mangasarian-fromovitz}
            \begin{empheq}[left=\empheqlbrace]{alignat=2}
                & \nabla \con{i}(\iter)^{\T} z < 0, && \quad \text{if~$i \in \act(\iter) \cap \iub$,}\\
                & \nabla \con{i}(\iter)^{\T} z = 0, && \quad \text{if~$i \in \ieq$.}
            \end{empheq}
        \end{subequations}
    \end{itemize}
\end{definition}

The \gls{licq} is stronger than the \gls{mfcq}.
If the \gls{licq} holds at~$\iter \in \fset$, then the system~\cref{eq:mangasarian-fromovitz} is consistent because of the linear independence of all~$\nabla \con{i}(\iter)$ for~$i \in \act(\iter)$.

Many other constraint qualifications exist.
Examples include the \gls{acq} and the \gls{gcq}, which are formulated using tangent and linearized cones of the feasible set.
There also exist several traditional constraint qualifications weaker than the \gls{mfcq}, such as the \gls{crcq}, the \gls{cpld}, or the \gls{qncq}.
We also note that dedicated constraint qualifications may exist when the problem has a particular structure, such as the \gls{sc} for convex problems.

\subsection{First-order optimality conditions}

\subsubsection{Statement of the optimality conditions}

Let~$\lag$ be the \emph{Lagrangian} of the problem~\cref{eq:problem-introduction}, defined by
\begin{equation*}
    \lag(\iter, \lm) \eqdef \obj(\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \con{i}(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$ for~$i \in \iub \cup \ieq$},
\end{equation*}
where~$\lm = [\lm_i]_{i \in \iub \cup \ieq}^{\T}$.
\Cref{thm:first-order-necessary-conditions} introduces the first-order necessary conditions for a point to be a local solution of problem~\cref{eq:problem-introduction}.

\begin{theorem}[First-order necessary conditions~{\cite[Thm.~12.1]{Nocedal_Wright_2006}}]
    \label{thm:first-order-necessary-conditions}
    Let~$\iter[\ast] \in \fset$ be a local solution to the problem~\cref{eq:problem-introduction}, assume that the functions~$\obj$ and~$\con{i}$ are continuously differentiable in a neighborhood of~$\iter[\ast]$ for all~$i \in \iub \cup \ieq$, and that the \gls{licq} holds at~$\iter[\ast]$.
    Then there exists a Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ such that
    \begin{subequations}
        \label{eq:kkt-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \nabla_x \lag(\iter[\ast], \lm[\ast]) = 0,    && \label{eq:kkt-introduction-stationarity}\\
            & \con{i}(\iter[\ast]) \le 0,                   && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-primal-feasibility-ub}\\
            & \con{i}(\iter[\ast]) = 0,                     && \quad \text{if~$i \in \ieq$,} \label{eq:kkt-introduction-primal-feasibility-eq}\\
            & \lm[\ast]_i \con{i}(\iter[\ast]) = 0,         && \quad \text{if~$i \in \iub$,} \label{eq:kkt-introduction-complementary-slackness}\\
            & \lm[\ast]_i \ge 0,                            && \quad \text{if~$i \in \iub$.} \label{eq:kkt-introduction-dual-feasibility}
        \end{empheq}
    \end{subequations}
\end{theorem}

We present \cref{thm:first-order-necessary-conditions} with the \gls{licq} as an example, but a similar conclusion can be established with other constraint qualifications, such as the \gls{mfcq} (see, e.g.,~\cite[P.~339]{Nocedal_Wright_2006}).
The conditions~\cref{eq:kkt-introduction} are commonly referred to as the \gls{kkt} conditions~\cite{Karush_1939,Kuhn_Tucker_1951}, and the pair~$(\iter[\ast], \lm[\ast])$ in \cref{thm:first-order-necessary-conditions} is referred to as a \emph{\gls{kkt} pair}.
More specifically, condition~\cref{eq:kkt-introduction-stationarity} is the \emph{stationarity} condition, conditions~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq} as the \emph{primal feasibility} conditions, condition~\cref{eq:kkt-introduction-complementary-slackness} as the \emph{complementary slackness} condition, and condition~\cref{eq:kkt-introduction-dual-feasibility} as the \emph{dual feasibility} condition.
A point~$\iter \in \R^n$ is a \emph{first-order stationary point} if it satisfies the \gls{kkt} conditions~\cref{eq:kkt-introduction}.
Such a point may not be a local solution.

\subsubsection{An illustration of the first-order optimality conditions}

We do not provide a proof of \cref{thm:first-order-necessary-conditions}, but we illustrate graphically the main idea on the simple~$2$-dimensional example
\begin{subequations}
    \label{eq:kkt-description}
    \begin{align}
        \min        & \quad \obj(\iter) = x_1 + x_2\\
        \text{s.t.} & \quad \con{1}(\iter) = x_1^2 + x_2^2 - 2 \le 0, \label{eq:kkt-description-c1}\\
                    & \quad \con{2}(\iter) = -x_2 \le 0, \label{eq:kkt-description-c2}\\
                    & \quad \iter \in \R^2, \nonumber
    \end{align}
\end{subequations}
whose solution is~$\iter[\ast] = [-\sqrt{2}, 0]^{\T}$.
A graphical representation of the problem~\cref{eq:kkt-description} is given in \cref{fig:kkt-description}, where the white area represents the feasible set.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[%
            xmin=-5,%
            xmax=2,%
            ymin=-2,%
            ymax=2,%
            axis equal image,%
            xlabel={$x_1$},%
            ylabel={$x_2$},%
            axis background/.style={%
                pattern=north west lines,%
                pattern color=black!20,%
                even odd rule,%
                insert path={let \p1=(axis cs:0,0), \p2=(axis cs:2^0.5,0), \n1={veclen(\x2-\x1,\y2-\y1)}, in (\p2) arc(0:180:\n1) -- cycle},%
            },%
        ]
            \draw[dashed] (0,0) circle[radius=2^0.5];
            \draw[dashed] (-5,0) -- (2,0);
            \draw[-latex] (-2^0.5,0) -- (-3*2^0.5,0) node[above right] {$\nabla \con{1}(\iter[\ast])$};
            \draw[-latex] (-2^0.5,0) -- (-2^0.5,-1) node[below left] {$\nabla \con{2}(\iter[\ast])$};
            \draw[-latex] (-2^0.5,0) -- (1-2^0.5,1) node[below right] {$\nabla \obj(\iter[\ast])$};
            \addplot[BrickRed,mark=*,only marks] coordinates {(-2^0.5,0)};
            \node[above left] at (-2^0.5,0) {$\iter[\ast]$};
        \end{axis}
    \end{tikzpicture}
    \caption{Graphical representation of the problem~\cref{eq:kkt-description}}
    \label{fig:kkt-description}
\end{figure}

As manifest in \cref{fig:kkt-description}, there does not exist any direction~$\step \in \R^2$ satisfying
\begin{subequations}
    \label{eq:kkt-proof}
    \begin{empheq}[left=\empheqlbrace]{alignat=1}
        & \nabla \obj(\iter[\ast])^{\T} \step < 0,\\
        & \nabla \con{1}(\iter[\ast])^{\T} \step \le 0,\\
        & \nabla \con{2}(\iter[\ast])^{\T} \step \le 0.
    \end{empheq}
\end{subequations}
The Farkas' lemma~\cite{Farkas_1902} ensures, therefore, that there exists a nonnegative Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_1, \lm[\ast]_2]^{\T}$ such that
\begin{equation*}
    \nabla \obj(\iter[\ast]) + \lm[\ast]_2 \nabla \con{1}(\iter[\ast]) + \lm[\ast]_2 \nabla \con{2}(\iter[\ast]) = 0.
\end{equation*}
This validates the condition~\cref{eq:kkt-introduction-stationarity}, while~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq,eq:kkt-introduction-complementary-slackness,eq:kkt-introduction-dual-feasibility} are conspicuous.
A solution to~\cref{eq:kkt-proof} is both a descent direction for~$\obj$ and a linearized feasible direction for the constraints.
In the context of~\cref{thm:first-order-necessary-conditions}, the nonexistence of such a direction is guaranteed by the \gls{licq}.
See~\cite[\S~12.4]{Nocedal_Wright_2006} for a complete proof of \cref{thm:first-order-necessary-conditions}.

\subsection{Second-order optimality conditions}

It is known that at a local solution of a smooth unconstrained optimization problem, the gradient of the objective function is zero and its Hessian matrix is positive semidefinite.
\Cref{thm:second-order-necessary-conditions} generalizes this fact to the optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order necessary conditions~{\cite[Thm.~12.5]{Nocedal_Wright_2006}}]
    \label{thm:second-order-necessary-conditions}
    Let~$\iter[\ast] \in \fset$ be a local solution to the problem~\cref{eq:problem-introduction}.
    Assume that the functions~$\obj$ and~$\con{i}$ are twice continuously differentiable in a neighborhood or~$\iter[\ast]$ for all~$i \in \iub \cup \ieq$, and that the \gls{licq} holds at~$\iter[\ast]$.
    Denote the active set for the problem~\cref{eq:problem-introduction} at~$\iter[\ast]$ by~$\act(\iter[\ast])$.
    Let~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ be a Lagrange multiplier with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ satisfying the \gls{kkt} conditions~\cref{eq:kkt-introduction}, and let~$z \in \R^n$ be any vector such that
    \begin{subequations}
        \label{eq:second-order-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \nabla \con{i}(\iter[\ast])^{\T} z = 0,      && \quad \text{if~$i \in \ieq$,}\\
            & \nabla \con{i}(\iter[\ast])^{\T} z = 0,      && \quad \text{if~$i \in \act(\iter[\ast]) \cap \iub$ and~$\lm[\ast]_i > 0$,}\\
            & \nabla \con{i}(\iter[\ast])^{\T} z \ge 0,    && \quad \text{if~$i \in \act(\iter[\ast]) \cap \iub$ and~$\lm[\ast]_i = 0$.}
        \end{empheq}
    \end{subequations}
    Then~$z^{\T} \nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast]) z \ge 0$.
\end{theorem}

A first-order stationary point~$\iter \in \fset$ is called a \emph{second-order stationary point} if it satisfies the conclusion of \cref{thm:second-order-necessary-conditions}.
We emphasize that a second-order stationary point may not be a local solution.
However, it is known in smooth unconstrained optimization that a point at which the gradient of the objective function is zero and its Hessian matrix is positive definite is a strict local solution.
\Cref{thm:second-order-sufficient-conditions} generalizes this fact to the optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order sufficient conditions~{\cite[Thm.~12.6]{Nocedal_Wright_2006}}]
    \label{thm:second-order-sufficient-conditions}
    Let~$\iter[\ast] \in \fset$ be a given point.
    Assume that the functions~$\obj$ and~$\con{i}$ are twice continuously differentiable in a neighborhood of~$\iter[\ast]$ for all~$i \in \iub \cup \ieq$, and that there exists a Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ satisfying the KKT condition~\cref{eq:kkt-introduction}.
    If for all vector~$z \in \R^n \setminus \set{0}$ satisfying the conditions~\cref{eq:second-order-introduction} we have~$z^{\T} \nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast]) z > 0$, then~$\iter[\ast]$ is a strict local solution to problem~\cref{eq:problem-introduction}.
\end{theorem}

\section{Methodology of \glsfmtlong{dfo}}

As summarized in~\cite{Conn_Scheinberg_Vicente_2009b}, two main strategies have been developed for solving \gls{dfo} problems.
One strategy consists of sampling the objective function around the current iterate and choosing the next iterate among the sampled points based on simple comparisons.
The \emph{direct-search} methods~\cite{Kolda_Lewis_Torczon_2003} are based on this framework.
The other strategy builds iteratively models that approximate the problems (e.g., using polynomials) around the current iterate and choose the next iterate according to the approximated problems.
These methods are referred to as \emph{model-based} methods.
Hybrid methods also exist, such as implicit filtering~\cite{Kelley_2011} and \gls{sidpsm}~\cite{Custodio_Rocha_Vicente_2009}, which combine direct search and models.

Some methods can solve \gls{dfo} problems but are not covered by the above categories.
Examples include random search methods~\cite{Zhigljavsky_1991}, simulated annealing methods~\cite{Kirkpatrick_Gelatt_Vecchi_1983}, the genetic algorithm~\cite{Jong_1975,Holland_1975}, and Bayesian optimization methods~\cite{Mockus_1975,Shahriari_Etal_2016}.
For more information on these methods, we refer to the review~\cite{Larson_Menickelly_Wild_2019} and the reference therein.

\subsection{Direct-search methods}

An early example of \gls{dfo} is a method from \citeauthor{Fermi_Metropolis_1952}~\cite{Fermi_Metropolis_1952}, who developed in \citeyear{Fermi_Metropolis_1952} a nonlinear least-squares solver on MANIAC, an computer based on the von Neumann architecture.
From a modern viewpoint, this method is a coordinate search method, a particular example of direct-search methods, where the search directions are defined to be the coordinate axes.

Several direct-search methods appeared after that.
\citeauthor{Rosenbrock_1960} designed a direct-search method for unconstrained problems.
It constructs an orthogonal basis using previous steps and searching along the directions in this basis~\cite{Rosenbrock_1960}.
Another famous early direct-search method is the Hooke-Jeeves method~\cite{Hooke_Jeeves_1961}.
This method combines exploratory moves along coordinates axes with pattern moves, to exploit the pattern revealed by previous successful directions.
In \citeyear{Nelder_Mead_1965}, \citeauthor{Nelder_Mead_1965}~\cite{Nelder_Mead_1965} introduced the simplex method\footnote{Note that it is different from the simplex method in linear programming.}, which evaluates the objective function at the vertices of a simplex, and updates this simplex according to these function values, one vertex at each iteration.
It is arguably the most widely used \gls{dfo} method, available as the \verb|fminsearch| function in MATLAB, and many variations exist~\cite{Wright_2012}.
Many other works on direct search appeared in the same period, and summaries can be found in~\cite{Fletcher_1965,Box_1966}.
Nowadays, the direct-search paradigm offers an abundance of algorithms~\cite{Kolda_Lewis_Torczon_2003}, such as the \gls{gps} methods~\cite{Booker_Etal_1999}, later extended to the \gls{mads} methods~\cite{Audet_Dennis_2006,Abramson_Audet_2006,Abramson_Etal_2009,Audet_Dennis_Digabel_2008,Digabel_2011}.
A recent example of direct-search methods is the \gls{bfo}~\cite{Porcelli_Toint_2017,Porcelli_Toint_2022}, which handles integer and categorical variables and can be self-tuned.

\Citeauthor{Gratton_Etal_2015} recently proposed incorporating stochastic strategies in direct-search methods~\cite{Gratton_Etal_2015,Gratton_Etal_2019}.
This improves both the performance and the worst-case complexity bounds compared with traditional direct-search methods.

\subsection{Model-based methods}
\label{subsec:model-based-methods}

Unlike direct-search methods, model-based methods approximate locally the functions involved in the optimization problems by simple functions called \emph{models} or \emph{surrogates}.
To make model-based methods globally convergent, the models are exploited by a globalization strategy, such as a \emph{line-search} framework~\cite[Ch.~3]{Nocedal_Wright_2006} or a \emph{trust-region} framework~\cite{Conn_Gould_Toint_2000,Yuan_2015}.
Most of the existing model-based methods use linear or quadratic approximations~\cite{Powell_1994,Conn_Scheinberg_Vicente_2008a,Conn_Scheinberg_Vicente_2008b}, although other models have also been successfully used, such as \glspl{rbf}~\cite{Oeuvray_2005}.

Model-based methods are highly appealing in practice, as they provide excellent performances in real applications.
Compared with direct-search methods, the information contained in the function values is better exploited due to the use of models.
The first trust-region \gls{dfo} method was developed by~\citeauthor{Winfield_1969}~\cite{Winfield_1969,Winfield_1973} in \citeyear{Winfield_1969}.
It is also regarded as the first trust-region method with or without derivatives~\cite[\S~1.2]{Conn_Gould_Toint_2000}.
A similar method was later developed by \citeauthor{Powell_2002}, namely \gls{uobyqa}~\cite{Powell_2002}.
Both methods use fully-determined quadratic interpolation models (see \cref{sec:multivariate-interpolation} for detailed discussions).
By updating the Lagrange functions of the interpolation problem, \gls{uobyqa} needs only~$\bigo(n^4)$ computer operations to obtain each quadratic interpolant, whereas the complexity is~$\bigo(n^6)$ in \citeauthor{Winfield_1969}'s method.

The methods of both \citeauthor{Winfield_1969} and \citeauthor{Powell_2002} require~$\bigo(n^2)$ function evaluations to establish each quadratic model.
Such an amount of function values are needed to initialize the methods, although most of them will be reused at subsequent iterations.
However, this amount of function evaluations is not scalable to moderately large problems.
It motivates methods that use underdetermined quadratic interpolation models.
Such models typically require~$\bigo(n)$ function values to be built, and the remaining freedom bequeathed by the interpolation conditions is taken up by minimizing a functional that reflects the regularity of the model.
For example, the method of \citeauthor{Conn_Toint_1996}~\cite{Conn_Toint_1996} uses such models with the least~$\ell_2$-norm of the coefficients.
Powell's methods, namely \gls{newuoa}~\cite{Powell_2006}, \gls{bobyqa}~\cite{Powell_2009}, and \gls{lincoa}~\cite{Powell_2015}, use models that minimize the Frobenius norm of the change to their Hessian matrices.
Another example of such algorithms is \gls{mnh}~\cite{Wild_2008}, whose models are underdetermined quadratic interpolants with the least Frobenius norm of their Hessian matrices.

Particular care must be given to the geometry of the interpolation set in order to maintain reasonable accuracies of the models (see \cref{sec:poisedness}).
Model-based methods usually make explicit geometry-improving steps when necessary~\cite{Conn_Scheinberg_Vicente_2008a,Conn_Scheinberg_Vicente_2008b}.
The Wedge method~\cite{Marazzi_Nocedal_2002} of \citeauthor{Marazzi_Nocedal_2002}, however, does not include such steps.
Instead, it adds a so-called wedge constraint to the trust-region subproblems to prevent the trial steps from lying in a region that is likely to worsen the geometry of the interpolation set.
The method of \citeauthor{Fasano_Morales_Nocedal_2009}~\cite{Fasano_Morales_Nocedal_2009} does not include geometry-improving steps either but performs surprisingly well.
The authors conjecture that a self-correcting mechanism may be at play and prevents the geometry from deteriorating.
\citeauthor{Scheinberg_Toint_2010}~\cite{Scheinberg_Toint_2010} point out that the geometry improvement cannot be dispensed in general, but a slight modification of the algorithm in~\cite{Fasano_Morales_Nocedal_2009} does enjoy a self-correcting mechanism that garantees the convergence without taking explicit geometry-improving steps.

There are many other trust-region \gls{dfo} methods, such as \gls{csv2}~\cite{Billups_Larson_Graf_2013}, which determines its models by regression and not interpolation.
Examples of solvers that use nonpolynomial models include \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}, which use cubic \gls{rbf}.
There also exist methods for more specific problems, such as \gls{dfls}~\cite{Zhang_Conn_Scheinberg_2010} and \gls{dfols}~\cite{Cartis_Etal_2019}, which aim at solving nonlinear least-squares \gls{dfo} problems.

Randomization is also exploited to improve the performance of trust-region \gls{dfo} methods.
This idea was first proposed by \citeauthor{Bandeira_Scheinberg_Vicente_2012}~\cite{Bandeira_Scheinberg_Vicente_2012}, who established the global convergence of a trust-region method based on random models.
They only require approximating the objective function well enough with a certain probability, and such models can be obtained by interpolation on randomly selected points~\cite{Bandeira_Scheinberg_Vicente_2014}.
The global convergence rate of this method is established by \citeauthor{Gratton_Etal_2018}~\cite{Gratton_Etal_2018}, using an idea elaborated earlier in~\cite[\S~6]{Gratton_Etal_2015}.
Similar results are established independently by \citeauthor{Cartis_Scheinberg_2018}~\cite{Cartis_Scheinberg_2018} but for a more general class of methods.
The work of \citeauthor{Bandeira_Scheinberg_Vicente_2012} has motivated many investigations on methods that use randomized models, such as~\cite{Chen_Menickelly_Scheinberg_2018}.

\subsection{Comments on methods based on finite differences}
\label{subsec:finite-difference}

Perhaps the most straightforward approach to solving \gls{dfo} problems is to make finite-difference approximations of the derivatives and then employ derivative-based methods.
This section  briefly discusses the advantages and disadvantages of this approach.

The~$i$th coordinate of the gradient of a smooth function~$\obj : \R^n \to \R$ at a point~$\iter \in \R^n$ can be approximated by the forward difference
\begin{equation}
    \label{eq:forward-difference}
    \frac{\partial \obj}{\partial x_i}(\iter) = \frac{\obj(\iter + h e_i) - \obj(\iter)}{h} + \bigo(h),
\end{equation}
or the central difference
\begin{equation}
    \label{eq:central-difference}
    \frac{\partial \obj}{\partial x_i}(\iter) = \frac{\obj(\iter + h e_i) - \obj(\iter - h e_i)}{2h} + \bigo(h^2),
\end{equation}
where~$h > 0$ is the difference parameter,~$e_i \in \R^n$ is the~$i$th standard coordinate vector of~$\R^n$, and the order of precisions require standard assumptions (see, e.g.,~\cite[\S~8.1]{Nocedal_Wright_2006}).

Methods based on finite differences have several advantages.
Firstly, they are relatively easy to implement, whereas the implementation of many commonly used \gls{dfo} methods is highly nontrivial and challenging\footnote{For example, \citeauthor{Powell_2006}~\cite{Powell_2006} wrote \enquote{The development of \gls{newuoa} has taken nearly three years. The work was very frustrating [\dots]}}.
Moreover, there is a profusion of well-established derivative-based methods that can be explored with finite-difference approximations.
Finally, we highlight that the function evaluations needed by the approximated derivatives in~\cref{eq:forward-difference,eq:central-difference} can be straightforwardly parallelized, making the resulting algorithm scalable in many situations.

Meanwhile, such methods come with disadvantages as well.
First of all, if the problem is noisy, it is nontrivial to choose the difference parameter~$h$.
From a theoretical standpoint, the optimal choice for the difference parameter depends on the noise level and the Lipschitz constants of the derivatives of the function~$\obj$ (see, e.g.,~\cite[\S~8.1]{Nocedal_Wright_2006} and~\cite[Eqs.~(2.13) and~(2.14)]{Shi_Etal_2021}).
There exist procedures to estimate these quantities (see, e.g.,~\cite[\S~3]{More_Wild_2011} and~\cite[Proc.~I]{Shi_Etal_2021}).
However, these procedures are often costly in terms of function evaluations in practice.
In addition, it is difficult to reuse the function values in methods based on finite differences.
A finite difference at~$x \in \R^n$ requires the function to be evaluated on a mesh of~$\bigo(n)$ points around~$x$ oriented along the coordinate directions.
When~$x$ changes, most of the mesh points change completely, and hence, the function needs to be evaluated at another batch of~$\bigo(n)$ points.

There is no sharp division between methods based on finite difference and those based on interpolation models.
Indeed, the forward difference~\cref{eq:forward-difference} produces the gradient of the linear function that interpolates~$f$ at
\begin{equation*}
    \set{x, x + h e_1, x + h e_2, \dots, x + h e_n}.
\end{equation*}
Similarly, the central difference~\cref{eq:central-difference} generates the gradient of the unique quadratic function with diagonal Hessian matrix that interpolates~$f$ at
\begin{equation*}
    \set{x, x + h e_1, x - h e_1, x + h e_2, x - h e_2, \dots, x + h e_n, x - h e_n}.
\end{equation*}
Interpolation can be regarded as a generalization of finite difference because there is more freedom in choosing the interpolation set.
This freedom allows us to reuse most of the interpolation points, which is essential for the efficiency of model-based methods.

\Cref{sec:benchmarking-tools} will present some simple numerical experiments after introducing the benchmarking tools we use in this thesis for comparing \gls{dfo} solvers.
These results will demonstrate that the methods based on finite differences are highly sensitive to noise.
Unsurprisingly, the methods suffer if the difference parameter does not consider the noise level and the smoothness of the functions.

\section{Benchmarking tools for \glsfmtlong{dfo} methods}
\label{sec:benchmarking-tools}

We introduce in this section the benchmarking tools that we will use throughout this thesis to compare \gls{dfo} solvers.
We use the performance and data profiles~\cite{Dolan_More_2002,More_Wild_2009}, developed by Dolan, Mor{\'{e}}, and Wild.
Most of the information in this section can be found in the aforementioned articles.

\subsection{Expense measure and convergence test}
\label{subsec:convergence-test}

We denote by~$\xsv$ a set of solvers to benchmark and~$\xpb$ a set of test problems, assumed to represent the problems for which the solvers have been designed.
Let~$t_{p, s}$ be the expense for the solver~$s \in \xsv$ to achieve a given convergence test on the problem~$p \in \xpb$.
As mentioned in \cref{sec:overview}, the major cost of \gls{dfo} in practice is the function evaluations.
Therefore, in a \gls{dfo} context,~$t_{p, s}$ measures the number of function evaluations required by~$s \in \xsv$ to solve~$p \in \xpb$ up to the convergence test.
When~$s$ fails to satisfy the convergence test for~$p$ within a given budget (e.g., a maximal number of function evaluations), we define~$t_{p, s} = \infty$ by convention.

In this thesis, the numerical experiments select~$\xpb$ from the CUTEst set~\cite{Gould_Orban_Toint_2015}.
We consider the following convergence test for \gls{dfo} solvers, following~\cite[\S~2]{More_Wild_2009}.
We define for each problem~$p \in \xpb$ a merit function~$\varphi_p$, i.e., a function that measures the quality of a point, taking into account the values of both the constraint and the objective functions.
The smaller value of~$\varphi_p$, the better.
Let~$x_p^0$ be the initial guess for a given problem~$p \in \xpb$, and~$\varphi_p^{\ast}$ be the least value of~$\varphi_p$ obtained by the solvers in~$\xsv$.
Given a tolerance~$\tau \in (0, 1)$, a point~$x$ satisfies the convergence test if
\begin{equation}
    \label{eq:convergence-test-profiles}
    \varphi_p(\iter) \le \varphi_p^{\ast} + \tau [\varphi_p(\iter_p^0) - \varphi_p^{\ast}],
\end{equation}
in which case we say that~$x$ solves problem~$p$ up to the tolerance~$\tau$.
This convergence test can be interpreted as follows.
A point~$x$ satisfies the test if the reduction~$\varphi_p(\iter_p^0) - \varphi_p(\iter)$ is at least~$1 - \tau$ times the maximal reduction~$\varphi_p(\iter_p^0) - \varphi_p^{\ast}$ achieved by all solvers in~$\xsv$.
We say that a solver solves the problem~$p$ up to the tolerance~$\tau$ whenever it produces an iterate that achieves the convergence test~\cref{eq:convergence-test-profiles}.

In the convergence test~\cref{eq:convergence-test-profiles}, it is tempting to define~$\varphi_p^{\ast}$ as the merit function value at a minimizer of~$p$, which is the case when testing derivative-based solvers~\cite{Dolan_More_2002}.
However, according to~\cite{More_Wild_2009}, this may not be appropriate in \gls{dfo} because it may happen that no solver in~$\xsv$ achieves the test within the given computational budget if the function evaluations are expensive.

\subsection{Performance profile}

Fix a tolerance~$\tau \in (0, 1)$ in the convergence test~\cref{eq:convergence-test-profiles}.
For a solver~$s \in \xsv$ and a problem~$p \in \xpb$, define the \emph{performance ratio} by
\begin{equation}
    \label{eq:performance-ratio}
    r_{p, s} \eqdef \frac{t_{p, s}}{\min \set{t_{p, u} : u \in \xsv}},
\end{equation}
which is the \emph{relative} expense for~$s$ to solve~$p$ compared with the most efficient solver in~$\xsv$ for this problem.
The \emph{performance profile} of~$s$ is defined as
\begin{equation*}
    \rho_s(\alpha) \eqdef \frac{1}{\card(\xpb)} \card (\set{p \in \xpb : r_{p, s} \le \alpha}), \quad \text{for~$\alpha \ge 1$},
\end{equation*}
where~$\card(\cdot)$ denotes the cardinal number of a set.
Clearly,~$\rho_s(\alpha)$ is the proportion of problems in~$\xpb$ that are solved by~$s$ with a performance ratio at most~$\alpha$.
It can also be interpreted as the probability for the solver~$s$ to solve a random problem from~$\xpb$ under the restriction on the performance ratio.
In particular, $\rho_s(1)$ is the proportion of problems that~$s$ solves faster than any other solver in~$\xsv$.
Meanwhile,
\begin{equation*}
    \lim_{\alpha \to \infty} \rho_s(\alpha)
\end{equation*}
is the proportion of problems that are solved by~$s$ (within the budget restriction).
Given two solvers~$s_1$ and~$s_2$,~$\rho_{s_1}(\alpha) > \rho_{s_2}(\alpha)$ means that~$s_1$ solves more problems than~$s_2$ under the constraint that~$r_{p, s_1} \le \alpha$ and~$r_{p, s_2} \le \alpha$ for~$p \in \xpb$.
Therefore, a larger value of~$\rho_s$ indicates a better performance of~$s$.

\subsection{Data profile}

We now introduce the data profile, another benchmarking tool proposed by~\cite{More_Wild_2009}.
The \emph{data profile} of a solver~$s \in \xsv$ is defined as
\begin{equation}
    \label{eq:data-profile}
    d_s(\alpha) \eqdef \frac{1}{\card(\xpb)} \card \bigg(\set[\bigg]{p \in \xpb : \frac{t_{p, s}}{n_p + 1} \le \alpha}\bigg), \quad \text{for~$\alpha \ge 0$},
\end{equation}
where~$n_p$ is the dimension of the problem~$p$.
Therefore,~$d_s(\alpha)$ is the proportion of problems in~$\xpb$ that are solved by~$s$ with at most~$\alpha (n_p + 1)$ function evaluations.
It can also be interpreted as the probability for the solver~$s$ to solve a random problem from~$\xpb$, under the budget described above.
In particular,~$d_s(0) = 0$ and, the same as~$\rho_s$,
\begin{equation*}
    \lim_{\alpha \to \infty} d_s(\alpha)
\end{equation*}
is the proportion of problems that are solved by~$s$.
In equation~\cref{eq:data-profile}, the denominator~$n_p + 1$ is a unit cost that serves to normalize $t_{p, s}$, so that the computational expenses for different problems are comparable even if their dimensions are quite different.
Note that~$n_p + 1$ is the number of function evaluations needed for evaluating a simplex gradient~\cite{Bortz_Kelley_1998}.
Therefore,~$d_s(\alpha)$ is the proportion of problems solved by~$s$ within a budget equivalent to~$\alpha$ simplex gradient estimates.
Given two solvers~$s_1$ and~$s_2$,~$d_{s_1}(\alpha) > d_{s_2}(\alpha)$ means that~$s_1$ solves more problems than~$s_2$ using at most~$\alpha (n_p + 1)$ function evaluations for each~$p \in \xpb$.
Therefore, a larger value of~$d_s$ indicates a better performance of~$s$.

\subsection{An illustrative example}

In this section, as an example, we compare three solvers using performance and data profiles.
These solvers, constituting the set~$\xsv$, are
\begin{itemize}
    \item \gls{newuoa} (see \cref{subsec:newuoa-bobyqa-lincoa}), a model-based \gls{dfo} method; and
    \item \gls{bfgs} and \gls{cg}, two gradient-based solvers provided by SciPy~1.0~\cite{Virtanen_Etal_2020}.
    When no derivatives are provided, they use forward finite-difference to approximate gradients, with the difference parameter~$h = \sqrt{u}$, where~$u$ is the unit roundoff.
\end{itemize}
The set~$\xpb$ contains~$154$ smooth unconstrained CUTEst problems of dimension at most~$50$, and the convergence tolerance in~\cref{eq:convergence-test-profiles} is~$\tau = 10^{-3}$.
Since these problems are unconstrained, the merit function~$\varphi_p$ of each problem~$p \in \xpb$ is set to be the objective function~$\obj_p$.
Therefore, the convergence test~\cref{eq:convergence-test-profiles} becomes
\begin{equation}
    \label{eq:convergence-test-profiles-unconstrained}
    \obj_p(\iter) \le \obj_p^{\ast} + \tau [\obj_p(\iter_p^0) - \obj_p^{\ast}].
\end{equation}

We conduct two experiments as follows.
\begin{enumerate}
    \item The first experiment is made without modifying the problems in~$\xpb$.
    In~\cref{eq:convergence-test-profiles-unconstrained},~$\obj_p^{\ast}$ is set to the least value of~$f_p$ obtained by all solvers in~$\xsv$.
    The value of~$t_{p, s}$ is defined as the number of function evaluations that the solver~$s \in \xsv$ needs to solve the problem~$p \in \xpb$ up to the tolerance~$\tau$.
    \item The second experiment is a noisy variation of the previous one.
    Let~$\sigma > 0$ be the noise level.
    For each problem~$p \in \xpb$, the objective function is evaluated by
    \begin{equation}
        \label{eq:relative-gaussian-noisy-objective-function}
        \tilde{\obj}_p(\iter) \eqdef [1 + \epsilon(\iter)] \obj_p(\iter),
    \end{equation}
    where~$\epsilon(\iter) \sim N(0, \sigma^2)$.
    Each problem is solved ten times with each solver.
    In the convergence test~\cref{eq:convergence-test-profiles-unconstrained},~$\obj_p^{\ast}$ is either the least value of~$\obj_p$ obtained by all solvers during these ten runs, or the value obtained in the previous noise-free experiment, whichever is smaller.
    The value of~$t_{p, s}$ is set to the average number of function evaluations that~$s \in \xsv$ needs to solve~$p \in \xpb$ up to the tolerance~$\tau$.
    Note that the convergence test~\cref{eq:convergence-test-profiles-unconstrained} uses the values of~$\obj_p$ and not those of~$\tilde{\obj}_p$.
    This means that we evaluate the solvers according to the true objective function, even though the solvers can access only the noisy values produced by~\cref{eq:relative-gaussian-noisy-objective-function}.
\end{enumerate}

\subsubsection{Performance profiles}

\Cref{fig:performance-profile-example} plots the performance profiles obtained by these experiments.
In general, the best solver is indicated by the highest curve.
Note that~$\alpha$ is displayed in~$\log_2$-scale.
\Cref{fig:performance-profile-example-noiseless} corresponds to the noise-free experiment and \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6} correspond to the noisy experiment, with~$\sigma$ being~$10^{-10}$,~$10^{-8}$, and~$10^{-6}$, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noise-free experiment}
        \label{fig:performance-profile-example-noiseless}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-10}$}
        \label{fig:performance-profile-example-noisy-10}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-8-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-8}$}
        \label{fig:performance-profile-example-noisy-8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-6-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-6}$}
        \label{fig:performance-profile-example-noisy-6}
    \end{subfigure}
    \caption{Performance profiles of \gls{newuoa}, \gls{bfgs}, and \gls{cg} with~$\tau = 10^{-3}$}
    \label{fig:performance-profile-example}
\end{figure}

We can make the following observations on the noise-free experiment in \cref{fig:performance-profile-example-noiseless}.
\begin{enumerate}
    \item \Gls{bfgs} solves more than~$90\%$ of the problems, while the two other solvers solve slightly fewer problems.
    According to~\cite[\S~2]{More_Wild_2009}, \gls{bfgs} is said more reliable than the other two solvers on the test problems in~$\xpb$, but the margin is narrow.
    \item \Gls{newuoa} uses the least number of function evaluations on about~$50\%$ of the problems, while the percentages for \gls{bfgs} and \gls{cg} are about~$30\%$ and~$25\%$ respectively.
\end{enumerate}

We observe the following for the noisy experiments according to~\cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}.
\begin{enumerate}
    \item \Gls{newuoa} solves always more than~$70\%$ of the problems while the two other solvers solve significantly less problems.
    \item When~$\sigma = 10^{-10}$, \gls{newuoa} uses the least number of function evaluations on more than~$55\%$ of the problems, while the percentages for \gls{bfgs} and \gls{cg} are about~$20\%$ and~$15\%$ respectively.
    For~$\sigma \in \set{10^{-8}, 10^{-6}}$, \gls{newuoa} is the most efficient on almost all the problems that it can solve, while the other two solvers fail on most of the problems in~$\xpb$.
\end{enumerate}

Overall, on the noise-free problems, \gls{bfgs} is slightly more reliable than \gls{newuoa} and \gls{cg}, although \gls{newuoa} is more efficient on about half of the problems.
In contrast, \gls{newuoa} outperforms the two other solvers on noisy problems.
The performances of \gls{bfgs} and \gls{cg} deteriorate significantly when Gaussian noise is imposed as in~\cref{eq:relative-gaussian-noisy-objective-function}, even though the noise level is not high and the convergence tolerance is not demanding.

Note that our results do not contradict the observations of \citeauthor{Shi_Etal_2021}~\cite{Shi_Etal_2021}, where they choose the difference parameter~$h$ more carefully, according to the noise levels and the smoothness of the problems.
We rather keep the default value provided for the solvers \gls{bfgs} and \gls{cg} in SciPy~1.0~\cite{Virtanen_Etal_2020}.
As commented in~\cite{Shi_Etal_2021}, the performance of methods based on finite differences is encouraging when there is no noise, yet much more care is needed when the problems are noisy.
\Gls{newuoa} performs robustly with the noise according to this experiment, which agrees with the observations in~\cite{Shi_Etal_2021}.

We also mention that no conclusive comparison can be made between \gls{bfgs} and \gls{cg} according to \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}.
In fact, as pointed out in~\cite{Gould_Scott_2016}, when there are more than two solvers to compare, performance profiles have limitations when ranking the solvers, except for the one that outperforms the others (if any), such as \gls{newuoa} in \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}.

\subsubsection{Data profiles}

\Cref{fig:data-profile-example} displays the data profiles obtained in these experiments.
A higher curve indicates a better performance.
\Cref{fig:data-profile-example-noiseless} corresponds to the noise-free experiment, and \cref{fig:data-profile-example-noisy-10,fig:data-profile-example-noisy-8,fig:data-profile-example-noisy-6} correspond to the noisy experiment, with~$\sigma$ being~$10^{-10}$,~$10^{-8}$, and~$10^{-6}$, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noise-free experiment}
        \label{fig:data-profile-example-noiseless}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-10}$}
        \label{fig:data-profile-example-noisy-10}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-8-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-8}$}
        \label{fig:data-profile-example-noisy-8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-6-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-6}$}
        \label{fig:data-profile-example-noisy-6}
    \end{subfigure}
    \caption{Data profiles of \gls{newuoa}, \gls{bfgs}, and \gls{cg} with~$\tau = 10^{-3}$}
    \label{fig:data-profile-example}
\end{figure}

We can make the following observations on the noise-free experiment on \cref{fig:data-profile-example-noiseless}.
\begin{enumerate}
    \item \Gls{bfgs} solves more than~$90\%$ of the problems while the two other solvers solve slightly less problems.
    It agrees with what is indicated by \cref{fig:performance-profile-example-noiseless}, as it should be the case according to theory.
    \item \Gls{bfgs} uses less than~$300 (n_p + 1)$ function evaluations to solve most of the problems~$p \in \xpb$ that it can solve, while \gls{newuoa} and \gls{cg} use about~$400 (n_p + 1)$.
\end{enumerate}

We observe the following for the noisy experiments according to~\cref{fig:data-profile-example-noisy-10,fig:data-profile-example-noisy-8,fig:data-profile-example-noisy-6}.
\begin{enumerate}
    \item \Gls{newuoa} solves always more than~$70\%$ of the problems while the two other solvers solve significantly fewer problems.
    It agrees with what is indicated by \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}, as it should be the case.
    \item When~$\sigma = 10^{-10}$, \gls{newuoa} uses less than~$200 (n_p + 1)$ function evaluations to solve most of the problems~$p \in \xpb$ that it can solve, while \gls{bfgs} and \gls{cg} use less than~$100 (n_p + 1)$, altough they solve much fewer problems.
    For~$\sigma \in \set{10^{-8}, 10^{-6}}$, \gls{newuoa} uses less than~$100 (n_p + 1)$ function evaluations\footnote{Even though the corresponding value is~$200 (n_p + 1)$ when~$\sigma = 10^{-10}$, we should not interpreted that \gls{newuoa} performs better with higher noise. When~$\sigma$ is larger, \gls{newuoa} solves fewer problems and~$\obj_p^{\ast}$ in~\cref{eq:convergence-test-profiles-unconstrained} is often larger, leading to a weaker convergence test.} to solve most of the problems that it can solve, while the other two solvers fail on most of the problems.
\end{enumerate}

\subsection{Absoluteness and relativeness of the profiles}

Before concluding this section, we mention a significant difference between performance and data profiles.
Data profiles are absolute because~\cref{eq:data-profile} compares the expense~$t_{p, s}$ with~$n_p + 1$ for each~$p \in \xpb$, which is independent of the other solvers.
The data profile of a solver does depend weakly on the other solvers, the only dependence being the value of~$\varphi_p^{\ast}$ in the convergence test~\cref{eq:convergence-test-profiles}.
On the other hand, performance profiles are relative because the value of~$r_{p, s}$ in~\cref{eq:performance-ratio} compares the expense of~$s$ with that of the most efficient solver on~$p$, which depends on all the other solvers. 
For example, the data profiles in \cref{fig:data-profile-example-noisy-10} suggest that \gls{cg} outperforms \gls{bfgs} during the corresponding experiment, and the profiles are unlikely to change dramatically if \gls{newuoa} is removed from the comparison.
In contrast, the comparison between \gls{bfgs} and \gls{cg} is not as conclusive based on the performance profiles in \cref{fig:performance-profile-example-noisy-10} because the profiles may change significantly if \gls{newuoa} is removed from the comparison.
This phenomenon is studied in~\cite{Gould_Scott_2016}, which presents examples on which comparisons using performance profiles should be made with care.

\section{Notations}

\todo[noline]{Complete the notation description}

Throughout this thesis, the functions~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, always denote the objective and constraint functions of an optimization problem, respectively.
The Lagrangian function of this problem is denoted by~$\lag$.
Its first argument~$\iter$ is the decision variable, while its second argument~$\lm = [\lm_i]_{i \in \iub \cup \ieq}$ is the dual variable of the considered problem.
A quadratic approximation of a function~$u$ is always denoted by~$\hat{u}$, and may be subscripted if this model is used in an iterative process.
For example, if an optimization method maintains a quadratic approximation of the objective function, the~$k$th model is denoted by~$\objm[k]$.

In general, an iteration number is written as a superscript.
When an exponent is used in a formula that may lead to confusion, we will explicitely state the meaning of the notation in the context.
Moreover, a subscript usually indicates a component number of a vector, a matrix, etc.
An exception to this rule is the notation~$e_i \in \R^n$, which indicates the~$i$th standard coordinate vector, i.e., the~$i$th column of the identity matrix in~$\R^{n \times n}$.

We will mainly consider the~$\ell_2$-norm in this thesis.
Hence,~$\norm{\cdot}$ denotes the~$\ell_2$-norm, unless otherwise stated.
If any other norm is needed, its choice is clarified using subscripts.
For example~$\norm{\cdot}_{\mathsf{F}}$ denotes the Frobenius norm of a matrix.
