%% contents/introduction.tex
%% Copyright 2021-2022 Tom M. Ragonneau
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2005/12/01 or later.
%
% This work has the LPPL maintenance status `maintained'.
%
% The Current Maintainer of this work is Tom M. Ragonneau.
\chapter{Introduction}
\label{ch:introduction}

This thesis will develop model-based \gls{dfo} methods and software.
In this chapter, we provide an overview of \gls{dfo}, introduce some applications, and present the concepts and tools that we will use throughout this thesis.
In particular, \cref{sec:overview} includes basic concepts of \gls{dfo} and its motivation.
\Cref{sec:dfo-examples} provides several examples of \gls{dfo} problems from academic, engineering, and industrial applications.
\Cref{sec:optimality-conditions} introduces the optimality conditions for smooth nonlinear optimization, which will be used in the development of our algorithms.
We then summarize existing methodologies for the design of \gls{dfo} algorithms in \cref{sec:methodology-dfo}.
Finally, \cref{sec:benchmarking-tools} presents two benchmarking tools that will be used to compare and assess the performance of \gls{dfo} solvers.

\section{Overview of \glsfmtlong{dfo}}
\label{sec:overview}

\index{DFO@\glsfmtshort{dfo}|(}Optimization is the study of extremal points and values of mathematical functions.
It aims at minimizing (or maximizing) a real-valued function~$\obj$\nomenclature[Fa]{$\obj$}{Real-valued objective function defined on~$\R^n$}, referred to as the \emph{objective function}\index{objective function}, within a given set of points~$\fset \subseteq \R^n$\nomenclature[Nb]{$\subseteq$}{Set inclusion notation}\nomenclature[Sf]{$\R^n$}{Real coordinate space of dimension~$n$}\nomenclature[Sj]{$\fset$}{Feasible set, included in~$\R^n$}, referred to as the \emph{feasible set}\index{feasible set}.
It is well known that essential information for optimization is embraced in the (possibly generalized) derivatives of the functions involved.
However, in practice, evaluations of such derivatives may be unreliable or prohibitively expensive, if not impossible.
It motivates the study of \gls{dfo}~\cite{Conn_Scheinberg_Vicente_2009b,Audet_Hare_2017,Custodio_Scheinberg_Vicente_2017,Larson_Menickelly_Wild_2019}, where problems are solved using only function values.
This thesis focuses on methods and software for \gls{dfo}.

\Gls{dfo} problems arise naturally when the objective function or the feasible set results from complex experiments or simulations.
Regarding these functions as black boxes, people often refer to those problems as \gls{bbo}\index{DFO@\glsfmtshort{dfo}!BBO@\glsfmtshort{bbo}} problems~\cite{Audet_Hare_2017}, which constitute a significant type of \gls{dfo} problem in practice.
Note that \gls{dfo} differs from nonsmooth optimization~\cite{Clark_1983,Cui_Pang_2021}, which studies problems involving nonsmooth functions.
In \gls{dfo}, the major difficulty is not the possible nonsmoothness of the functions involved but the lack of knowledge about the structures of the problems.
In theoretical analysis of \gls{dfo} methods, it is not uncommon to assume that the underlying functions enjoy some smoothness, although algorithms cannot retrieve their (classical or generalized) derivatives.
We emphasize that if any derivative information can be evaluated at an affordable cost or approximated well enough, \gls{dfo} methods are not recommended, as they are very unlikely to outperform methods that use derivatives.
Consider, for example, minimizing an objective function defined by a sophisticated simulation whose source code is available.
One may then attempt to evaluate derivatives using automatic differentiation tools~\cite{Griewank_2003,Griewank_Walther_2008} and apply derivative-based methods.

For \gls{dfo} methods, the leading complexity measure we consider is the number of function evaluations.
In practice, each function evaluation may require several minutes or even several days to complete~\cite[\S~1.4]{Audet_Hare_2017}.
For instance, a recent application of \gls{dfo} is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}, for which every objective function evaluation necessitates training a machine learning model (see \cref{subsec:machine-learning}).
Hence, in \gls{dfo} methods, the expense of numerical linear algebra is less of a concern, although we will maintain it acceptable.

In this introduction, we consider the nonlinearly constrained problem
\begin{subequations}
    \label{eq:problem-introduction}
    \begin{align}
        \min_{\iter \in \R^n}   & \quad \obj(\iter)\\
        \text{s.t.}             & \quad \con{i}(\iter) \le 0, ~ i \in \iub, \label{eq:problem-introduction-cub}\\
                                & \quad \con{i}(\iter) = 0, ~ i \in \ieq, \label{eq:problem-introduction-ceq}
    \end{align}
\end{subequations}
\nomenclature[Fb]{$\con{i}$}{Real-valued constraint function defined on~$\R^n$, with~$i \in \iub \cup \ieq$}%
\nomenclature[Na]{$\in$}{Set membership notation}%
\nomenclature[Sk]{$\ieq$}{Set of indices of the equality constraints}%
\nomenclature[Sl]{$\iub$}{Set of indices of the inequality constraints}%
where the \emph{objective}\index{objective function} and \emph{constraint functions}\index{constraint function}~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are real-valued functions on~$\R^n$, and where the sets of indices~$\iub$ and~$\ieq$ are finite (perhaps empty) and disjoint.
The feasible set of this problem is
\begin{equation*}
    \fset \eqdef \set{\iter \in \R^n : \text{$\con{i}(\iter) \le 0, ~ i \in \iub$ and~$\con{i}(\iter) = 0, ~ i \in \ieq$}}.
\end{equation*}
If~$\obj$ is convex, while~$\con{i}$ is convex for all~$i \in \iub$ and affine for all~$i \in \ieq$, then problem~\cref{eq:problem-introduction} is \emph{convex}.
However, this thesis does \emph{not} assume convexity.

We emphasize that~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq} may include bound constraints.
We do not extract them explicitly in this chapter, but they may need to be handled differently from other constraints because they often represent inalienable physical or theoretical restrictions.
This will be considered by our new \gls{dfo} method (see \cref{ch:cobyqa-introduction}).

\section{Examples of applications}
\label{sec:dfo-examples}

\subsection{Automatic error analysis}

A typical example of \gls{dfo} applications is automatic error analysis~\cite{Higham_1993,Higham_2002}, which formulates numerical computation's accuracies and stabilities using optimization problems.
Consider, for instance, the Gaussian elimination with partial pivoting of a matrix~$A \in \R^{n \times n}$, given in \cref{alg:gaussian-elimination}, where the superscripts denote iteration numbers.

\begin{algorithm}
    \caption{Gaussian elimination with partial pivoting}
    \label{alg:gaussian-elimination}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Matrix~$A \in \R^{n \times n}$.}
    \KwResult{Factorized matrix~$A^{(n - 1)} \in \R^{n \times n}$.}
    Initialize $A^{(0)} \gets A$\;
    \For{$k = 1, 2, \dots, n - 1$}{
        Determine a pivot index~$j$ that solves~$\max \set[\big]{\abs[\big]{A_{i, k}^{(k - 1)}} : k \le i \le n}$\nomenclature[Oc]{$\abs{\cdot}$}{Modulus operator}\;
        \eIf{$A_{j, k}^{(k - 1)} \neq 0$}{
            Exchange the~$k$th and the~$j$th rows of~$A^{(k - 1)}$\;
            Evaluate the multiplier~$\tau^k \in \R^n$ with components
            \begin{algoempheq}[left={\tau_i^k = \empheqlbrace}]{alignat*=2}
                & A_{i, k}^{(k - 1)} / A_{k, k}^{(k - 1)},  && \quad \text{if~$i > k$,}\\
                & 0,                                        && \quad \text{otherwise}
            \end{algoempheq}
            Update~$A^{(k)} \gets (I_n - \tau^k e_k^{\T})A^{(k - 1)}$\nomenclature[Nd]{$A^{\T}$,~$v^{\T}$}{Transpose of a matrix or a vector}\nomenclature[Ne]{$I_n$}{Identity matrix on~$\R^{n \times n}$}\nomenclature[Nf]{$e_i$}{Standard coordinate vector of~$\R^n$ ($i$th column of~$I_n$), with~$1 \le i \le n$}\;
        }{
            Set~$A^{(k)} \gets A^{(k - 1)}$\;
        }
    }
\end{algorithm}

\Citeauthor{Wilkinson_1963}'s backward error analysis (see, e.g., equation~(25.14) of chapter~3 in~\cite{Wilkinson_1963}, where~$t$ is introduced at the beginning of Paragraph~10 and~$g$ at the end of p.~97) demonstrates that the growth factor of the Gaussian elimination, defined as
\begin{equation}
    \label{eq:gaussian-elimination-growth-factor}
    \rho_n(A) \eqdef \frac{1}{\norm{A}_{\max}} \max_{0 \le k \le n - 1} \norm{A^{(k)}}_{\max},
\end{equation}
\nomenclature[Of]{$\norm{\cdot}$}{Norm of a vector or a matrix (may be subscripted for clarity)}%
determinates the numerical stability of \cref{alg:gaussian-elimination}, where~$\norm{\cdot}_{\max}$ denotes the max norm of a matrix, i.e., the largest absolute value of the matrix's entries.
More specifically, the~$\ell_{\infty}$-norm of the backward error of the computed solution is bounded from above by a term proportional to~$\rho_n(A)$.
To study the worst-case scenario, we wish to determine how large~$\rho_n$ can be and hence, to solve
\begin{equation}
    \label{eq:gaussian-elimination-problem}
    \max_{A \in \R^{n \times n}} \rho_n(A).
\end{equation}
Note that~$\R^{n \times n}$ is isomorphic to~$\R^{n^2}$; hence, problem~\cref{eq:gaussian-elimination-problem} can be formulated as problem~\cref{eq:problem-introduction}.
Besides, although the growth factor is defined everywhere, it may not be continuous at the points yielding a tie in selecting the pivot element.
Moreover, it is not differentiable at the points yielding a tie in any maximum operator in equation~\cref{eq:gaussian-elimination-growth-factor}.
Hence, optimization methods based on derivative information cannot be used for this problem.
In such a case, \gls{dfo} methods can help solve problem~\cref{eq:gaussian-elimination-problem}.
Note that the optimal value and all local solutions to problem~\cref{eq:gaussian-elimination-problem} are known~\cite{Higham_Higham_1989}, but \gls{dfo} methods can be used to help the theoretical development~\cite{Higham_1993}.

\subsection{Tuning nonlinear optimization methods}
\label{subsec:tuning-nonlinear-optimization-methods}

\index{DFO@\glsfmtshort{dfo}!model-based!trust-region|(}Another well-known example of \gls{dfo} applications is the parameter tuning of nonlinear optimization methods~\cite{Audet_Orban_2006}.
For example, consider \cref{alg:trust-region}, a basic trust-region method for solving the problem~\cref{eq:problem-introduction} when~$\iub = \ieq = \emptyset$\nomenclature[Sa]{$\emptyset$}{Empty set}, where~$\norm{\cdot}$ can be any norm.

\begin{algorithm}
    \caption{Basic trust-region method for unconstrained optimization}
    \label{alg:trust-region}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Objective function~$\obj$, initial guess~$\iter[0] \in \R^n$, initial trust-region radius~$\rad[0] > 0$, and parameters~$0 < \eta_1 \le \eta_2 < 1$ and~$0 < \theta_1 < 1 < \theta_2$.}
    \For{$k = 0, 1, \dots$}{
        Define a simple function~$m_k$ such that~$m_k(d) \approx f(\iter[k] + d)$ for~$\norm{d} \le \rad[k]$\;
        Set the trial step~$\step[k]$ to an approximate solution to
        \begin{algomathdisplay}
            \begin{aligned}
                \min_{\step \in \R^n}   & \quad m_k(\step)\\
                \text{s.t.}             & \quad \norm{\step} \le \rad[k]
            \end{aligned}
        \end{algomathdisplay}
        Evaluate the trust-region ratio
        \begin{algomathdisplay}
            \ratio[k] \gets \frac{\obj(\iter[k]) - \obj(\iter[k] + \step[k])}{m_k(0) - m_k(\step[k])}
        \end{algomathdisplay}
        \eIf{$\ratio[k] \ge \eta_1$}{ \nllabel{alg:trust-region-success}
            Update the trial point~$\iter[k + 1] \gets \iter[k] + \step[k]$\;
        }{
            Retain the trial point~$\iter[k + 1] \gets \iter[k]$\;
        }
        Update the trust-region radius
        \begin{algoempheq}[left={\rad[k + 1] \gets \empheqlbrace}]{alignat*=2}
            & \theta_1 \rad[k],  && \quad \text{if~$\ratio[k] \le \eta_1$,}\\
            & \rad[k],           && \quad \text{if~$\eta_1 < \ratio[k] \le \eta_2$,}\\
            & \theta_2 \rad[k],  && \quad \text{otherwise}
        \end{algoempheq}
    }
\end{algorithm}

A critical simplification in \cref{alg:trust-region} lies in \cref{alg:trust-region-success}.
A complete framework includes a parameter~$\eta_0 \ge 0$ satisfying~$\eta_0 \le \eta_1$, and the condition in \cref{alg:trust-region-success} is replaced by~$\ratio[k] \ge \eta_0$.
In practice, it is not uncommon to set~$\eta_0 = 0$.

To choose the parameters~$\eta_1$,~$\eta_2$,~$\theta_1$, and~$\theta_2$, we minimize some measure of the expense of the method (e.g., the \glsxtrshort{cpu} time to solve a given set of problems), which will be denoted by~$C$.
In other words, we wish to solve
\begin{subequations}
    \label{eq:tuning-algorithms-problem}
    \begin{align}
        \min        & \quad C(\eta_1, \eta_2, \theta_1, \theta_2)\\
        \text{s.t.} & \quad 0 \le \eta_1 \le \eta_2 < 1,\\
                    & \quad 0 < \theta_1 < 1 < \theta_2.
    \end{align}
\end{subequations}
Derivatives of~$C$ cannot be evaluated, if they even exist.
Moreover, observe that the objective function~$C$ is likely to be undefined if the bound constraints are violated.
This problem can be solved using \gls{dfo} methods, such as the \gls{mads} method~\cite{Audet_Orban_2006,Audet_Digabel_Tribes_2019}.

Interestingly, \gls{dfo} methods can be self-tuned following this methodology.
The \gls{bfo}~\cite{Porcelli_Toint_2017}, a method for bound-constrained problems mixing continuous and discrete variables, is an example of self-tuned \gls{dfo} methods.\index{DFO@\glsfmtshort{dfo}!model-based!trust-region|)}

\subsection{Hyperparameter tuning in machine learning}
\label{subsec:machine-learning}

A more recent example of \gls{dfo} applications is hyperparameter tuning in machine learning~\cite{Ghanbari_Scheinberg_2017}.
Google solves for instance hyperparameter tuning problems using Google Vizier~\cite{Golovin_Etal_2017}, the Google-internal service for performing black-box optimization.
To illustrate this example, we consider the following hyperparameter tuning problem of a \gls{svm} for binary classification.
\index{SVM@\glsfmtshort{svm}|(}Given a binary-labeled dataset~$\set{(\iter_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$, we build an \gls{svm} to classify the data with their respective labels.
Binary classification is obtained using a~$C$-\gls{svc}\index{SVM@\glsfmtshort{svm}!CSVC@$C$-\glsfmtshort{svc}}~\cite{Chang_Lin_2011} by solving the optimization problem
\begin{subequations}
    \label{eq:csvc}
    \begin{align}
        \min_{(\omega, \beta, \xi) \in \R^{\ell} \times \R \times \R^m} & \quad \frac{1}{2} \norm{\omega}_2^2 + C \norm{\xi}_1\\
        \text{s.t.}                                                     & \quad y_i \big( \beta + \omega^{\T} \psi_{\gamma}(\iter_i) \big) \ge 1 - \xi_i, ~ i \in \set{1, 2, \dots, m},\\
                                                                        & \quad \xi \ge 0,
    \end{align}
\end{subequations}
where~$\psi_{\gamma}$ is a function mapping the data to a higher-dimensional space~$\R^{\ell}$ and~$\gamma > 0$ and~$C > 0$ are parameters.
Given a solution~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast}) \in \R^{\ell} \times \R \times \R^m$\nomenclature[Se]{$\R$}{Set of real numbers} to the problem~\cref{eq:csvc}, the~$C$-\gls{svc} classifies any data~$\iter \in \R^n$ according to
\begin{equation}
    \label{eq:csvc-classifier}
    \delta(\iter) \eqdef \sgn \big( \beta^{\ast} + (\omega^{\ast})^{\T} \psi_{\gamma}(\iter) \big),
\end{equation}
\nomenclature[On]{$\sgn$}{Sign operator}%
which maps an observation~$\iter \in \R^n$ to a label in~$\set{\pm 1}$.\index{SVM@\glsfmtshort{svm}|)}
It is clear that~$\delta$ depends on the two parameters~$C$ and~$\gamma$, which can be chosen by solving an optimization problem.
The objective function~$P$ of this problem can be a~$5$-fold cross-validation based on some performance measure of model~\cref{eq:csvc-classifier}, presented in \cref{alg:cross-validation}.

\begin{algorithm}
    \caption{$k$-fold cross-validation of an \glsfmtshort{svc} with parameters~$C$ and~$\gamma$}
    \label{alg:cross-validation}
    \DontPrintSemicolon
    \onehalfspacing
    \KwData{Labeled dataset~$\set{(\iter_i, y_i)}_{i = 1, 2, \dots, m} \subseteq \R^n \times \set{\pm 1}$, fold number~$k > 0$, and parameters~$C$ and~$\gamma$.}
    \KwResult{Performance measure~$P(C, \gamma)$.}
    Split the dataset into~$k$ balanced groups\;
    \For{$i = 1, 2, \dots, k$}{
        Calculate~$(\omega^{\ast}, \beta^{\ast}, \xi^{\ast})$ with the all the data except those in the~$i$th group\;
        Evaluate the performance~$p_i$ of the model~\cref{eq:csvc-classifier} on the data in the~$i$th group\;
    }
    Define~$P(C, \gamma)$ by summarizing the performances in~$\set{p_1, p_2, \dots, p_k}$\;
\end{algorithm}

Typical example of model's performances are the model's accuracy, i.e., the ratio of data correctly classified, and the~\gls{auc}~\cite{Hanley_Mcneil_1982}, particularly effective for imbalanced datasets~\cite{Bradley_1997}.
The hyperparameter tuning problem is
\begin{align*}
    \min        & \quad P(C, \gamma)\\
    \text{s.t.} & \quad C, \gamma > 0,
\end{align*}
It is clear that derivatives of the objective function of such a problem cannot be easily evaluated and may even not exist.
This problem can be solved using \gls{dfo} methods.

As in~\cite{Qian_Yu_2021}, \gls{dfo} can also be applied to reinforcement learning.
Instead of training a model on a fixed labeled dataset, reinforcement learning bases the training process on rewarding expected behaviors and punishing undesired ones.
Hence, it often consists in finding optimal parameters that maximize a reward.
However, these reward derivatives often cannot be evaluated, and \gls{dfo} methods can be an approach to solving such problems.
This concept is often referred to as derivative-free reinforcement learning.

\subsection{Other industrial and engineering applications}

\Gls{dfo} methods are also widely used in industry and engineering, especially for solving problems that involve heavy simulations.
Such problems arise from helicopter rotor blade manufacturing~\cite{Booker_Etal_1998a,Booker_Etal_1998b,Serafini_1998}, aeroacoustic shape design~\cite{Marsden_2004,Marsden_Etal_2004}, computational fluid dynamics~\cite{Duvigneau_Visonneau_2004}, worst-case analysis of analog circuits~\cite{Latorre_Etal_2019}, rapid-cycling synchrotron accelerator modeling~\cite{Eldred_Etal_2022}, nuclear energy engineering~\cite{Kortelainen_Etal_2010,Kortelainen_Etal_2012,Kortelainen_Etal_2014}, reservoir engineering and engine calibration~\cite{Langouet_2011}, and groundwater supply and bioremediation engineering~\cite{Fowler_Etal_2008,Mugunthan_Shoemaker_Regis_2005,Yoon_Shoemaker_1999}, to name but a few.
In general, problems that involve sophisticated models, simulations, or experiments, induce \gls{dfo} problems.

A particular application of \gls{dfo} comes from \gls{mdo}\index{dfo@\glsfmtshort{dfo}!mdo@\glsfmtshort{mdo}} in the industry.
It is a field that uses optimization methods to solve design problems defined by multiple disciplines.
The objective and constraint functions of an \gls{mdo} problem can be provided by different departments of the same company or even by different companies.
It is the case in aircraft engineering~\cite{Gazaix_Etal_2019}, where the design problem of one component is solved while taking into account constraints imposed by other components handled by different departments.
\Gls{mdo} problems often involve simulations or experiments, and therefore, \gls{dfo} methods are often needed.
In \cref{ch:pdfo} of this thesis, we will present a piece of software we implemented for solving \gls{dfo} problems based on methods by Powell~\cite{Powell_1994,Powell_2002,Powell_2006,Powell_2009,Powell_2015}.
It has been included in GEMSEO~\cite{Gallard_Etal_2018}, an engine for \gls{mdo} initiated by a team from IRT Saint Exup{\'{e}}ry\footnote{See \url{https://www.irt-saintexupery.com}.} in France.\index{DFO@\glsfmtshort{dfo}|)}

\section{Optimality conditions for smooth optimization}
\label{sec:optimality-conditions}

We discuss in this section optimality conditions for problem~\cref{eq:problem-introduction}.
We do not assume any structure on the objective and constraint functions, except some smoothness.
More specialized results can be obtained by assuming that problem~\cref{eq:problem-introduction} is convex, for example, but it is out of the scope of this work.

\subsection{Local and global solutions}

\index{solution|(}Before solving problem~\cref{eq:problem-introduction}, we must define what a solution is.
\Cref{def:global-solution} presents the most natural understanding of a solution.

\begin{definition}[Global solution]
    \label{def:global-solution}
    \index{solution!global solution|(}To problem~\cref{eq:problem-introduction}, a point~$\iter[\ast] \in \R^n$ is a \emph{global solution} if~$\iter[\ast] \in \fset$ and~$\obj(\iter) \ge \obj(\iter[\ast])$ for all~$\iter \in \fset$.\index{solution!global solution|)}
\end{definition}

The following relaxed concepts of solutions are of interest in theory and practice.

\begin{definition}[Local solution]
    \index{solution!local solution|(}To problem~\cref{eq:problem-introduction}, a point~$\iter[\ast] \in \R^n$ is
    \begin{itemize}
        \item a \emph{local solution} if~$\iter[\ast] \in \fset$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$\iter[\ast]$ such that~$\obj(\iter) \ge \obj(\iter[\ast])$ for all~$\iter \in \mathcal{N} \cap \fset$,
        \item a \emph{strict local solution} if~$\iter[\ast] \in \fset$ and there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$\iter[\ast]$ such that~$\obj(\iter) > \obj(\iter[\ast])$ for all~$\iter \in \mathcal{N} \cap \fset \setminus \set{\iter[\ast]}$, and
        \item an \emph{isolated local solution} if there exists a neighborhood~$\mathcal{N} \subseteq \R^n$ of~$\iter[\ast]$ such that it is the only local solution in~$\mathcal{N} \cap \fset$.
    \end{itemize}\index{solution!local solution|)}
\end{definition}

It is known that finding a local solution to a general nonconvex problem is NP-hard.
In many cases, it is already NP-hard to check whether a given point is a local solution.
There also exist convex optimization problems that are NP-hard, such as copositive programming, for which checking the feasibility of a given point is indeed NP-hard.
See~\cite{Murty_Kabadi_1987} for more discussions.

The methods we consider in this thesis are local.
They attempt to find approximate local solutions to problem~\cref{eq:problem-introduction}.
However, in general, theoretical analyses of these methods can only guarantee approximations of stationary points, which will be introduced hereafter\index{solution|)}.

\subsection{Constraint qualifications}

\index{constraint qualification|(}Before introducing any necessary and sufficient conditions for local optimality, we discuss some regularity conditions on the constraints~\cref{eq:problem-introduction-cub,eq:problem-introduction-ceq}, referred to as \emph{constraint qualifications}.
They will be required for the necessary conditions to hold.
We first introduce the notion of \emph{active sets}.

\begin{definition}[Active set]
    \label{def:active-set}
    The \emph{active set}~$\act(\iter) \subseteq \iub \cup \ieq$ for problem~\cref{eq:problem-introduction} at a point~$\iter \in \R^n$ is defined by
    \begin{equation*}
        \act(\iter) \eqdef \ieq \cup \set{i \in \iub : \con{i}(\iter) \ge 0}.
    \end{equation*}
\end{definition}

If a constraint belongs to the active set\footnote{For simplicity, we do not distinguish a constraint from its index.} at a given point, it is said to be \emph{active} at this point and \emph{inactive} otherwise.
Note that a violated constraint is always considered active.

We introduce hereafter two classical constraint qualifications.

\begin{definition}[Constraint qualifications]
    \label{def:constraint-qualifications}
    Given~$\iter \in \fset$, denote~$\act(\iter)$ the active set for problem~\cref{eq:problem-introduction} at~$x$, and assume that the constraint functions~$\con{i}$ are differentiable at~$x$ for all~$i \in \act(\iter)$.
    We say that
    \begin{itemize}
        \item the \gls{licq}\index{constraint qualification!LICQ@\glsfmtshort{licq}} holds at~$x$ if the gradients~$\nabla \con{i}(\iter)$\nomenclature[Og]{$\nabla$}{Gradient operator (elements~$\partial / \partial x_i$, with~$i \in \set{1, 2, \dots, n}$)} are linearly independent for all~$i \in \act(\iter)$, and
        \item the \gls{mfcq}\index{constraint qualification!MFCQ@\glsfmtshort{mfcq}} holds at~$x$ if the gradients~$\nabla \con{i}(\iter)$ are linearly independent for all~$i \in \ieq$ and there exists a vector~$z \in \R^n$ such that
        \begin{subequations}
            \label{eq:mangasarian-fromovitz}
            \begin{empheq}[left=\empheqlbrace]{alignat=1}
                & \nabla \con{i}(\iter)^{\T} z < 0, ~ i \in \act(\iter) \cap \iub,\\
                & \nabla \con{i}(\iter)^{\T} z = 0, ~ i \in \ieq.
            \end{empheq}
        \end{subequations}
    \end{itemize}
\end{definition}

The \gls{licq} is stronger than the \gls{mfcq}.
If the \gls{licq} holds at~$\iter \in \fset$, then system~\cref{eq:mangasarian-fromovitz} is consistent because of the linear independence of all~$\nabla \con{i}(\iter)$ for~$i \in \act(\iter)$.

Many other constraint qualifications exist.
Examples include the \gls{acq} and the \gls{gcq}, which are formulated using tangent and linearized cones of the feasible set.
There also exist several traditional constraint qualifications weaker than the \gls{mfcq}, such as the \gls{crcq}, the \gls{cpld}, or the \gls{qncq}.
We also note that dedicated constraint qualifications may exist when the problem has a particular structure, such as the \gls{sc} for convex problems.\index{constraint qualification|)}

\subsection{First-order optimality conditions}

\subsubsection{Statement of the optimality conditions}

\index{KKT conditions@\glsfmtshort{kkt} conditions|(}Let~$\lag$\nomenclature[Fc]{$\lag$}{Lagrangian function} be the \emph{Lagrangian}\index{Lagrangian function} of     problem~\cref{eq:problem-introduction}, defined by
\begin{equation*}
    \lag(\iter, \lm) \eqdef \obj(\iter) + \sum_{\mathclap{i \in \iub \cup \ieq}} \lm_i \con{i}(\iter), \quad \text{for~$\iter \in \R^n$ and~$\lm_i \in \R$ for~$i \in \iub \cup \ieq$},
\end{equation*}
where~$\lm = [\lm_i]_{i \in \iub \cup \ieq}^{\T}$.

\begin{theorem}[First-order necessary conditions~{\cite[Thm.~12.1]{Nocedal_Wright_2006}}]
    \label{thm:first-order-necessary-conditions}
    Let~$\iter[\ast] \in \fset$ be a local solution to~\cref{eq:problem-introduction}, assume that the functions~$\obj$ and~$\con{i}$ are continuously differentiable in a neighborhood of~$\iter[\ast]$ for all~$i \in \iub \cup \ieq$, and that the \gls{licq} holds at~$\iter[\ast]$.
    Then there exists a Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ such that
    \begin{subequations}
        \label{eq:kkt-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=1}
            & \nabla_x \lag(\iter[\ast], \lm[\ast]) = 0, \label{eq:kkt-introduction-stationarity}\\
            & \con{i}(\iter[\ast]) \le 0, ~ i \in \iub \label{eq:kkt-introduction-primal-feasibility-ub}\\
            & \con{i}(\iter[\ast]) = 0, ~ i \in \ieq \label{eq:kkt-introduction-primal-feasibility-eq}\\
            & \lm[\ast]_i \con{i}(\iter[\ast]) = 0, ~ i \in \iub \label{eq:kkt-introduction-complementary-slackness}\\
            & \lm[\ast]_i \ge 0, ~ i \in \iub \label{eq:kkt-introduction-dual-feasibility}
        \end{empheq}
    \end{subequations}
\end{theorem}

We present \cref{thm:first-order-necessary-conditions} with the \gls{licq} as an example, but a similar conclusion can be established with other constraint qualifications, such as the \gls{mfcq} presented above (see, e.g.,~\cite[p.~339]{Nocedal_Wright_2006}).
The conditions~\cref{eq:kkt-introduction} are commonly referred to as the \gls{kkt} conditions~\cite{Karush_1939,Kuhn_Tucker_1951}, and the pair~$(\iter[\ast], \lm[\ast])$ in \cref{thm:first-order-necessary-conditions} is referred to as a \emph{\gls{kkt} pair}.
More specifically, condition~\cref{eq:kkt-introduction-stationarity} is the \emph{stationarity} condition, conditions~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq} as the \emph{primal feasibility} conditions, condition~\cref{eq:kkt-introduction-complementary-slackness} as the \emph{complementary slackness} condition, and condition~\cref{eq:kkt-introduction-dual-feasibility} as the \emph{dual feasibility} condition.
A \emph{first-order stationary point} is a point~$\iter \in \R^n$ satisfying the \gls{kkt} conditions~\cref{eq:kkt-introduction}.
Such a point may not be a local solution.\index{KKT conditions@\glsfmtshort{kkt} conditions|)}

\subsubsection{An illustration of the first-order optimality conditions}

We do not provide a proof of \cref{thm:first-order-necessary-conditions}, but we illustrate graphically the main idea on the simple~$2$-dimensional example
\begin{subequations}
    \label{eq:kkt-description}
    \begin{align}
        \min_{\iter \in \R^2}   & \quad \obj(\iter) = x_1 + x_2\\
        \text{s.t.}             & \quad \con{1}(\iter) = x_1^2 + x_2^2 - 2 \le 0, \label{eq:kkt-description-c1}\\
                                & \quad \con{2}(\iter) = -x_2 \le 0, \label{eq:kkt-description-c2}
    \end{align}
\end{subequations}
whose solution is~$\iter[\ast] = [-\sqrt{2}, 0]^{\T}$.
A graphical representation of problem~\cref{eq:kkt-description} is given in \cref{fig:kkt-description}, where the white area represents the feasible set.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[%
            xmin=-5,%
            xmax=2,%
            ymin=-2,%
            ymax=2,%
            axis equal image,%
            xlabel={$x_1$},%
            ylabel={$x_2$},%
            axis background/.style={%
                pattern=north west lines,%
                pattern color=black!20,%
                even odd rule,%
                insert path={let \p1=(axis cs:0,0), \p2=(axis cs:2^0.5,0), \n1={veclen(\x2-\x1,\y2-\y1)}, in (\p2) arc(0:180:\n1) -- cycle},%
            },%
        ]
            \draw[dashed] (0,0) circle[radius=2^0.5];
            \draw[dashed] (-5,0) -- (2,0);
            \draw[-latex] (-2^0.5,0) -- (-3*2^0.5,0) node[above right] {$\nabla \con{1}(\iter[\ast])$};
            \draw[-latex] (-2^0.5,0) -- (-2^0.5,-1) node[below left] {$\nabla \con{2}(\iter[\ast])$};
            \draw[-latex] (-2^0.5,0) -- (1-2^0.5,1) node[below right] {$\nabla \obj(\iter[\ast])$};
            \addplot[BrickRed,mark=*,only marks] coordinates {(-2^0.5,0)};
            \node[above left] at (-2^0.5,0) {$\iter[\ast]$};
        \end{axis}
    \end{tikzpicture}
    \caption{Graphical representation of the problem~\cref{eq:kkt-description}}
    \label{fig:kkt-description}
\end{figure}

As manifest in \cref{fig:kkt-description}, there does not exist any direction~$\step \in \R^2$ satisfying
\begin{subequations}
    \label{eq:kkt-proof}
    \begin{empheq}[left=\empheqlbrace]{alignat=1}
        & \nabla \obj(\iter[\ast])^{\T} \step < 0,\\
        & \nabla \con{1}(\iter[\ast])^{\T} \step \le 0,\\
        & \nabla \con{2}(\iter[\ast])^{\T} \step \le 0.
    \end{empheq}
\end{subequations}
The Farkas' lemma~\cite{Farkas_1902} ensures, therefore, that there exists a nonnegative Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_1, \lm[\ast]_2]^{\T}$ such that
\begin{equation*}
    \nabla \obj(\iter[\ast]) + \lm[\ast]_2 \nabla \con{1}(\iter[\ast]) + \lm[\ast]_2 \nabla \con{2}(\iter[\ast]) = 0.
\end{equation*}
This validates the condition~\cref{eq:kkt-introduction-stationarity}, while~\cref{eq:kkt-introduction-primal-feasibility-ub,eq:kkt-introduction-primal-feasibility-eq,eq:kkt-introduction-complementary-slackness,eq:kkt-introduction-dual-feasibility} are conspicuous.
A solution to~\cref{eq:kkt-proof} is both a descent direction for~$\obj$ and a linearized feasible direction for the constraints.
In the context of~\cref{thm:first-order-necessary-conditions}, the nonexistence of such a direction is guaranteed by the \gls{licq}.
See~\cite[\S~12.4]{Nocedal_Wright_2006} for a complete proof of \cref{thm:first-order-necessary-conditions}.

\subsection{Second-order optimality conditions}

It is known that at a local solution of a smooth unconstrained optimization problem, the gradient of the objective function is zero and its Hessian matrix is positive semidefinite.
\Cref{thm:second-order-necessary-conditions} generalizes this fact to    problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order necessary conditions~{\cite[Thm.~12.5]{Nocedal_Wright_2006}}]
    \label{thm:second-order-necessary-conditions}
    Let~$\iter[\ast] \in \fset$ be a local solution to~\cref{eq:problem-introduction}.
    Assume that the functions~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are twice continuously differentiable in a neighborhood or~$\iter[\ast]$, and that the \gls{licq} holds at~$\iter[\ast]$.
    Denote the active set for problem~\cref{eq:problem-introduction} at~$\iter[\ast]$ by~$\act(\iter[\ast])$.
    Let~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ be a Lagrange multiplier with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ satisfying the \gls{kkt} conditions~\cref{eq:kkt-introduction}, and let~$z \in \R^n$ be any vector such that
    \begin{subequations}
        \label{eq:second-order-introduction}
        \begin{empheq}[left=\empheqlbrace]{alignat=2}
            & \nabla \con{i}(\iter[\ast])^{\T} z = 0, ~ i \in \ieq,\\
            & \nabla \con{i}(\iter[\ast])^{\T} z = 0, ~ i \in \act(\iter[\ast]) \cap \iub, ~ \lm[\ast]_i > 0,\\
            & \nabla \con{i}(\iter[\ast])^{\T} z \ge 0, ~ i \in \act(\iter[\ast]) \cap \iub, ~ \lm[\ast]_i = 0.
        \end{empheq}
    \end{subequations}
    Then~$z^{\T} \nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast]) z \ge 0$\nomenclature[Oh]{$\nabla^2$}{Hessian operator (elements~$\partial^2 / \partial x_i \partial x_j$, with~$i, j \in \set{1, 2, \dots, n}$)}.
\end{theorem}

A first-order stationary point~$\iter \in \fset$ is called a \emph{second-order stationary point} if it satisfies the conclusion of \cref{thm:second-order-necessary-conditions}.
We emphasize that a second-order stationary point may not be a local solution.
However, it is known in smooth unconstrained optimization that a point at which the gradient of the objective function is zero and its Hessian matrix is positive definite is a strict local solution.
\Cref{thm:second-order-sufficient-conditions} generalizes this fact to the optimization problem~\cref{eq:problem-introduction}.

\begin{theorem}[Second-order sufficient conditions~{\cite[Thm.~12.6]{Nocedal_Wright_2006}}]
    \label{thm:second-order-sufficient-conditions}
    Let~$\iter[\ast] \in \fset$ be a given point.
    Assume that the functions~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, are twice continuously differentiable in a neighborhood of~$\iter[\ast]$, and that at this point there exists a Lagrange multiplier~$\lm[\ast] = [\lm[\ast]_i]_{i \in \iub \cup \ieq}^{\T}$ with~$\lm[\ast]_i \in \R$ for all~$i \in \iub \cup \ieq$ satisfying the KKT condition~\cref{eq:kkt-introduction}.
    If for all vector~$z \in \R^n \setminus \set{0}$ satisfying the conditions~\cref{eq:second-order-introduction} we have~$z^{\T} \nabla_{x, x}^2 \lag(\iter[\ast], \lm[\ast]) z > 0$, then~$\iter[\ast]$ is a strict local solution to problem~\cref{eq:problem-introduction}.
\end{theorem}

\section{Methodology of \glsfmtlong{dfo}}
\label{sec:methodology-dfo}

\index{DFO@\glsfmtshort{dfo}|(}As summarized in~\cite{Conn_Scheinberg_Vicente_2009b}, two main strategies have been developed for solving \gls{dfo} problems.
One strategy consists of sampling the objective function around the current iterate and choosing the next iterate among the sampled points based on simple comparisons.
The \emph{direct-search} methods~\cite{Kolda_Lewis_Torczon_2003} are based on this framework.
The other strategy builds iteratively models that approximate the problems (e.g., using polynomials) around the current iterate and choose the next iterate according to the approximated problems.
These methods are referred to as \emph{model-based} methods.
Hybrid methods also exist, such as implicit filtering~\cite{Kelley_2011} and \gls{sidpsm}~\cite{Custodio_Rocha_Vicente_2009}, which combine direct search and models.

We remark that there exist methods that can solve \gls{dfo} problems, but that are not covered by the above categories.
Examples include random search methods~\cite{Zhigljavsky_1991}, simulated annealing methods~\cite{Kirkpatrick_Gelatt_Vecchi_1983}, the genetic algorithm~\cite{Jong_1975,Holland_1975}, and Bayesian optimization methods~\cite{Mockus_1975,Shahriari_Etal_2016}.
For more information on these methods, we refer to the review~\cite{Larson_Menickelly_Wild_2019} and the reference therein.

\subsection{Direct-search methods}

\index{DFO@\glsfmtshort{dfo}!direct-search|(}An early example of \gls{dfo} is a method from \citeauthor{Fermi_Metropolis_1952}~\cite{Fermi_Metropolis_1952}, who developed in \citeyear{Fermi_Metropolis_1952} a nonlinear least-squares solver on MANIAC, a computer based on the von Neumann architecture.
From a modern viewpoint, this method is a coordinate search method, a particular example of direct-search methods, where the search directions are defined to be the coordinate axes.

Several direct-search methods appeared after that.
\citeauthor{Rosenbrock_1960} designed a direct-search method for unconstrained problems.
It constructs an orthogonal basis using previous steps and searching along the directions in this basis~\cite{Rosenbrock_1960}.
Another famous early direct-search method is the Hooke-Jeeves method~\cite{Hooke_Jeeves_1961}.
This method combines exploratory moves along coordinates axes with pattern moves, to exploit the pattern revealed by previous successful directions.
In \citeyear{Nelder_Mead_1965}, \citeauthor{Nelder_Mead_1965}~\cite{Nelder_Mead_1965} introduced the simplex method\footnote{Note that it is different from the simplex method in linear programming.}, which evaluates the objective function at the vertices of a simplex, and updates this simplex according to these function values, one vertex at each iteration.
It is arguably the most widely used \gls{dfo} method, available as the \verb|fminsearch| function in MATLAB, and many variations exist~\cite{Wright_2012}.
Many other works on direct search appeared in the same period, and summaries can be found in~\cite{Fletcher_1965,Box_1966}.
Nowadays, the direct-search paradigm offers an abundance of algorithms~\cite{Kolda_Lewis_Torczon_2003}, such as the \gls{gps} methods~\cite{Booker_Etal_1999}, later extended to the \gls{mads} methods~\cite{Audet_Dennis_2006,Abramson_Audet_2006,Abramson_Etal_2009,Audet_Dennis_Digabel_2008,Digabel_2011}.
A recent example of direct-search methods is the \gls{bfo}~\cite{Porcelli_Toint_2017,Porcelli_Toint_2022}, which handles integer and categorical variables and can be self-tuned.

\Citeauthor{Gratton_Etal_2015} recently proposed incorporating stochastic strategies in direct-search methods~\cite{Gratton_Etal_2015,Gratton_Etal_2019}.
This improves both the performance and the worst-case complexity bounds compared with traditional direct-search methods.\index{DFO@\glsfmtshort{dfo}!direct-search|)}

\subsection{Model-based methods}
\label{subsec:model-based-methods}

\index{DFO@\glsfmtshort{dfo}!model-based|(}Unlike direct-search methods, model-based methods approximate locally the functions involved in the optimization problems by simple functions called \emph{models} or \emph{surrogates}.
To make model-based methods globally convergent, the models are exploited by a globalization strategy, such as a \emph{line-search}\index{DFO@\glsfmtshort{dfo}!model-based!line-search} framework~\cite[Ch.~3]{Nocedal_Wright_2006} or a \emph{trust-region} framework~\cite{Conn_Gould_Toint_2000,Yuan_2015}.
Most of the existing model-based methods use linear or quadratic approximations~\cite{Powell_1994,Conn_Scheinberg_Vicente_2008a,Conn_Scheinberg_Vicente_2008b}, although other models have also been successfully used, such as \glspl{rbf}~\cite{Powell_2004a,Oeuvray_2005}.

Model-based methods are highly appealing in practice, as they provide excellent performances in real applications.
Compared with direct-search methods, the information contained in the function values is better exploited due to the use of models.
The first trust-region \gls{dfo} method was developed by~\citeauthor{Winfield_1969}~\cite{Winfield_1969,Winfield_1973} in \citeyear{Winfield_1969}.
It is also regarded as the first trust-region method with or without derivatives~\cite[\S~1.2]{Conn_Gould_Toint_2000}.
A similar method was later developed by \citeauthor{Powell_2002}, namely \gls{uobyqa}~\cite{Powell_2002}.
Both methods use fully-determined quadratic interpolation models (see \cref{sec:multivariate-interpolation} for detailed discussions).
By updating the Lagrange functions of the interpolation problem, \gls{uobyqa} needs only~$\bigo(n^4)$\nomenclature[Ng]{$\bigo(\cdot)$}{Big-O notation} computer operations to obtain each quadratic interpolant, whereas the complexity is~$\bigo(n^6)$ in \citeauthor{Winfield_1969}'s method.

The methods of both \citeauthor{Winfield_1969} and \citeauthor{Powell_2002} require~$\bigo(n^2)$ function evaluations to establish each quadratic model.
Such an amount of function values are needed to initialize the methods, although most of them will be reused at subsequent iterations.
However, this amount of function evaluations is not scalable to moderately large problems.
It motivates methods that use underdetermined quadratic interpolation models.
Such models typically require~$\bigo(n)$ function values to be built, and the remaining freedom bequeathed by the interpolation conditions is taken up by minimizing a functional that reflects the regularity of the model.
For example, the method of \citeauthor{Conn_Toint_1996}~\cite{Conn_Toint_1996} uses such models with the least~$\ell_2$-norm of the coefficients.
Powell's methods, namely \gls{newuoa}~\cite{Powell_2006}, \gls{bobyqa}~\cite{Powell_2009}, and \gls{lincoa}~\cite{Powell_2015}, use models that minimize the Frobenius norm of the change to their Hessian matrices.
Another example of such algorithms is \gls{mnh}~\cite{Wild_2008}, whose models are underdetermined quadratic interpolants with the least Frobenius norm of their Hessian matrices.

Particular care must be given to the geometry of the interpolation set in order to maintain reasonable accuracies of the models (see \cref{sec:poisedness}).
Model-based methods usually make explicit geometry-improving steps when necessary~\cite{Conn_Scheinberg_Vicente_2008a,Conn_Scheinberg_Vicente_2008b}.
The Wedge method~\cite{Marazzi_Nocedal_2002} of \citeauthor{Marazzi_Nocedal_2002}, however, does not include such steps.
Instead, it adds a so-called wedge constraint to the trust-region subproblems to prevent the trial steps from lying in a region that is likely to worsen the geometry of the interpolation set.
The method of \citeauthor{Fasano_Morales_Nocedal_2009}~\cite{Fasano_Morales_Nocedal_2009} does not include geometry-improving steps either but performs surprisingly well.
The authors conjecture that a self-correcting mechanism may be at play and prevents the geometry from deteriorating.
\citeauthor{Scheinberg_Toint_2010}~\cite{Scheinberg_Toint_2010} point out that the geometry improvement cannot be dispensed in general, but a slight modification of the algorithm in~\cite{Fasano_Morales_Nocedal_2009} does enjoy a self-correcting mechanism that guarantees the convergence without taking explicit geometry-improving steps.

There are many other trust-region \gls{dfo} methods, such as \gls{csv2}~\cite{Billups_Larson_Graf_2013}, which determines its models by regression and not interpolation.
Examples of solvers that use nonpolynomial models include \gls{orbit}~\cite{Wild_Regis_Shoemaker_2008}, \gls{conorbit}~\cite{Regis_Wild_2017}, and \gls{boosters}~\cite{Oeuvray_Bierlaire_2009}, which use cubic \gls{rbf}.
There also exist methods for more specific problems, such as \gls{dfls}~\cite{Zhang_Conn_Scheinberg_2010} and \gls{dfols}~\cite{Cartis_Etal_2019}, aiming at solving nonlinear least-squares \gls{dfo} problems.

Randomization is also exploited to improve the performance of trust-region \gls{dfo} methods.
This idea was first proposed by \citeauthor{Bandeira_Scheinberg_Vicente_2012}~\cite{Bandeira_Scheinberg_Vicente_2012}, who established the global convergence of a trust-region method based on random models.
They only require approximating the objective function well enough with a certain probability, and such models can be obtained by interpolation on randomly selected points~\cite{Bandeira_Scheinberg_Vicente_2014}.
The global convergence rate of this method is established by \citeauthor{Gratton_Etal_2018}~\cite{Gratton_Etal_2018}, using an idea elaborated earlier in~\cite[\S~6]{Gratton_Etal_2015}.
Similar results are established independently by \citeauthor{Cartis_Scheinberg_2018}~\cite{Cartis_Scheinberg_2018} but for a more general class of methods.
The work of \citeauthor{Bandeira_Scheinberg_Vicente_2012} has motivated many investigations on methods that use randomized models, such as~\cite{Chen_Menickelly_Scheinberg_2018}.\index{DFO@\glsfmtshort{dfo}!model-based|)}

\subsection{Comments on methods based on finite differences}
\label{subsec:finite-difference}

Perhaps the most straightforward approach to solving \gls{dfo} problems is to make finite-difference approximations of the derivatives and then employ derivative-based methods.
Here, we briefly discuss the advantages and disadvantages of this approach.

The~$i$th coordinate of the gradient of a smooth function~$\obj : \R^n \to \R$ at a point~$\iter \in \R^n$ can be approximated by the forward difference
\begin{equation}
    \label{eq:forward-difference}
    \frac{\partial \obj}{\partial x_i}(\iter) = \frac{\obj(\iter + h e_i) - \obj(\iter)}{h} + \bigo(h),
\end{equation}
or the central difference
\begin{equation}
    \label{eq:central-difference}
    \frac{\partial \obj}{\partial x_i}(\iter) = \frac{\obj(\iter + h e_i) - \obj(\iter - h e_i)}{2h} + \bigo(h^2),
\end{equation}
where~$h > 0$ is the difference parameter,~$e_i \in \R^n$ is the~$i$th standard coordinate vector of~$\R^n$, and the order of precision require standard assumptions (see, e.g.,~\cite[\S~8.1]{Nocedal_Wright_2006}).

Methods based on finite differences have several advantages.
Firstly, they are relatively easy to implement, whereas the implementation of many commonly used \gls{dfo} methods is highly nontrivial and challenging\footnote{For example, \citeauthor{Powell_2006}~\cite{Powell_2006} wrote \enquote{The development of \gls{newuoa} has taken nearly three years. The work was very frustrating [\dots]}}.
Moreover, there is a profusion of well-established derivative-based methods that can be explored with finite-difference approximations.
Finally, we highlight that the function evaluations needed by the approximated derivatives in~\cref{eq:forward-difference,eq:central-difference} can be straightforwardly parallelized, making the resulting algorithm scalable in many situations.

Meanwhile, such methods come with disadvantages as well.
First of all, if the problem is noisy, it is nontrivial to choose the difference parameter~$h$.
From a theoretical standpoint, the optimal choice for the difference parameter depends on the noise level and the Lipschitz constants of the derivatives of the function~$\obj$ (see, e.g.,~\cite[\S~8.1]{Nocedal_Wright_2006} and~\cite[Eqs.~(2.13) and~(2.14)]{Shi_Etal_2022}).
There exist procedures to estimate these quantities (see, e.g.,~\cite[\S~3]{More_Wild_2011} and~\cite[Proc.~I]{Shi_Etal_2022}).
However, these procedures are often costly in terms of function evaluations in practice.
In addition, it is difficult to reuse the function values in methods based on finite differences.
A finite difference at~$x \in \R^n$ requires the function to be evaluated on a mesh of~$\bigo(n)$ points around~$x$ oriented along the coordinate directions.
When~$x$ changes, most of the mesh points change completely, and hence, the function needs to be evaluated at another batch of~$\bigo(n)$ points.

There is no sharp division between methods based on finite difference and those based on interpolation models.
Indeed, the forward difference~\cref{eq:forward-difference} produces the gradient of the linear function that interpolates~$f$ at
\begin{equation*}
    \set{x, x + h e_1, x + h e_2, \dots, x + h e_n}.
\end{equation*}
Similarly, the central difference~\cref{eq:central-difference} generates the gradient of the unique quadratic function with diagonal Hessian matrix that interpolates~$f$ at
\begin{equation*}
    \set{x, x + h e_1, x - h e_1, x + h e_2, x - h e_2, \dots, x + h e_n, x - h e_n}.
\end{equation*}
Interpolation can be regarded as a generalization of finite difference because there is more freedom in choosing the interpolation set.
This freedom allows us to reuse most of the interpolation points, which is essential for the efficiency of model-based methods.

\Cref{sec:benchmarking-tools} will present some simple numerical experiments after introducing the benchmarking tools we use in this thesis for comparing \gls{dfo} solvers.
These results will demonstrate that the methods based on finite differences are highly sensitive to noise.
Unsurprisingly, the methods suffer if the difference parameter is not adapted to the noise level.

\section{Benchmarking tools for \glsfmtlong{dfo} methods}
\label{sec:benchmarking-tools}

We introduce in this section the benchmarking tools that we will use throughout this thesis to compare \gls{dfo} solvers.
We use the performance and data profiles~\cite{Dolan_More_2002,More_Wild_2009}, developed by Dolan, Mor{\'{e}}, and Wild.
Most of the information in this section can be found in the aforementioned articles.

\subsection{Expense measure and convergence test}
\label{subsec:convergence-test}

We denote by~$\xsv$ a set of solvers to benchmark and~$\xpb$ a set of test problems, assumed to represent the problems for which the solvers have been designed.
Let~$t_{p, s}$ be the expense for the solver~$s \in \xsv$ to achieve a given convergence test on the problem~$p \in \xpb$.
As mentioned in \cref{sec:overview}, the major cost of \gls{dfo} in practice is the function evaluations.
Therefore, in a \gls{dfo} context,~$t_{p, s}$ measures the number of function evaluations required by~$s \in \xsv$ to solve~$p \in \xpb$ up to the convergence test.
When~$s$ fails to satisfy the convergence test for~$p$ within a given budget (e.g., a maximal number of function evaluations), we define~$t_{p, s} = \infty$ by convention.

In this thesis, the numerical experiments select~$\xpb$ from the CUTEst set~\cite{Gould_Orban_Toint_2015}.
We consider the following convergence test for \gls{dfo} solvers, following~\cite[\S~2]{More_Wild_2009}.
We define for each problem~$p \in \xpb$ a merit function~$\varphi_p$, i.e., a function that measures the quality of a point, taking into account the values of both the constraint and the objective functions.
The smaller value of~$\varphi_p$, the better.
Let~$x_p^0$ be the initial guess for a given problem~$p \in \xpb$, and~$\varphi_p^{\ast}$ be the least value of~$\varphi_p$ obtained by the solvers in~$\xsv$.
Given a tolerance~$\tau \in (0, 1)$\nomenclature[Sc]{$(a, b)$}{Open set~$\set{\iter \in \R : a < \iter < b}$ with~$a < b$}, a point~$x$ satisfies the convergence test if
\begin{equation}
    \label{eq:convergence-test-profiles}
    \varphi_p(\iter) \le \varphi_p^{\ast} + \tau [\varphi_p(\iter_p^0) - \varphi_p^{\ast}],
\end{equation}
in which case we say that~$x$ solves problem~$p$ up to the tolerance~$\tau$.
This convergence test can be interpreted as follows.
A point~$x$ satisfies the test if the reduction~$\varphi_p(\iter_p^0) - \varphi_p(\iter)$ is at least~$1 - \tau$ times the maximal reduction~$\varphi_p(\iter_p^0) - \varphi_p^{\ast}$ achieved by all solvers in~$\xsv$.
We say that a solver solves the problem~$p$ up to the tolerance~$\tau$ whenever it produces an iterate that achieves the convergence test~\cref{eq:convergence-test-profiles}.

In the convergence test~\cref{eq:convergence-test-profiles}, it is tempting to define~$\varphi_p^{\ast}$ as the merit function value at a minimizer of~$p$, which is the case when testing derivative-based solvers~\cite{Dolan_More_2002}.
However, according to~\cite{More_Wild_2009}, this may not be appropriate in \gls{dfo} because it may happen that no solver in~$\xsv$ achieves the test within the given computational budget if the function evaluations are expensive.

\subsection{Performance profile}

Fix a tolerance~$\tau \in (0, 1)$ in the convergence test~\cref{eq:convergence-test-profiles}.
For a solver~$s \in \xsv$ and a problem~$p \in \xpb$, define the \emph{performance ratio} by
\begin{equation}
    \label{eq:performance-ratio}
    r_{p, s} \eqdef \frac{t_{p, s}}{\min \set{t_{p, u} : u \in \xsv}},
\end{equation}
which is the \emph{relative} expense for~$s$ to solve~$p$ compared with the most efficient solver in~$\xsv$ for this problem.
The \emph{performance profile} of~$s$ is defined as
\begin{equation*}
    \rho_s(\alpha) \eqdef \frac{1}{\card(\xpb)} \card (\set{p \in \xpb : r_{p, s} \le \alpha}), \quad \text{for~$\alpha \ge 1$},
\end{equation*}
\nomenclature[Oj]{$\card$}{Cardinal function}%
where~$\card(\cdot)$ denotes the cardinal number of a set.
Clearly,~$\rho_s(\alpha)$ is the proportion of problems in~$\xpb$ that are solved by~$s$ with a performance ratio at most~$\alpha$.
It can also be interpreted as the probability for the solver~$s$ to solve a random problem from~$\xpb$ under the restriction on the performance ratio.
In particular, $\rho_s(1)$ is the proportion of problems that~$s$ solves faster than any other solver in~$\xsv$.
Meanwhile,
\begin{equation*}
    \lim_{\alpha \to \infty} \rho_s(\alpha)
\end{equation*}
is the proportion of problems that are solved by~$s$ (within the budget restriction).
Given two solvers~$s_1$ and~$s_2$,~$\rho_{s_1}(\alpha) > \rho_{s_2}(\alpha)$ means that~$s_1$ solves more problems than~$s_2$ under the constraint that~$r_{p, s_1} \le \alpha$ and~$r_{p, s_2} \le \alpha$ for~$p \in \xpb$.
Therefore, a larger value of~$\rho_s$ indicates a better performance of~$s$.

\subsection{Data profile}

We now introduce the data profile, another benchmarking tool proposed by~\cite{More_Wild_2009}.
The \emph{data profile} of a solver~$s \in \xsv$ is defined as
\begin{equation}
    \label{eq:data-profile}
    d_s(\alpha) \eqdef \frac{1}{\card(\xpb)} \card \bigg(\set[\bigg]{p \in \xpb : \frac{t_{p, s}}{n_p + 1} \le \alpha}\bigg), \quad \text{for~$\alpha \ge 0$},
\end{equation}
where~$n_p$ is the dimension of the problem~$p$.
Therefore,~$d_s(\alpha)$ is the proportion of problems in~$\xpb$ that are solved by~$s$ with at most~$\alpha (n_p + 1)$ function evaluations.
It can also be interpreted as the probability for the solver~$s$ to solve a random problem from~$\xpb$, under the budget described above.
In particular,~$d_s(0) = 0$ and, the same as~$\rho_s$,
\begin{equation*}
    \lim_{\alpha \to \infty} d_s(\alpha)
\end{equation*}
is the proportion of problems that are solved by~$s$.
In equation~\cref{eq:data-profile}, the denominator~$n_p + 1$ is a unit cost that serves to normalize $t_{p, s}$, so that the computational expenses for different problems are comparable even if their dimensions are quite different.
Note that~$n_p + 1$ is the number of function evaluations needed for evaluating a simplex gradient~\cite{Bortz_Kelley_1998}.
Therefore,~$d_s(\alpha)$ is the proportion of problems solved by~$s$ within a budget equivalent to~$\alpha$ simplex gradient estimates.
Given two solvers~$s_1$ and~$s_2$,~$d_{s_1}(\alpha) > d_{s_2}(\alpha)$ means that~$s_1$ solves more problems than~$s_2$ using at most~$\alpha (n_p + 1)$ function evaluations for each~$p \in \xpb$.
Therefore, a larger value of~$d_s$ indicates a better performance of~$s$.

\subsection{An illustrative example}
\label{subsec:profiles-example}

As an example, we now compare three solvers using performance and data profiles.
These solvers, constituting the set~$\xsv$, are
\begin{enumerate}
    \item \gls{newuoa} (see \cref{subsec:newuoa}), a model-based \gls{dfo} method by Powell~\cite{Powell_2006}; and
    \item \gls{bfgs} and \gls{cg}, two gradient-based solvers provided by SciPy~1.0~\cite{Virtanen_Etal_2020}.
    When no derivatives are provided, they use forward finite-difference to approximate gradients, with the difference parameter~$h = \sqrt{u}$, where~$u$ is the unit roundoff.
\end{enumerate}
The set~$\xpb$ contains~$154$ smooth unconstrained CUTEst problems of dimension at most~$50$, and the convergence tolerance in~\cref{eq:convergence-test-profiles} is~$\tau = 10^{-3}$.
Since these problems are unconstrained, the merit function~$\varphi_p$ of each problem~$p \in \xpb$ is set to be the objective function~$\obj_p$.
Therefore, the convergence test~\cref{eq:convergence-test-profiles} becomes
\begin{equation}
    \label{eq:convergence-test-profiles-unconstrained}
    \obj_p(\iter) \le \obj_p^{\ast} + \tau [\obj_p(\iter_p^0) - \obj_p^{\ast}].
\end{equation}

We conduct two experiments as follows.
\begin{enumerate}
    \item The first experiment is made without modifying the problems in~$\xpb$.
    In~\cref{eq:convergence-test-profiles-unconstrained},~$\obj_p^{\ast}$ is set to the least value of~$f_p$ obtained by all solvers in~$\xsv$.
    The value of~$t_{p, s}$ is defined as the number of function evaluations that the solver~$s \in \xsv$ needs to solve the problem~$p \in \xpb$ up to the tolerance~$\tau$.
    \item The second experiment is a noisy variation of the previous one.
    Let~$\sigma > 0$ be the noise level.
    For each problem~$p \in \xpb$, the objective function is evaluated by
    \begin{equation}
        \label{eq:relative-gaussian-noisy-objective-function}
        \tilde{\obj}_p(\iter) \eqdef [1 + \epsilon(\iter)] \obj_p(\iter),
    \end{equation}
    where~$\epsilon(\iter) \sim N(0, \sigma^2)$\nomenclature[Ni]{$N(\mu, \sigma^2)$}{Gaussian distribution with mean~$\mu$ and variance~$\sigma^2$}.
    Each problem is solved ten times with each solver.
    In the convergence test~\cref{eq:convergence-test-profiles-unconstrained},~$\obj_p^{\ast}$ is either the least value of~$\obj_p$ obtained by all solvers during these ten runs, or the value obtained in the previous noise-free experiment, whichever is smaller.
    The value of~$t_{p, s}$ is set to the average number of function evaluations that~$s \in \xsv$ needs to solve~$p \in \xpb$ up to the tolerance~$\tau$.
    Note that the convergence test~\cref{eq:convergence-test-profiles-unconstrained} uses the values of~$\obj_p$ and not those of~$\tilde{\obj}_p$.
    This means that we evaluate the solvers according to the true objective function, even though the solvers can access only the noisy values produced by~\cref{eq:relative-gaussian-noisy-objective-function}.
\end{enumerate}

In these experiments, we invoke \gls{newuoa} via the \gls{pdfo} package (see~\cref{ch:pdfo}) under the default settings. In particular, the initial trust-region radius is~$1$, the final trust-region radius is~$10^{-6}$, and the number of interpolation points is~$2n+1$, with~$n$ being the dimension of the problem being solved. \gls{bfgs} and \gls{cg} are called with the default configurations in SciPy~1.0. For each testing problem, the starting point is set to the one provided by CUTEst, and the maximal number of function evaluations is set to~$500$ times the number of variables.

\subsubsection{Performance profiles}

\Cref{fig:performance-profile-example} plots the performance profiles obtained by these experiments.
In general, the best solver is indicated by the highest curve.
Note that~$\alpha$ is displayed in~$\log_2$-scale.
\Cref{fig:performance-profile-example-noiseless} corresponds to the noise-free experiment and \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6} correspond to the noisy experiment, with~$\sigma$ being~$10^{-10}$,~$10^{-8}$, and~$10^{-6}$, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noise-free experiment}
        \label{fig:performance-profile-example-noiseless}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-10}$}
        \label{fig:performance-profile-example-noisy-10}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-8-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-8}$}
        \label{fig:performance-profile-example-noisy-8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawperformanceprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-6-perf-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-6}$}
        \label{fig:performance-profile-example-noisy-6}
    \end{subfigure}
    \caption{Performance profiles of \gls{newuoa}, \gls{bfgs}, and \gls{cg} with~$\tau = 10^{-3}$}
    \label{fig:performance-profile-example}
\end{figure}

We can make the following observations on the noise-free experiment in \cref{fig:performance-profile-example-noiseless}.
\begin{enumerate}
    \item \Gls{bfgs} solves more than \SI{90}{\percent} of the problems, while the two other solvers solve slightly fewer problems.
    According to~\cite[\S~2]{More_Wild_2009}, \gls{bfgs} is said more reliable than the other two solvers on the test problems in~$\xpb$, but the margin is narrow.
    \item \Gls{newuoa} uses the least number of function evaluations on about \SI{50}{\percent} of the problems, while the percentages for \gls{bfgs} and \gls{cg} are about \SI{30}{\percent} and \SI{25}{\percent} respectively.
\end{enumerate}

We observe the following for the noisy experiments according to~\cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}.
\begin{enumerate}
    \item \Gls{newuoa} always solves more than \SI{70}{\percent} of the problems while the two other solvers solve significantly fewer problems.
    \item When~$\sigma = 10^{-10}$, \gls{newuoa} uses the least number of function evaluations on more than \SI{55}{\percent} of the problems, while the percentages for \gls{bfgs} and \gls{cg} are about \SI{20}{\percent} and \SI{15}{\percent} respectively.
    For~$\sigma \in \set{10^{-8}, 10^{-6}}$, \gls{newuoa} is the most efficient on almost all the problems that it can solve, while the other two solvers fail on most of the problems in~$\xpb$.
\end{enumerate}

Overall, on the noise-free problems, \gls{bfgs} is slightly more reliable than \gls{newuoa} and \gls{cg}, although \gls{newuoa} is more efficient on about half of the problems.
In contrast, \gls{newuoa} outperforms the two other solvers on noisy problems.
The performances of \gls{bfgs} and \gls{cg} deteriorate significantly when Gaussian noise is imposed as in~\cref{eq:relative-gaussian-noisy-objective-function}, even though the noise level is not high and the convergence tolerance is not demanding.

Note that our results do not contradict the observations of \citeauthor{Shi_Etal_2022}~\cite{Shi_Etal_2022}, where they choose the difference parameter~$h$ more carefully, according to the noise levels and the smoothness of the problems.
We rather keep the default value provided for the solvers \gls{bfgs} and \gls{cg} in SciPy~1.0~\cite{Virtanen_Etal_2020}.
As commented in~\cite{Shi_Etal_2022}, the performance of methods based on finite differences is encouraging when there is no noise, yet much more care is needed when the problems are noisy.
\Gls{newuoa} performs robustly with the noise according to this experiment, which agrees with the observations in~\cite{Shi_Etal_2022}.

We also mention that no conclusive comparison can be made between \gls{bfgs} and \gls{cg} according to \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}.
In fact, as pointed out in~\cite{Gould_Scott_2016}, when there are more than two solvers to compare, performance profiles have limitations when ranking the solvers, except for the one that outperforms the others (if any), such as \gls{newuoa} in \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}.

\subsubsection{Data profiles}

\Cref{fig:data-profile-example} displays the data profiles obtained in these experiments.
A higher curve indicates a better performance.
\Cref{fig:data-profile-example-noiseless} corresponds to the noise-free experiment, and \cref{fig:data-profile-example-noisy-10,fig:data-profile-example-noisy-8,fig:data-profile-example-noisy-6} correspond to the noisy experiment, with~$\sigma$ being~$10^{-10}$,~$10^{-8}$, and~$10^{-6}$, respectively.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{plain-1-50-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noise-free experiment}
        \label{fig:data-profile-example-noiseless}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-10-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-10}$}
        \label{fig:data-profile-example-noisy-10}
    \end{subfigure}
    \par\bigskip
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-8-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-8}$}
        \label{fig:data-profile-example-noisy-8}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \drawdataprofiles{{"NEWUOA","BFGS","CG"}}{noisy-1-50-6-data-bfgs-cg-newuoa-u.csv}{3}
        \caption{Noisy experiment with~$\sigma = 10^{-6}$}
        \label{fig:data-profile-example-noisy-6}
    \end{subfigure}
    \caption{Data profiles of \gls{newuoa}, \gls{bfgs}, and \gls{cg} with~$\tau = 10^{-3}$}
    \label{fig:data-profile-example}
\end{figure}

We can make the following observations on the noise-free experiment on \cref{fig:data-profile-example-noiseless}.
\begin{enumerate}
    \item \Gls{bfgs} solves more than \SI{90}{\percent} of the problems while the two other solvers solve slightly fewer problems.
    It agrees with what is indicated by \cref{fig:performance-profile-example-noiseless}, as it should be the case according to theory.
    \item \Gls{bfgs} uses less than~$300 (n_p + 1)$ function evaluations to solve most of the problems~$p \in \xpb$ that it can solve, while \gls{newuoa} and \gls{cg} use about~$400 (n_p + 1)$.
\end{enumerate}

We observe the following for the noisy experiments according to~\cref{fig:data-profile-example-noisy-10,fig:data-profile-example-noisy-8,fig:data-profile-example-noisy-6}.
\begin{enumerate}
    \item \Gls{newuoa} always solves more than \SI{70}{\percent} of the problems while the two other solvers solve significantly fewer problems.
    It agrees with what is indicated by \cref{fig:performance-profile-example-noisy-10,fig:performance-profile-example-noisy-8,fig:performance-profile-example-noisy-6}, as it should be the case.
    \item When~$\sigma = 10^{-10}$, \gls{newuoa} uses less than~$200 (n_p + 1)$ function evaluations to solve most of the problems~$p \in \xpb$ that it can solve, while \gls{bfgs} and \gls{cg} use less than~$100 (n_p + 1)$, although they solve much fewer problems.
    For~$\sigma \in \set{10^{-8}, 10^{-6}}$, \gls{newuoa} uses less than~$100 (n_p + 1)$ function evaluations\footnote{Even though the corresponding value is~$200 (n_p + 1)$ when~$\sigma = 10^{-10}$, we should not interpret that \gls{newuoa} performs better with higher noise. When~$\sigma$ is larger, \gls{newuoa} solves fewer problems and~$\obj_p^{\ast}$ in~\cref{eq:convergence-test-profiles-unconstrained} is often larger, leading to a weaker convergence test.} to solve most of the problems that it can solve, while the other two solvers fail on most of the problems.
\end{enumerate}

\subsection{Absoluteness and relativeness of the profiles}

Before concluding this section, we mention a significant difference between performance and data profiles.
Data profiles are absolute because~\cref{eq:data-profile} compares the expense~$t_{p, s}$ with a quantity independent of the other solvers, namely,~$n_p + 1$ for each~$p \in \xpb$.
The data profile of a solver does depend weakly on the other solvers, the only dependence being the value of~$\varphi_p^{\ast}$ in the convergence test~\cref{eq:convergence-test-profiles}.
On the other hand, performance profiles are relative because the value of~$r_{p, s}$ in~\cref{eq:performance-ratio} compares the expense of~$s$ with that of the most efficient solver on~$p$, which depends on all the other solvers.
For example, the data profiles in \cref{fig:data-profile-example-noisy-10} suggest that \gls{cg} outperforms \gls{bfgs} during the corresponding experiment, and the profiles are unlikely to change dramatically if \gls{newuoa} is removed from the comparison.
In contrast, the comparison between \gls{bfgs} and \gls{cg} is not as conclusive based on the performance profiles in \cref{fig:performance-profile-example-noisy-10} because the profiles may change significantly if \gls{newuoa} is removed from the comparison.
This phenomenon is studied in~\cite{Gould_Scott_2016}, which presents examples on which comparisons using performance profiles should be made with care.\index{DFO@\glsfmtshort{dfo}|)}

\section{Contributions of the thesis}

The main contributions of this thesis are as follows.

\begin{enumerate}
    \item \emph{An analysis on an interpolation set for underdetermined quadratic interpolation}.
    We show in \cref{sec:optimal-interpolation-set} that an interpolation set introduced by \citeauthor{Powell_2006}~\cite{Powell_2006} for underdetermined quadratic interpolation is optimal in terms of the~$\Lambda$-poisedness in the smallest ball that contains the set.
    This analysis justifies the choice of the default interpolation set employed by Powell in \gls{newuoa}~\cite{Powell_2006}.
    It also provides theoretical guidance for us to choose the initial interpolation set of our new \gls{dfo} solver presented in \cref{ch:cobyqa-introduction}.
    \item \emph{A cross-platform software package for Powell's \gls{dfo} solvers}.
    We introduce in \cref{ch:pdfo} our MATLAB and Python package named \gls{pdfo}, which is open-source and distributed on all the major platforms (Windows, macOS, and Linux), available at \url{https://www.pdfo.net/}.
    It provides user-friendly interfaces for Powell's five model-based \gls{dfo} solvers that are widely used in industrial and engineering applications.
    Powell originally implemented these solvers in Fortran 77, which is becoming an obstacle for many users.
    \Gls{pdfo} simplifies the usage of the solvers, and users do not need to deal with Fortran when using \gls{pdfo}.
    As of August 2022, the package has been downloaded more than \num{30000} times.
    It has been included as a major optimization engine in GEMSEO (\url{https://gemseo.readthedocs.io/}), an industrial software package for \gls{mdo}.
    \item \emph{New perspectives on theory and practice of the \glsfmtshort{sqp} method}.
    \begin{enumerate}
        \item \Cref{thm:sqp-path} provides a new interpretation of the objective function of the \gls{sqp} subproblem, showing that it is a natural quadratic approximation of the original objective function in the tangent space of a surface.
        \item We reveal in \cref{prop:vardi-byrd-omojokun} a relation between the Byrd-Omojokun and the Vardi approaches for solving the trust-region \gls{sqp} subproblem in the equality-constrained case, interpreting the former as an approximation of the latter.
        \item We extend the Byrd-Omojokun and the Vardi approaches to the inequality-constrained case in \cref{subsec:composite-step-inequality}.
        They differ from those given in~\cite[\S~15.4.4]{Conn_Gould_Toint_2000}.
        In particular, our Byrd-Omojokun approach is employed in the new solver described in \cref{ch:cobyqa-introduction}, and performs much better than the one in~\cite[\S~15.4.4]{Conn_Gould_Toint_2000}.
    \end{enumerate}
    \item \emph{A new model-based \gls{dfo} algorithm and its implementation.}
    \begin{enumerate}
        \item We present in \cref{ch:cobyqa-introduction} a new derivative-free trust-region \gls{sqp} algorithm for nonlinearly constrained optimization, named \gls{cobyqa}.
        To the best of our knowledge, this is the first trust-region \gls{sqp} algorithm that can solve \gls{dfo} problems with nonlinear equality and inequality constraints without introducing slack variables.
        \item We detail in \cref{ch:cobyqa-subproblems} the methods for solving the subproblems of \gls{cobyqa} in our implementation.
        In particular, we propose in \cref{ch:cobyqa-subproblems} a new method for approximately solving convex piecewise quadratic programming problems within a trust region.
        We use this method to deal with the normal subproblem of \gls{cobyqa}.
        \item \Cref{ch:cobyqa-implementation} presents the Python implementation of \gls{cobyqa}, which is open-source and publicly available at \url{https://www.cobyqa.com/}.
        \Cref{sec:cobyqa-experiments} details numerical experiments that compare \gls{cobyqa} with Powell's solvers.
        \Gls{cobyqa} is competitive with \gls{newuoa} on unconstrained problems and outperforms \gls{bobyqa} on bound-constrained ones, while being able to handle more general problems.
        Compared with \gls{lincoa} and \gls{cobyla}, a strength of \gls{cobyqa} is that it always respects the bound constraints, while \gls{lincoa} and \gls{cobyla} do not.
        Such a strength is crucial in many applications, as is requested by engineers from IRT Saint Exup{\'{e}}ry.
        On linearly constrained problems, \gls{cobyqa} performs much better than \gls{lincoa} if the problems also contain bound constraints that cannot be violated.
        Most importantly, \gls{cobyqa} outperforms \gls{cobyla} on all types of problems, no matter whether bound constraints (if any) can be violated or not.
        As a general-purpose solver, \gls{cobyqa} is a good successor to \gls{cobyla}.
    \end{enumerate}
\end{enumerate}

\section{Organization of the thesis}

The remaining of this thesis is organized as follows.

\Cref{ch:interpolation} presents interpolation models that are useful to \gls{dfo}.
We will focus on linear and quadratic models, as these are the models employed by the \gls{dfo} methods presented later in this thesis.

\Cref{ch:pdfo} introduces \gls{pdfo}, a package providing MATLAB and Python interfaces for using late Prof.\ M. J. D. Powell's \gls{dfo} solvers.
It also provides an overview of Powell's \gls{dfo} solvers.

\Cref{ch:sqp} presents an overview of the \gls{sqp} framework and provides some perspectives.
In particular, we elaborate an interpretation of the \gls{sqp} subproblem that is new to the best of our knowledge.
We also present a trust-region \gls{sqp} method, discuss various approaches to solving its subproblem, and show an interesting connection between two of them.

\Cref{ch:cobyqa-introduction} presents a new model-based \gls{dfo} method named \gls{cobyqa}.
It is a trust-region \gls{sqp} method, designed to tackle nonlinearly constrained problems that include equality and inequality constraints.
It builds the trust-region quadratic models using derivative-free symmetric Broyden update presented in \cref{ch:interpolation}.

\Cref{ch:cobyqa-subproblems} introduces in details the methods employed by \gls{cobyqa} to solve its subproblems, including the trust-region subproblem, the multiplier-estimating subproblem, and the geometry-improving subproblem.

\Cref{ch:cobyqa-implementation} provides details on the Python implementation of \gls{cobyqa}, in particular its preprocessing procedure, its stopping criteria, and some algorithmic details.
It illustrates the usage of the Python package by several examples.
It finally presents some numerical experiments that compare \gls{cobyqa} with \gls{newuoa}, \gls{bobyqa}, \gls{lincoa}, and \gls{cobyla}.

\Cref{ch:conclusion} summarizes the thesis, and discusses several future research directions.

\section{Notations}

Throughout this thesis, the functions~$\obj$ and~$\con{i}$, with~$i \in \iub \cup \ieq$, always denote the objective and constraint functions of an optimization problem.
The Lagrangian function of this problem is denoted by~$(\iter, \lm) \mapsto \lag(\iter, \lm)$.
Its first argument~$\iter$ is the decision variable, while its second argument~$\lm = [\lm_i]_{i \in \iub \cup \ieq}^{\T}$ is the dual variable of the considered problem.
A quadratic approximation of a function~$u$ is always denoted by~$\hat{u}$, and may be superscripted if this model is used in an iterative process.
For example, if an optimization method maintains a quadratic approximation of the objective function, the~$k$th model of the objective function is denoted by~$\objm[k]$\nomenclature[Ff]{$\objm$}{Quadratic model of~$\obj$}.

In general, an iteration number is written as a \emph{superscript}.
When an exponent is used in a formula that may lead to confusion, we will explicitly state the meaning of the notation in the context.
Moreover, a \emph{subscript} usually signifies a component index of a vector, a matrix, etc.
An exception to this rule is the notation~$e_i \in \R^n$, which represents the~$i$th standard coordinate vector, i.e., the~$i$th column of the identity matrix in~$\R^{n \times n}$.

We will mainly use the~$\ell_2$-norm in this thesis.
Hence,~$\norm{\cdot}$ denotes the~$\ell_2$-norm, unless otherwise stated.
If any other norm is needed, its choice is clarified using subscripts.
For example,~$\norm{\cdot}_{\mathsf{F}}$ denotes the Frobenius norm of a matrix, and~$\norm{\cdot}_1$ denotes the~$\ell_1$-norm of a vector or the corresponding induced norm of a matrix.
